{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_ylWt7YjoY"
      },
      "source": [
        "# StyleGAN3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs28QcYEwPM_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bdgviQQO8WJ",
        "outputId": "47802169-5f18-4990-b0c0-a7d36bad3e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-104095a6-b883-3201-3222-d0cca8a47eff)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL1ERDs1PKJy",
        "outputId": "a2cf7ea4-6a32-431c-f9d7-5999b7d07e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfOWvhrkcJzS",
        "outputId": "6487d0fe-c86a-4f80-a7e4-73f9857c10df"
      },
      "outputs": [],
      "source": [
        "#Uninstall new JAX\n",
        "!pip uninstall jax jaxlib -y\n",
        "#GPU frontend\n",
        "!pip install \"jax[cuda11_cudnn805]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "#CPU frontend\n",
        "#!pip install jax[cpu]==0.3.10\n",
        "#Downgrade Pytorch\n",
        "!pip uninstall torch torchvision -y\n",
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install timm==0.4.12 ftfy==6.1.1 ninja==1.10.2 imageio-ffmpeg opensimplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHzZRl_bXgOT"
      },
      "outputs": [],
      "source": [
        "#to start this you will need to download torch_utils from: https://github.com/NVlabs/stylegan3/\n",
        "#this was originally cloned for me to train, but below is the code for relevant architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uh-PJzEx9yCn"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.dpi'] = 200  # I would like to make it higher, but too slow\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N3Qwwu15AdX"
      },
      "outputs": [],
      "source": [
        "#dnnlib\n",
        "#this is from:https://github.com/NVlabs/stylegan3/tree/main/dnnlib\n",
        "import ctypes\n",
        "import fnmatch\n",
        "import importlib\n",
        "import inspect\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import types\n",
        "import io\n",
        "import pickle\n",
        "import re\n",
        "import requests\n",
        "import html\n",
        "import hashlib\n",
        "import glob\n",
        "import tempfile\n",
        "import urllib\n",
        "import urllib.request\n",
        "import uuid\n",
        "\n",
        "from distutils.util import strtobool\n",
        "from typing import Any, List, Tuple, Union\n",
        "\n",
        "\n",
        "# Util classes\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class EasyDict(dict):\n",
        "\n",
        "    def __getattr__(self, name: str) -> Any:\n",
        "        try:\n",
        "            return self[name]\n",
        "        except KeyError:\n",
        "            raise AttributeError(name)\n",
        "\n",
        "    def __setattr__(self, name: str, value: Any) -> None:\n",
        "        self[name] = value\n",
        "\n",
        "    def __delattr__(self, name: str) -> None:\n",
        "        del self[name]\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n",
        "        self.file = None\n",
        "\n",
        "        if file_name is not None:\n",
        "            self.file = open(file_name, file_mode)\n",
        "\n",
        "        self.should_flush = should_flush\n",
        "        self.stdout = sys.stdout\n",
        "        self.stderr = sys.stderr\n",
        "\n",
        "        sys.stdout = self\n",
        "        sys.stderr = self\n",
        "\n",
        "    def __enter__(self) -> \"Logger\":\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
        "        self.close()\n",
        "\n",
        "    def write(self, text: Union[str, bytes]) -> None:\n",
        "        if isinstance(text, bytes):\n",
        "            text = text.decode()\n",
        "        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n",
        "            return\n",
        "\n",
        "        if self.file is not None:\n",
        "            self.file.write(text)\n",
        "\n",
        "        self.stdout.write(text)\n",
        "\n",
        "        if self.should_flush:\n",
        "            self.flush()\n",
        "\n",
        "    def flush(self) -> None:\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "\n",
        "        self.stdout.flush()\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.flush()\n",
        "\n",
        "        # if using multiple loggers, prevent closing in wrong order\n",
        "        if sys.stdout is self:\n",
        "            sys.stdout = self.stdout\n",
        "        if sys.stderr is self:\n",
        "            sys.stderr = self.stderr\n",
        "\n",
        "        if self.file is not None:\n",
        "            self.file.close()\n",
        "            self.file = None\n",
        "\n",
        "\n",
        "# Cache directories\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "_dnnlib_cache_dir = None\n",
        "\n",
        "def set_cache_dir(path: str) -> None:\n",
        "    global _dnnlib_cache_dir\n",
        "    _dnnlib_cache_dir = path\n",
        "\n",
        "def make_cache_dir_path(*paths: str) -> str:\n",
        "    if _dnnlib_cache_dir is not None:\n",
        "        return os.path.join(_dnnlib_cache_dir, *paths)\n",
        "    if 'DNNLIB_CACHE_DIR' in os.environ:\n",
        "        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n",
        "    if 'HOME' in os.environ:\n",
        "        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n",
        "    if 'USERPROFILE' in os.environ:\n",
        "        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n",
        "    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n",
        "\n",
        "# Small util functions\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def format_time(seconds: Union[int, float]) -> str:\n",
        "    s = int(np.rint(seconds))\n",
        "\n",
        "    if s < 60:\n",
        "        return \"{0}s\".format(s)\n",
        "    elif s < 60 * 60:\n",
        "        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n",
        "    elif s < 24 * 60 * 60:\n",
        "        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n",
        "    else:\n",
        "        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n",
        "\n",
        "\n",
        "def format_time_brief(seconds: Union[int, float]) -> str:\n",
        "    s = int(np.rint(seconds))\n",
        "\n",
        "    if s < 60:\n",
        "        return \"{0}s\".format(s)\n",
        "    elif s < 60 * 60:\n",
        "        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n",
        "    elif s < 24 * 60 * 60:\n",
        "        return \"{0}h {1:02}m\".format(s // (60 * 60), (s // 60) % 60)\n",
        "    else:\n",
        "        return \"{0}d {1:02}h\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24)\n",
        "\n",
        "\n",
        "def ask_yes_no(question: str) -> bool:\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"{0} [y/n]\".format(question))\n",
        "            return strtobool(input().lower())\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "\n",
        "def tuple_product(t: Tuple) -> Any:\n",
        "    result = 1\n",
        "\n",
        "    for v in t:\n",
        "        result *= v\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "_str_to_ctype = {\n",
        "    \"uint8\": ctypes.c_ubyte,\n",
        "    \"uint16\": ctypes.c_uint16,\n",
        "    \"uint32\": ctypes.c_uint32,\n",
        "    \"uint64\": ctypes.c_uint64,\n",
        "    \"int8\": ctypes.c_byte,\n",
        "    \"int16\": ctypes.c_int16,\n",
        "    \"int32\": ctypes.c_int32,\n",
        "    \"int64\": ctypes.c_int64,\n",
        "    \"float32\": ctypes.c_float,\n",
        "    \"float64\": ctypes.c_double\n",
        "}\n",
        "\n",
        "\n",
        "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n",
        "    type_str = None\n",
        "\n",
        "    if isinstance(type_obj, str):\n",
        "        type_str = type_obj\n",
        "    elif hasattr(type_obj, \"__name__\"):\n",
        "        type_str = type_obj.__name__\n",
        "    elif hasattr(type_obj, \"name\"):\n",
        "        type_str = type_obj.name\n",
        "    else:\n",
        "        raise RuntimeError(\"Cannot infer type name from input\")\n",
        "\n",
        "    assert type_str in _str_to_ctype.keys()\n",
        "\n",
        "    my_dtype = np.dtype(type_str)\n",
        "    my_ctype = _str_to_ctype[type_str]\n",
        "\n",
        "    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n",
        "\n",
        "    return my_dtype, my_ctype\n",
        "\n",
        "\n",
        "def is_pickleable(obj: Any) -> bool:\n",
        "    try:\n",
        "        with io.BytesIO() as stream:\n",
        "            pickle.dump(obj, stream)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Functionality to import modules/objects by name, and call functions by name\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n",
        "\n",
        "    # allow convenience shorthands, substitute them by full names\n",
        "    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n",
        "    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n",
        "\n",
        "    # list alternatives for (module_name, local_obj_name)\n",
        "    parts = obj_name.split(\".\")\n",
        "    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n",
        "\n",
        "    # try each alternative in turn\n",
        "    for module_name, local_obj_name in name_pairs:\n",
        "        try:\n",
        "            module = importlib.import_module(module_name) # may raise ImportError\n",
        "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
        "            return module, local_obj_name\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # maybe some of the modules themselves contain errors?\n",
        "    for module_name, _local_obj_name in name_pairs:\n",
        "        try:\n",
        "            importlib.import_module(module_name) # may raise ImportError\n",
        "        except ImportError:\n",
        "            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n",
        "                raise\n",
        "\n",
        "    # maybe the requested attribute is missing?\n",
        "    for module_name, local_obj_name in name_pairs:\n",
        "        try:\n",
        "            module = importlib.import_module(module_name) # may raise ImportError\n",
        "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "    # we are out of luck, but we have no idea why\n",
        "    raise ImportError(obj_name)\n",
        "\n",
        "\n",
        "def get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n",
        "    if obj_name == '':\n",
        "        return module\n",
        "    obj = module\n",
        "    for part in obj_name.split(\".\"):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def get_obj_by_name(name: str) -> Any:\n",
        "    module, obj_name = get_module_from_obj_name(name)\n",
        "    return get_obj_from_module(module, obj_name)\n",
        "\n",
        "\n",
        "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n",
        "    assert func_name is not None\n",
        "    func_obj = get_obj_by_name(func_name)\n",
        "    assert callable(func_obj)\n",
        "    return func_obj(*args, **kwargs)\n",
        "\n",
        "\n",
        "def construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n",
        "    return call_func_by_name(*args, func_name=class_name, **kwargs)\n",
        "\n",
        "\n",
        "def get_module_dir_by_obj_name(obj_name: str) -> str:\n",
        "    module, _ = get_module_from_obj_name(obj_name)\n",
        "    return os.path.dirname(inspect.getfile(module))\n",
        "\n",
        "\n",
        "def is_top_level_function(obj: Any) -> bool:\n",
        "    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n",
        "\n",
        "\n",
        "def get_top_level_function_name(obj: Any) -> str:\n",
        "    assert is_top_level_function(obj)\n",
        "    module = obj.__module__\n",
        "    if module == '__main__':\n",
        "        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n",
        "    return module + \".\" + obj.__name__\n",
        "\n",
        "\n",
        "# File system helpers\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n",
        "    assert os.path.isdir(dir_path)\n",
        "    base_name = os.path.basename(os.path.normpath(dir_path))\n",
        "\n",
        "    if ignores is None:\n",
        "        ignores = []\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for root, dirs, files in os.walk(dir_path, topdown=True):\n",
        "        for ignore_ in ignores:\n",
        "            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n",
        "\n",
        "            # dirs need to be edited in-place\n",
        "            for d in dirs_to_remove:\n",
        "                dirs.remove(d)\n",
        "\n",
        "            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n",
        "\n",
        "        absolute_paths = [os.path.join(root, f) for f in files]\n",
        "        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n",
        "\n",
        "        if add_base_to_relative:\n",
        "            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n",
        "\n",
        "        assert len(absolute_paths) == len(relative_paths)\n",
        "        result += zip(absolute_paths, relative_paths)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n",
        "    for file in files:\n",
        "        target_dir_name = os.path.dirname(file[1])\n",
        "\n",
        "        # will create all intermediate-level directories\n",
        "        if not os.path.exists(target_dir_name):\n",
        "            os.makedirs(target_dir_name)\n",
        "\n",
        "        shutil.copyfile(file[0], file[1])\n",
        "\n",
        "\n",
        "# URL helpers\n",
        "# ------------------------------------------------------------------------------------------\n",
        "\n",
        "def is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n",
        "    if not isinstance(obj, str) or not \"://\" in obj:\n",
        "        return False\n",
        "    if allow_file_urls and obj.startswith('file://'):\n",
        "        return True\n",
        "    try:\n",
        "        res = requests.compat.urlparse(obj)\n",
        "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
        "            return False\n",
        "        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n",
        "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n",
        "    assert num_attempts >= 1\n",
        "    assert not (return_filename and (not cache))\n",
        "    if not re.match('^[a-z]+://', url):\n",
        "        return url if return_filename else open(url, \"rb\")\n",
        "\n",
        "    if url.startswith('file://'):\n",
        "        filename = urllib.parse.urlparse(url).path\n",
        "        if re.match(r'^/[a-zA-Z]:', filename):\n",
        "            filename = filename[1:]\n",
        "        return filename if return_filename else open(filename, \"rb\")\n",
        "\n",
        "    assert is_url(url)\n",
        "\n",
        "    # Lookup from cache.\n",
        "    if cache_dir is None:\n",
        "        cache_dir = make_cache_dir_path('downloads')\n",
        "\n",
        "    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
        "    if cache:\n",
        "        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n",
        "        if len(cache_files) == 1:\n",
        "            filename = cache_files[0]\n",
        "            return filename if return_filename else open(filename, \"rb\")\n",
        "\n",
        "    # Download.\n",
        "    url_name = None\n",
        "    url_data = None\n",
        "    with requests.Session() as session:\n",
        "        if verbose:\n",
        "            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n",
        "        for attempts_left in reversed(range(num_attempts)):\n",
        "            try:\n",
        "                with session.get(url) as res:\n",
        "                    res.raise_for_status()\n",
        "                    if len(res.content) == 0:\n",
        "                        raise IOError(\"No data received\")\n",
        "\n",
        "                    if len(res.content) < 8192:\n",
        "                        content_str = res.content.decode(\"utf-8\")\n",
        "                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n",
        "                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n",
        "                            if len(links) == 1:\n",
        "                                url = requests.compat.urljoin(url, links[0])\n",
        "                                raise IOError(\"Google Drive virus checker nag\")\n",
        "                        if \"Google Drive - Quota exceeded\" in content_str:\n",
        "                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n",
        "\n",
        "                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n",
        "                    url_name = match[1] if match else url\n",
        "                    url_data = res.content\n",
        "                    if verbose:\n",
        "                        print(\" done\")\n",
        "                    break\n",
        "            except KeyboardInterrupt:\n",
        "                raise\n",
        "            except:\n",
        "                if not attempts_left:\n",
        "                    if verbose:\n",
        "                        print(\" failed\")\n",
        "                    raise\n",
        "                if verbose:\n",
        "                    print(\".\", end=\"\", flush=True)\n",
        "\n",
        "    # Save to cache.\n",
        "    if cache:\n",
        "        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n",
        "        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n",
        "        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        with open(temp_file, \"wb\") as f:\n",
        "            f.write(url_data)\n",
        "        os.replace(temp_file, cache_file) # atomic\n",
        "        if return_filename:\n",
        "            return cache_file\n",
        "\n",
        "    # Return data as file object.\n",
        "    assert not return_filename\n",
        "    return io.BytesIO(url_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdYy1bIj119C"
      },
      "outputs": [],
      "source": [
        "#metric utils\n",
        "#this is from:https://github.com/NVlabs/stylegan3/blob/main/metrics/metric_utils.py\n",
        "import os\n",
        "import time\n",
        "import hashlib\n",
        "import pickle\n",
        "import copy\n",
        "import uuid\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class MetricOptions:\n",
        "    def __init__(self, G=None, G_kwargs={}, dataset_kwargs={}, num_gpus=1, rank=0, device=None, progress=None, cache=True):\n",
        "        assert 0 <= rank < num_gpus\n",
        "        self.G              = G\n",
        "        self.G_kwargs       = EasyDict(G_kwargs)\n",
        "        self.dataset_kwargs = EasyDict(dataset_kwargs)\n",
        "        self.num_gpus       = num_gpus\n",
        "        self.rank           = rank\n",
        "        self.device         = device if device is not None else torch.device('cuda', rank)\n",
        "        self.progress       = progress.sub() if progress is not None and rank == 0 else ProgressMonitor()\n",
        "        self.cache          = cache\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "_feature_detector_cache = dict()\n",
        "\n",
        "def get_feature_detector_name(url):\n",
        "    return os.path.splitext(url.split('/')[-1])[0]\n",
        "\n",
        "def get_feature_detector(url, device=torch.device('cpu'), num_gpus=1, rank=0, verbose=False):\n",
        "    assert 0 <= rank < num_gpus\n",
        "    key = (url, device)\n",
        "    if key not in _feature_detector_cache:\n",
        "        is_leader = (rank == 0)\n",
        "        if not is_leader and num_gpus > 1:\n",
        "            torch.distributed.barrier() # leader goes first\n",
        "        with open_url(url, verbose=(verbose and is_leader)) as f:\n",
        "            _feature_detector_cache[key] = pickle.load(f).to(device)\n",
        "        if is_leader and num_gpus > 1:\n",
        "            torch.distributed.barrier() # others follow\n",
        "    return _feature_detector_cache[key]\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def iterate_random_labels(opts, batch_size):\n",
        "    if opts.G.c_dim == 0:\n",
        "        c = torch.zeros([batch_size, opts.G.c_dim], device=opts.device)\n",
        "        while True:\n",
        "            yield c\n",
        "    else:\n",
        "        dataset = construct_class_by_name(**opts.dataset_kwargs)\n",
        "        while True:\n",
        "            c = [dataset.get_label(np.random.randint(len(dataset))) for _i in range(batch_size)]\n",
        "            c = torch.from_numpy(np.stack(c)).pin_memory().to(opts.device)\n",
        "            yield c\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class FeatureStats:\n",
        "    def __init__(self, capture_all=False, capture_mean_cov=False, max_items=None):\n",
        "        self.capture_all = capture_all\n",
        "        self.capture_mean_cov = capture_mean_cov\n",
        "        self.max_items = max_items\n",
        "        self.num_items = 0\n",
        "        self.num_features = None\n",
        "        self.all_features = None\n",
        "        self.raw_mean = None\n",
        "        self.raw_cov = None\n",
        "\n",
        "    def set_num_features(self, num_features):\n",
        "        if self.num_features is not None:\n",
        "            assert num_features == self.num_features\n",
        "        else:\n",
        "            self.num_features = num_features\n",
        "            self.all_features = []\n",
        "            self.raw_mean = np.zeros([num_features], dtype=np.float64)\n",
        "            self.raw_cov = np.zeros([num_features, num_features], dtype=np.float64)\n",
        "\n",
        "    def is_full(self):\n",
        "        return (self.max_items is not None) and (self.num_items >= self.max_items)\n",
        "\n",
        "    def append(self, x):\n",
        "        x = np.asarray(x, dtype=np.float32)\n",
        "        assert x.ndim == 2\n",
        "        if (self.max_items is not None) and (self.num_items + x.shape[0] > self.max_items):\n",
        "            if self.num_items >= self.max_items:\n",
        "                return\n",
        "            x = x[:self.max_items - self.num_items]\n",
        "\n",
        "        self.set_num_features(x.shape[1])\n",
        "        self.num_items += x.shape[0]\n",
        "        if self.capture_all:\n",
        "            self.all_features.append(x)\n",
        "        if self.capture_mean_cov:\n",
        "            x64 = x.astype(np.float64)\n",
        "            self.raw_mean += x64.sum(axis=0)\n",
        "            self.raw_cov += x64.T @ x64\n",
        "\n",
        "    def append_torch(self, x, num_gpus=1, rank=0):\n",
        "        assert isinstance(x, torch.Tensor) and x.ndim == 2\n",
        "        assert 0 <= rank < num_gpus\n",
        "        if num_gpus > 1:\n",
        "            ys = []\n",
        "            for src in range(num_gpus):\n",
        "                y = x.clone()\n",
        "                torch.distributed.broadcast(y, src=src)\n",
        "                ys.append(y)\n",
        "            x = torch.stack(ys, dim=1).flatten(0, 1) # interleave samples\n",
        "        self.append(x.cpu().numpy())\n",
        "\n",
        "    def get_all(self):\n",
        "        assert self.capture_all\n",
        "        return np.concatenate(self.all_features, axis=0)\n",
        "\n",
        "    def get_all_torch(self):\n",
        "        return torch.from_numpy(self.get_all())\n",
        "\n",
        "    def get_mean_cov(self):\n",
        "        assert self.capture_mean_cov\n",
        "        mean = self.raw_mean / self.num_items\n",
        "        cov = self.raw_cov / self.num_items\n",
        "        cov = cov - np.outer(mean, mean)\n",
        "        return mean, cov\n",
        "\n",
        "    def save(self, pkl_file):\n",
        "        with open(pkl_file, 'wb') as f:\n",
        "            pickle.dump(self.__dict__, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(pkl_file):\n",
        "        with open(pkl_file, 'rb') as f:\n",
        "            s = EasyDict(pickle.load(f))\n",
        "        obj = FeatureStats(capture_all=s.capture_all, max_items=s.max_items)\n",
        "        obj.__dict__.update(s)\n",
        "        return obj\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class ProgressMonitor:\n",
        "    def __init__(self, tag=None, num_items=None, flush_interval=1000, verbose=False, progress_fn=None, pfn_lo=0, pfn_hi=1000, pfn_total=1000):\n",
        "        self.tag = tag\n",
        "        self.num_items = num_items\n",
        "        self.verbose = verbose\n",
        "        self.flush_interval = flush_interval\n",
        "        self.progress_fn = progress_fn\n",
        "        self.pfn_lo = pfn_lo\n",
        "        self.pfn_hi = pfn_hi\n",
        "        self.pfn_total = pfn_total\n",
        "        self.start_time = time.time()\n",
        "        self.batch_time = self.start_time\n",
        "        self.batch_items = 0\n",
        "        if self.progress_fn is not None:\n",
        "            self.progress_fn(self.pfn_lo, self.pfn_total)\n",
        "\n",
        "    def update(self, cur_items):\n",
        "        assert (self.num_items is None) or (cur_items <= self.num_items)\n",
        "        if (cur_items < self.batch_items + self.flush_interval) and (self.num_items is None or cur_items < self.num_items):\n",
        "            return\n",
        "        cur_time = time.time()\n",
        "        total_time = cur_time - self.start_time\n",
        "        time_per_item = (cur_time - self.batch_time) / max(cur_items - self.batch_items, 1)\n",
        "        if (self.verbose) and (self.tag is not None):\n",
        "            print(f'{self.tag:<19s} items {cur_items:<7d} time {format_time(total_time):<12s} ms/item {time_per_item*1e3:.2f}')\n",
        "        self.batch_time = cur_time\n",
        "        self.batch_items = cur_items\n",
        "\n",
        "        if (self.progress_fn is not None) and (self.num_items is not None):\n",
        "            self.progress_fn(self.pfn_lo + (self.pfn_hi - self.pfn_lo) * (cur_items / self.num_items), self.pfn_total)\n",
        "\n",
        "    def sub(self, tag=None, num_items=None, flush_interval=1000, rel_lo=0, rel_hi=1):\n",
        "        return ProgressMonitor(\n",
        "            tag             = tag,\n",
        "            num_items       = num_items,\n",
        "            flush_interval  = flush_interval,\n",
        "            verbose         = self.verbose,\n",
        "            progress_fn     = self.progress_fn,\n",
        "            pfn_lo          = self.pfn_lo + (self.pfn_hi - self.pfn_lo) * rel_lo,\n",
        "            pfn_hi          = self.pfn_lo + (self.pfn_hi - self.pfn_lo) * rel_hi,\n",
        "            pfn_total       = self.pfn_total,\n",
        "        )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def compute_feature_stats_for_dataset(opts, detector_url, detector_kwargs, rel_lo=0, rel_hi=1, batch_size=64, data_loader_kwargs=None, max_items=None, **stats_kwargs):\n",
        "    dataset = construct_class_by_name(**opts.dataset_kwargs)\n",
        "    if data_loader_kwargs is None:\n",
        "        data_loader_kwargs = dict(pin_memory=True, num_workers=3, prefetch_factor=2)\n",
        "\n",
        "    # Try to lookup from cache.\n",
        "    cache_file = None\n",
        "    if opts.cache:\n",
        "        # Choose cache file name.\n",
        "        args = dict(dataset_kwargs=opts.dataset_kwargs, detector_url=detector_url, detector_kwargs=detector_kwargs, stats_kwargs=stats_kwargs)\n",
        "        md5 = hashlib.md5(repr(sorted(args.items())).encode('utf-8'))\n",
        "        cache_tag = f'{dataset.name}-{get_feature_detector_name(detector_url)}-{md5.hexdigest()}'\n",
        "        cache_file = make_cache_dir_path('gan-metrics', cache_tag + '.pkl')\n",
        "\n",
        "        # Check if the file exists (all processes must agree).\n",
        "        flag = os.path.isfile(cache_file) if opts.rank == 0 else False\n",
        "        if opts.num_gpus > 1:\n",
        "            flag = torch.as_tensor(flag, dtype=torch.float32, device=opts.device)\n",
        "            torch.distributed.broadcast(tensor=flag, src=0)\n",
        "            flag = (float(flag.cpu()) != 0)\n",
        "\n",
        "        # Load.\n",
        "        if flag:\n",
        "            return FeatureStats.load(cache_file)\n",
        "\n",
        "    # Initialize.\n",
        "    num_items = len(dataset)\n",
        "    if max_items is not None:\n",
        "        num_items = min(num_items, max_items)\n",
        "    stats = FeatureStats(max_items=num_items, **stats_kwargs)\n",
        "    progress = opts.progress.sub(tag='dataset features', num_items=num_items, rel_lo=rel_lo, rel_hi=rel_hi)\n",
        "    detector = get_feature_detector(url=detector_url, device=opts.device, num_gpus=opts.num_gpus, rank=opts.rank, verbose=progress.verbose)\n",
        "\n",
        "    # Main loop.\n",
        "    item_subset = [(i * opts.num_gpus + opts.rank) % num_items for i in range((num_items - 1) // opts.num_gpus + 1)]\n",
        "    for images, _labels in torch.utils.data.DataLoader(dataset=dataset, sampler=item_subset, batch_size=batch_size, **data_loader_kwargs):\n",
        "        if images.shape[1] == 1:\n",
        "            images = images.repeat([1, 3, 1, 1])\n",
        "        features = detector(images.to(opts.device), **detector_kwargs)\n",
        "        stats.append_torch(features, num_gpus=opts.num_gpus, rank=opts.rank)\n",
        "        progress.update(stats.num_items)\n",
        "\n",
        "    # Save to cache.\n",
        "    if cache_file is not None and opts.rank == 0:\n",
        "        os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
        "        temp_file = cache_file + '.' + uuid.uuid4().hex\n",
        "        stats.save(temp_file)\n",
        "        os.replace(temp_file, cache_file) # atomic\n",
        "    return stats\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def compute_feature_stats_for_generator(opts, detector_url, detector_kwargs, rel_lo=0, rel_hi=1, batch_size=64, batch_gen=None, **stats_kwargs):\n",
        "    if batch_gen is None:\n",
        "        batch_gen = min(batch_size, 4)\n",
        "    assert batch_size % batch_gen == 0\n",
        "\n",
        "    # Setup generator and labels.\n",
        "    G = copy.deepcopy(opts.G).eval().requires_grad_(False).to(opts.device)\n",
        "    c_iter = iterate_random_labels(opts=opts, batch_size=batch_gen)\n",
        "\n",
        "    # Initialize.\n",
        "    stats = FeatureStats(**stats_kwargs)\n",
        "    assert stats.max_items is not None\n",
        "    progress = opts.progress.sub(tag='generator features', num_items=stats.max_items, rel_lo=rel_lo, rel_hi=rel_hi)\n",
        "    detector = get_feature_detector(url=detector_url, device=opts.device, num_gpus=opts.num_gpus, rank=opts.rank, verbose=progress.verbose)\n",
        "\n",
        "    # Main loop.\n",
        "    while not stats.is_full():\n",
        "        images = []\n",
        "        for _i in range(batch_size // batch_gen):\n",
        "            z = torch.randn([batch_gen, G.z_dim], device=opts.device)\n",
        "            img = G(z=z, c=next(c_iter), **opts.G_kwargs)\n",
        "            img = (img * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "            images.append(img)\n",
        "        images = torch.cat(images)\n",
        "        if images.shape[1] == 1:\n",
        "            images = images.repeat([1, 3, 1, 1])\n",
        "        features = detector(images, **detector_kwargs)\n",
        "        stats.append_torch(features, num_gpus=opts.num_gpus, rank=opts.rank)\n",
        "        progress.update(stats.num_items)\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoscgYTdAbwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOo575AwAbtL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--u9F2_-zHGe"
      },
      "outputs": [],
      "source": [
        "#frechet\n",
        "#this is from: https://github.com/NVlabs/stylegan3/blob/main/metrics/frechet_inception_distance.py\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "from . import metric_utils\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def compute_fid(opts, max_real, num_gen):\n",
        "    # Direct TorchScript translation of http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
        "    detector_url = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/metrics/inception-2015-12-05.pkl'\n",
        "    detector_kwargs = dict(return_features=True) # Return raw features before the softmax layer.\n",
        "\n",
        "    mu_real, sigma_real = metric_utils.compute_feature_stats_for_dataset(\n",
        "        opts=opts, detector_url=detector_url, detector_kwargs=detector_kwargs,\n",
        "        rel_lo=0, rel_hi=0, capture_mean_cov=True, max_items=max_real).get_mean_cov()\n",
        "\n",
        "    mu_gen, sigma_gen = metric_utils.compute_feature_stats_for_generator(\n",
        "        opts=opts, detector_url=detector_url, detector_kwargs=detector_kwargs,\n",
        "        rel_lo=0, rel_hi=1, capture_mean_cov=True, max_items=num_gen).get_mean_cov()\n",
        "\n",
        "    if opts.rank != 0:\n",
        "        return float('nan')\n",
        "\n",
        "    m = np.square(mu_gen - mu_real).sum()\n",
        "    s, _ = scipy.linalg.sqrtm(np.dot(sigma_gen, sigma_real), disp=False) # pylint: disable=no-member\n",
        "    fid = np.real(m + np.trace(sigma_gen + sigma_real - s * 2))\n",
        "    return float(fid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aHo2rlOzjki"
      },
      "outputs": [],
      "source": [
        "#metric_main\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "_metric_dict = dict() # name => fn\n",
        "\n",
        "def register_metric(fn):\n",
        "    assert callable(fn)\n",
        "    _metric_dict[fn.__name__] = fn\n",
        "    return fn\n",
        "\n",
        "def is_valid_metric(metric):\n",
        "    return metric in _metric_dict\n",
        "\n",
        "def list_valid_metrics():\n",
        "    return list(_metric_dict.keys())\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def calc_metric(metric, **kwargs): # See metric_utils.MetricOptions for the full list of arguments.\n",
        "    assert is_valid_metric(metric)\n",
        "    opts = metric_utils.MetricOptions(**kwargs)\n",
        "\n",
        "    # Calculate.\n",
        "    start_time = time.time()\n",
        "    results = _metric_dict[metric](opts)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Broadcast results.\n",
        "    for key, value in list(results.items()):\n",
        "        if opts.num_gpus > 1:\n",
        "            value = torch.as_tensor(value, dtype=torch.float64, device=opts.device)\n",
        "            torch.distributed.broadcast(tensor=value, src=0)\n",
        "            value = float(value.cpu())\n",
        "        results[key] = value\n",
        "\n",
        "    # Decorate with metadata.\n",
        "    return EasyDict(\n",
        "        results         = EasyDict(results),\n",
        "        metric          = metric,\n",
        "        total_time      = total_time,\n",
        "        total_time_str  = format_time(total_time),\n",
        "        num_gpus        = opts.num_gpus,\n",
        "    )\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def report_metric(result_dict, run_dir=None, snapshot_pkl=None):\n",
        "    metric = result_dict['metric']\n",
        "    assert is_valid_metric(metric)\n",
        "    if run_dir is not None and snapshot_pkl is not None:\n",
        "        snapshot_pkl = os.path.relpath(snapshot_pkl, run_dir)\n",
        "\n",
        "    jsonl_line = json.dumps(dict(result_dict, snapshot_pkl=snapshot_pkl, timestamp=time.time()))\n",
        "    print(jsonl_line)\n",
        "    if run_dir is not None and os.path.isdir(run_dir):\n",
        "        with open(os.path.join(run_dir, f'metric-{metric}.jsonl'), 'at') as f:\n",
        "            f.write(jsonl_line + '\\n')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Recommended metrics.\n",
        "\n",
        "@register_metric\n",
        "def fid50k_full(opts):\n",
        "    opts.dataset_kwargs.update(max_size=None, xflip=False)\n",
        "    fid = compute_fid(opts, max_real=None, num_gen=50000)\n",
        "    return dict(fid50k_full=fid)\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Legacy metrics.\n",
        "\n",
        "@register_metric\n",
        "def fid50k(opts):\n",
        "    opts.dataset_kwargs.update(max_size=None)\n",
        "    fid = compute_fid(opts, max_real=50000, num_gen=50000)\n",
        "    return dict(fid50k=fid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3E4cISOAyrx"
      },
      "outputs": [],
      "source": [
        "#this is from: https://github.com/NVlabs/stylegan3/blob/main/training/networks_stylegan3.py\n",
        "#network stylegan3\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import scipy.optimize\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "from torch_utils import persistence\n",
        "from torch_utils.ops import conv2d_gradfix\n",
        "from torch_utils.ops import filtered_lrelu\n",
        "from torch_utils.ops import bias_act\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def modulated_conv2d(\n",
        "    x,                  # Input tensor: [batch_size, in_channels, in_height, in_width]\n",
        "    w,                  # Weight tensor: [out_channels, in_channels, kernel_height, kernel_width]\n",
        "    s,                  # Style tensor: [batch_size, in_channels]\n",
        "    demodulate  = True, # Apply weight demodulation?\n",
        "    padding     = 0,    # Padding: int or [padH, padW]\n",
        "    input_gain  = None, # Optional scale factors for the input channels: [], [in_channels], or [batch_size, in_channels]\n",
        "):\n",
        "    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "        batch_size = int(x.shape[0])\n",
        "    out_channels, in_channels, kh, kw = w.shape\n",
        "    misc.assert_shape(w, [out_channels, in_channels, kh, kw]) # [OIkk]\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None]) # [NIHW]\n",
        "    misc.assert_shape(s, [batch_size, in_channels]) # [NI]\n",
        "\n",
        "    # Pre-normalize inputs.\n",
        "    if demodulate:\n",
        "        w = w * w.square().mean([1,2,3], keepdim=True).rsqrt()\n",
        "        s = s * s.square().mean().rsqrt()\n",
        "\n",
        "    # Modulate weights.\n",
        "    w = w.unsqueeze(0) # [NOIkk]\n",
        "    w = w * s.unsqueeze(1).unsqueeze(3).unsqueeze(4) # [NOIkk]\n",
        "\n",
        "    # Demodulate weights.\n",
        "    if demodulate:\n",
        "        dcoefs = (w.square().sum(dim=[2,3,4]) + 1e-8).rsqrt() # [NO]\n",
        "        w = w * dcoefs.unsqueeze(2).unsqueeze(3).unsqueeze(4) # [NOIkk]\n",
        "\n",
        "    # Apply input scaling.\n",
        "    if input_gain is not None:\n",
        "        input_gain = input_gain.expand(batch_size, in_channels) # [NI]\n",
        "        w = w * input_gain.unsqueeze(1).unsqueeze(3).unsqueeze(4) # [NOIkk]\n",
        "\n",
        "    # Execute as one fused op using grouped convolution.\n",
        "    x = x.reshape(1, -1, *x.shape[2:])\n",
        "    w = w.reshape(-1, in_channels, kh, kw)\n",
        "    x = conv2d_gradfix.conv2d(input=x, weight=w.to(x.dtype), padding=padding, groups=batch_size)\n",
        "    x = x.reshape(batch_size, -1, *x.shape[2:])\n",
        "    return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class FullyConnectedLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_features,                # Number of input features.\n",
        "        out_features,               # Number of output features.\n",
        "        activation      = 'linear', # Activation function: 'relu', 'lrelu', etc.\n",
        "        bias            = True,     # Apply additive bias before the activation function?\n",
        "        lr_multiplier   = 1,        # Learning rate multiplier.\n",
        "        weight_init     = 1,        # Initial standard deviation of the weight tensor.\n",
        "        bias_init       = 0,        # Initial value of the additive bias.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.activation = activation\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) * (weight_init / lr_multiplier))\n",
        "        bias_init = np.broadcast_to(np.asarray(bias_init, dtype=np.float32), [out_features])\n",
        "        self.bias = torch.nn.Parameter(torch.from_numpy(bias_init / lr_multiplier)) if bias else None\n",
        "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
        "        self.bias_gain = lr_multiplier\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight.to(x.dtype) * self.weight_gain\n",
        "        b = self.bias\n",
        "        if b is not None:\n",
        "            b = b.to(x.dtype)\n",
        "            if self.bias_gain != 1:\n",
        "                b = b * self.bias_gain\n",
        "        if self.activation == 'linear' and b is not None:\n",
        "            x = torch.addmm(b.unsqueeze(0), x, w.t())\n",
        "        else:\n",
        "            x = x.matmul(w.t())\n",
        "            x = bias_act.bias_act(x, b, act=self.activation)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'in_features={self.in_features:d}, out_features={self.out_features:d}, activation={self.activation:s}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MappingNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality, 0 = no labels.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        num_ws,                     # Number of intermediate latents to output.\n",
        "        num_layers      = 2,        # Number of mapping layers.\n",
        "        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n",
        "        w_avg_beta      = 0.998,    # Decay for tracking the moving average of W during training.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_ws\n",
        "        self.num_layers = num_layers\n",
        "        self.w_avg_beta = w_avg_beta\n",
        "\n",
        "        # Construct layers.\n",
        "        self.embed = FullyConnectedLayer(self.c_dim, self.w_dim) if self.c_dim > 0 else None\n",
        "        features = [self.z_dim + (self.w_dim if self.c_dim > 0 else 0)] + [self.w_dim] * self.num_layers\n",
        "        for idx, in_features, out_features in zip(range(num_layers), features[:-1], features[1:]):\n",
        "            layer = FullyConnectedLayer(in_features, out_features, activation='lrelu', lr_multiplier=lr_multiplier)\n",
        "            setattr(self, f'fc{idx}', layer)\n",
        "        self.register_buffer('w_avg', torch.zeros([w_dim]))\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n",
        "        misc.assert_shape(z, [None, self.z_dim])\n",
        "        if truncation_cutoff is None:\n",
        "            truncation_cutoff = self.num_ws\n",
        "\n",
        "        # Embed, normalize, and concatenate inputs.\n",
        "        x = z.to(torch.float32)\n",
        "        x = x * (x.square().mean(1, keepdim=True) + 1e-8).rsqrt()\n",
        "        if self.c_dim > 0:\n",
        "            misc.assert_shape(c, [None, self.c_dim])\n",
        "            y = self.embed(c.to(torch.float32))\n",
        "            y = y * (y.square().mean(1, keepdim=True) + 1e-8).rsqrt()\n",
        "            x = torch.cat([x, y], dim=1) if x is not None else y\n",
        "\n",
        "        # Execute layers.\n",
        "        for idx in range(self.num_layers):\n",
        "            x = getattr(self, f'fc{idx}')(x)\n",
        "\n",
        "        # Update moving average of W.\n",
        "        if update_emas:\n",
        "            self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n",
        "\n",
        "        # Broadcast and apply truncation.\n",
        "        x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n",
        "        if truncation_psi != 1:\n",
        "            x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'z_dim={self.z_dim:d}, c_dim={self.c_dim:d}, w_dim={self.w_dim:d}, num_ws={self.num_ws:d}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisInput(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        w_dim,          # Intermediate latent (W) dimensionality.\n",
        "        channels,       # Number of output channels.\n",
        "        size,           # Output spatial size: int or [width, height].\n",
        "        sampling_rate,  # Output sampling rate.\n",
        "        bandwidth,      # Output bandwidth.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.w_dim = w_dim\n",
        "        self.channels = channels\n",
        "        self.size = np.broadcast_to(np.asarray(size), [2])\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.bandwidth = bandwidth\n",
        "\n",
        "        # Draw random frequencies from uniform 2D disc.\n",
        "        freqs = torch.randn([self.channels, 2])\n",
        "        radii = freqs.square().sum(dim=1, keepdim=True).sqrt()\n",
        "        freqs /= radii * radii.square().exp().pow(0.25)\n",
        "        freqs *= bandwidth\n",
        "        phases = torch.rand([self.channels]) - 0.5\n",
        "\n",
        "        # Setup parameters and buffers.\n",
        "        self.weight = torch.nn.Parameter(torch.randn([self.channels, self.channels]))\n",
        "        self.affine = FullyConnectedLayer(w_dim, 4, weight_init=0, bias_init=[1,0,0,0])\n",
        "        self.register_buffer('transform', torch.eye(3, 3)) # User-specified inverse transform wrt. resulting image.\n",
        "        self.register_buffer('freqs', freqs)\n",
        "        self.register_buffer('phases', phases)\n",
        "\n",
        "    def forward(self, w):\n",
        "        # Introduce batch dimension.\n",
        "        transforms = self.transform.unsqueeze(0) # [batch, row, col]\n",
        "        freqs = self.freqs.unsqueeze(0) # [batch, channel, xy]\n",
        "        phases = self.phases.unsqueeze(0) # [batch, channel]\n",
        "\n",
        "        # Apply learned transformation.\n",
        "        t = self.affine(w) # t = (r_c, r_s, t_x, t_y)\n",
        "        t = t / t[:, :2].norm(dim=1, keepdim=True) # t' = (r'_c, r'_s, t'_x, t'_y)\n",
        "        m_r = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1]) # Inverse rotation wrt. resulting image.\n",
        "        m_r[:, 0, 0] = t[:, 0]  # r'_c\n",
        "        m_r[:, 0, 1] = -t[:, 1] # r'_s\n",
        "        m_r[:, 1, 0] = t[:, 1]  # r'_s\n",
        "        m_r[:, 1, 1] = t[:, 0]  # r'_c\n",
        "        m_t = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1]) # Inverse translation wrt. resulting image.\n",
        "        m_t[:, 0, 2] = -t[:, 2] # t'_x\n",
        "        m_t[:, 1, 2] = -t[:, 3] # t'_y\n",
        "        transforms = m_r @ m_t @ transforms # First rotate resulting image, then translate, and finally apply user-specified transform.\n",
        "\n",
        "        # Transform frequencies.\n",
        "        phases = phases + (freqs @ transforms[:, :2, 2:]).squeeze(2)\n",
        "        freqs = freqs @ transforms[:, :2, :2]\n",
        "\n",
        "        # Dampen out-of-band frequencies that may occur due to the user-specified transform.\n",
        "        amplitudes = (1 - (freqs.norm(dim=2) - self.bandwidth) / (self.sampling_rate / 2 - self.bandwidth)).clamp(0, 1)\n",
        "\n",
        "        # Construct sampling grid.\n",
        "        theta = torch.eye(2, 3, device=w.device)\n",
        "        theta[0, 0] = 0.5 * self.size[0] / self.sampling_rate\n",
        "        theta[1, 1] = 0.5 * self.size[1] / self.sampling_rate\n",
        "        grids = torch.nn.functional.affine_grid(theta.unsqueeze(0), [1, 1, self.size[1], self.size[0]], align_corners=False)\n",
        "\n",
        "        # Compute Fourier features.\n",
        "        x = (grids.unsqueeze(3) @ freqs.permute(0, 2, 1).unsqueeze(1).unsqueeze(2)).squeeze(3) # [batch, height, width, channel]\n",
        "        x = x + phases.unsqueeze(1).unsqueeze(2)\n",
        "        x = torch.sin(x * (np.pi * 2))\n",
        "        x = x * amplitudes.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Apply trainable mapping.\n",
        "        weight = self.weight / np.sqrt(self.channels)\n",
        "        x = x @ weight.t()\n",
        "\n",
        "        # Ensure correct shape.\n",
        "        x = x.permute(0, 3, 1, 2) # [batch, channel, height, width]\n",
        "        misc.assert_shape(x, [w.shape[0], self.channels, int(self.size[1]), int(self.size[0])])\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '\\n'.join([\n",
        "            f'w_dim={self.w_dim:d}, channels={self.channels:d}, size={list(self.size)},',\n",
        "            f'sampling_rate={self.sampling_rate:g}, bandwidth={self.bandwidth:g}'])\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        w_dim,                          # Intermediate latent (W) dimensionality.\n",
        "        is_torgb,                       # Is this the final ToRGB layer?\n",
        "        is_critically_sampled,          # Does this layer use critical sampling?\n",
        "        use_fp16,                       # Does this layer use FP16?\n",
        "\n",
        "        # Input & output specifications.\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        in_size,                        # Input spatial size: int or [width, height].\n",
        "        out_size,                       # Output spatial size: int or [width, height].\n",
        "        in_sampling_rate,               # Input sampling rate (s).\n",
        "        out_sampling_rate,              # Output sampling rate (s).\n",
        "        in_cutoff,                      # Input cutoff frequency (f_c).\n",
        "        out_cutoff,                     # Output cutoff frequency (f_c).\n",
        "        in_half_width,                  # Input transition band half-width (f_h).\n",
        "        out_half_width,                 # Output Transition band half-width (f_h).\n",
        "\n",
        "        # Hyperparameters.\n",
        "        conv_kernel         = 3,        # Convolution kernel size. Ignored for final the ToRGB layer.\n",
        "        filter_size         = 6,        # Low-pass filter size relative to the lower resolution when up/downsampling.\n",
        "        lrelu_upsampling    = 2,        # Relative sampling rate for leaky ReLU. Ignored for final the ToRGB layer.\n",
        "        use_radial_filters  = False,    # Use radially symmetric downsampling filter? Ignored for critically sampled layers.\n",
        "        conv_clamp          = 256,      # Clamp the output to [-X, +X], None = disable clamping.\n",
        "        magnitude_ema_beta  = 0.999,    # Decay rate for the moving average of input magnitudes.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.w_dim = w_dim\n",
        "        self.is_torgb = is_torgb\n",
        "        self.is_critically_sampled = is_critically_sampled\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.in_size = np.broadcast_to(np.asarray(in_size), [2])\n",
        "        self.out_size = np.broadcast_to(np.asarray(out_size), [2])\n",
        "        self.in_sampling_rate = in_sampling_rate\n",
        "        self.out_sampling_rate = out_sampling_rate\n",
        "        self.tmp_sampling_rate = max(in_sampling_rate, out_sampling_rate) * (1 if is_torgb else lrelu_upsampling)\n",
        "        self.in_cutoff = in_cutoff\n",
        "        self.out_cutoff = out_cutoff\n",
        "        self.in_half_width = in_half_width\n",
        "        self.out_half_width = out_half_width\n",
        "        self.conv_kernel = 1 if is_torgb else conv_kernel\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.magnitude_ema_beta = magnitude_ema_beta\n",
        "\n",
        "        # Setup parameters and buffers.\n",
        "        self.affine = FullyConnectedLayer(self.w_dim, self.in_channels, bias_init=1)\n",
        "        self.weight = torch.nn.Parameter(torch.randn([self.out_channels, self.in_channels, self.conv_kernel, self.conv_kernel]))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([self.out_channels]))\n",
        "        self.register_buffer('magnitude_ema', torch.ones([]))\n",
        "\n",
        "        # Design upsampling filter.\n",
        "        self.up_factor = int(np.rint(self.tmp_sampling_rate / self.in_sampling_rate))\n",
        "        assert self.in_sampling_rate * self.up_factor == self.tmp_sampling_rate\n",
        "        self.up_taps = filter_size * self.up_factor if self.up_factor > 1 and not self.is_torgb else 1\n",
        "        self.register_buffer('up_filter', self.design_lowpass_filter(\n",
        "            numtaps=self.up_taps, cutoff=self.in_cutoff, width=self.in_half_width*2, fs=self.tmp_sampling_rate))\n",
        "\n",
        "        # Design downsampling filter.\n",
        "        self.down_factor = int(np.rint(self.tmp_sampling_rate / self.out_sampling_rate))\n",
        "        assert self.out_sampling_rate * self.down_factor == self.tmp_sampling_rate\n",
        "        self.down_taps = filter_size * self.down_factor if self.down_factor > 1 and not self.is_torgb else 1\n",
        "        self.down_radial = use_radial_filters and not self.is_critically_sampled\n",
        "        self.register_buffer('down_filter', self.design_lowpass_filter(\n",
        "            numtaps=self.down_taps, cutoff=self.out_cutoff, width=self.out_half_width*2, fs=self.tmp_sampling_rate, radial=self.down_radial))\n",
        "\n",
        "        # Compute padding.\n",
        "        pad_total = (self.out_size - 1) * self.down_factor + 1 # Desired output size before downsampling.\n",
        "        pad_total -= (self.in_size + self.conv_kernel - 1) * self.up_factor # Input size after upsampling.\n",
        "        pad_total += self.up_taps + self.down_taps - 2 # Size reduction caused by the filters.\n",
        "        pad_lo = (pad_total + self.up_factor) // 2 # Shift sample locations according to the symmetric interpretation (Appendix C.3).\n",
        "        pad_hi = pad_total - pad_lo\n",
        "        self.padding = [int(pad_lo[0]), int(pad_hi[0]), int(pad_lo[1]), int(pad_hi[1])]\n",
        "\n",
        "    def forward(self, x, w, noise_mode='random', force_fp32=False, update_emas=False):\n",
        "        assert noise_mode in ['random', 'const', 'none'] # unused\n",
        "        misc.assert_shape(x, [None, self.in_channels, int(self.in_size[1]), int(self.in_size[0])])\n",
        "        misc.assert_shape(w, [x.shape[0], self.w_dim])\n",
        "\n",
        "        # Track input magnitude.\n",
        "        if update_emas:\n",
        "            with torch.autograd.profiler.record_function('update_magnitude_ema'):\n",
        "                magnitude_cur = x.detach().to(torch.float32).square().mean()\n",
        "                self.magnitude_ema.copy_(magnitude_cur.lerp(self.magnitude_ema, self.magnitude_ema_beta))\n",
        "        input_gain = self.magnitude_ema.rsqrt()\n",
        "\n",
        "        # Execute affine layer.\n",
        "        styles = self.affine(w)\n",
        "        if self.is_torgb:\n",
        "            weight_gain = 1 / np.sqrt(self.in_channels * (self.conv_kernel ** 2))\n",
        "            styles = styles * weight_gain\n",
        "\n",
        "        # Execute modulated conv2d.\n",
        "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n",
        "        x = modulated_conv2d(x=x.to(dtype), w=self.weight, s=styles,\n",
        "            padding=self.conv_kernel-1, demodulate=(not self.is_torgb), input_gain=input_gain)\n",
        "\n",
        "        # Execute bias, filtered leaky ReLU, and clamping.\n",
        "        gain = 1 if self.is_torgb else np.sqrt(2)\n",
        "        slope = 1 if self.is_torgb else 0.2\n",
        "        x = filtered_lrelu.filtered_lrelu(x=x, fu=self.up_filter, fd=self.down_filter, b=self.bias.to(x.dtype),\n",
        "            up=self.up_factor, down=self.down_factor, padding=self.padding, gain=gain, slope=slope, clamp=self.conv_clamp)\n",
        "\n",
        "        # Ensure correct shape and dtype.\n",
        "        misc.assert_shape(x, [None, self.out_channels, int(self.out_size[1]), int(self.out_size[0])])\n",
        "        assert x.dtype == dtype\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def design_lowpass_filter(numtaps, cutoff, width, fs, radial=False):\n",
        "        assert numtaps >= 1\n",
        "\n",
        "        # Identity filter.\n",
        "        if numtaps == 1:\n",
        "            return None\n",
        "\n",
        "        # Separable Kaiser low-pass filter.\n",
        "        if not radial:\n",
        "            f = scipy.signal.firwin(numtaps=numtaps, cutoff=cutoff, width=width, fs=fs)\n",
        "            return torch.as_tensor(f, dtype=torch.float32)\n",
        "\n",
        "        # Radially symmetric jinc-based filter.\n",
        "        x = (np.arange(numtaps) - (numtaps - 1) / 2) / fs\n",
        "        r = np.hypot(*np.meshgrid(x, x))\n",
        "        f = scipy.special.j1(2 * cutoff * (np.pi * r)) / (np.pi * r)\n",
        "        beta = scipy.signal.kaiser_beta(scipy.signal.kaiser_atten(numtaps, width / (fs / 2)))\n",
        "        w = np.kaiser(numtaps, beta)\n",
        "        f *= np.outer(w, w)\n",
        "        f /= np.sum(f)\n",
        "        return torch.as_tensor(f, dtype=torch.float32)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '\\n'.join([\n",
        "            f'w_dim={self.w_dim:d}, is_torgb={self.is_torgb},',\n",
        "            f'is_critically_sampled={self.is_critically_sampled}, use_fp16={self.use_fp16},',\n",
        "            f'in_sampling_rate={self.in_sampling_rate:g}, out_sampling_rate={self.out_sampling_rate:g},',\n",
        "            f'in_cutoff={self.in_cutoff:g}, out_cutoff={self.out_cutoff:g},',\n",
        "            f'in_half_width={self.in_half_width:g}, out_half_width={self.out_half_width:g},',\n",
        "            f'in_size={list(self.in_size)}, out_size={list(self.out_size)},',\n",
        "            f'in_channels={self.in_channels:d}, out_channels={self.out_channels:d}'])\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        w_dim,                          # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,                 # Output image resolution.\n",
        "        img_channels,                   # Number of color channels.\n",
        "        channel_base        = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max         = 512,      # Maximum number of channels in any layer.\n",
        "        num_layers          = 14,       # Total number of layers, excluding Fourier features and ToRGB.\n",
        "        num_critical        = 2,        # Number of critically sampled layers at the end.\n",
        "        first_cutoff        = 2,        # Cutoff frequency of the first layer (f_{c,0}).\n",
        "        first_stopband      = 2**2.1,   # Minimum stopband of the first layer (f_{t,0}).\n",
        "        last_stopband_rel   = 2**0.3,   # Minimum stopband of the last layer, expressed relative to the cutoff.\n",
        "        margin_size         = 10,       # Number of additional pixels outside the image.\n",
        "        output_scale        = 0.25,     # Scale factor for the output image.\n",
        "        num_fp16_res        = 4,        # Use FP16 for the N highest resolutions.\n",
        "        **layer_kwargs,                 # Arguments for SynthesisLayer.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_layers + 2\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.num_critical = num_critical\n",
        "        self.margin_size = margin_size\n",
        "        self.output_scale = output_scale\n",
        "        self.num_fp16_res = num_fp16_res\n",
        "\n",
        "        # Geometric progression of layer cutoffs and min. stopbands.\n",
        "        last_cutoff = self.img_resolution / 2 # f_{c,N}\n",
        "        last_stopband = last_cutoff * last_stopband_rel # f_{t,N}\n",
        "        exponents = np.minimum(np.arange(self.num_layers + 1) / (self.num_layers - self.num_critical), 1)\n",
        "        cutoffs = first_cutoff * (last_cutoff / first_cutoff) ** exponents # f_c[i]\n",
        "        stopbands = first_stopband * (last_stopband / first_stopband) ** exponents # f_t[i]\n",
        "\n",
        "        # Compute remaining layer parameters.\n",
        "        sampling_rates = np.exp2(np.ceil(np.log2(np.minimum(stopbands * 2, self.img_resolution)))) # s[i]\n",
        "        half_widths = np.maximum(stopbands, sampling_rates / 2) - cutoffs # f_h[i]\n",
        "        sizes = sampling_rates + self.margin_size * 2\n",
        "        sizes[-2:] = self.img_resolution\n",
        "        channels = np.rint(np.minimum((channel_base / 2) / cutoffs, channel_max))\n",
        "        channels[-1] = self.img_channels\n",
        "\n",
        "        # Construct layers.\n",
        "        self.input = SynthesisInput(\n",
        "            w_dim=self.w_dim, channels=int(channels[0]), size=int(sizes[0]),\n",
        "            sampling_rate=sampling_rates[0], bandwidth=cutoffs[0])\n",
        "        self.layer_names = []\n",
        "        for idx in range(self.num_layers + 1):\n",
        "            prev = max(idx - 1, 0)\n",
        "            is_torgb = (idx == self.num_layers)\n",
        "            is_critically_sampled = (idx >= self.num_layers - self.num_critical)\n",
        "            use_fp16 = (sampling_rates[idx] * (2 ** self.num_fp16_res) > self.img_resolution)\n",
        "            layer = SynthesisLayer(\n",
        "                w_dim=self.w_dim, is_torgb=is_torgb, is_critically_sampled=is_critically_sampled, use_fp16=use_fp16,\n",
        "                in_channels=int(channels[prev]), out_channels= int(channels[idx]),\n",
        "                in_size=int(sizes[prev]), out_size=int(sizes[idx]),\n",
        "                in_sampling_rate=int(sampling_rates[prev]), out_sampling_rate=int(sampling_rates[idx]),\n",
        "                in_cutoff=cutoffs[prev], out_cutoff=cutoffs[idx],\n",
        "                in_half_width=half_widths[prev], out_half_width=half_widths[idx],\n",
        "                **layer_kwargs)\n",
        "            name = f'L{idx}_{layer.out_size[0]}_{layer.out_channels}'\n",
        "            setattr(self, name, layer)\n",
        "            self.layer_names.append(name)\n",
        "\n",
        "    def forward(self, ws, **layer_kwargs):\n",
        "        misc.assert_shape(ws, [None, self.num_ws, self.w_dim])\n",
        "        ws = ws.to(torch.float32).unbind(dim=1)\n",
        "\n",
        "        # Execute layers.\n",
        "        x = self.input(ws[0])\n",
        "        for name, w in zip(self.layer_names, ws[1:]):\n",
        "            x = getattr(self, name)(x, w, **layer_kwargs)\n",
        "        if self.output_scale != 1:\n",
        "            x = x * self.output_scale\n",
        "\n",
        "        # Ensure correct shape and dtype.\n",
        "        misc.assert_shape(x, [None, self.img_channels, self.img_resolution, self.img_resolution])\n",
        "        x = x.to(torch.float32)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '\\n'.join([\n",
        "            f'w_dim={self.w_dim:d}, num_ws={self.num_ws:d},',\n",
        "            f'img_resolution={self.img_resolution:d}, img_channels={self.img_channels:d},',\n",
        "            f'num_layers={self.num_layers:d}, num_critical={self.num_critical:d},',\n",
        "            f'margin_size={self.margin_size:d}, num_fp16_res={self.num_fp16_res:d}'])\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Generator3(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output resolution.\n",
        "        img_channels,               # Number of output color channels.\n",
        "        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n",
        "        **synthesis_kwargs,         # Arguments for SynthesisNetwork.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.synthesis = SynthesisNetwork(w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels, **synthesis_kwargs)\n",
        "        self.num_ws = self.synthesis.num_ws\n",
        "        self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dim, w_dim=w_dim, num_ws=self.num_ws, **mapping_kwargs)\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, update_emas=False, **synthesis_kwargs):\n",
        "        ws = self.mapping(z, c, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n",
        "        img = self.synthesis(ws, update_emas=update_emas, **synthesis_kwargs)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks8g_WUqAr-n"
      },
      "outputs": [],
      "source": [
        "#this is from: https://github.com/NVlabs/stylegan3/blob/main/training/networks_stylegan2.py\n",
        "#network stylegan2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "from torch_utils import persistence\n",
        "from torch_utils.ops import conv2d_resample\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from torch_utils.ops import bias_act\n",
        "from torch_utils.ops import fma\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def normalize_2nd_moment(x, dim=1, eps=1e-8):\n",
        "    return x * (x.square().mean(dim=dim, keepdim=True) + eps).rsqrt()\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@misc.profiled_function\n",
        "def modulated_conv2d(\n",
        "    x,                          # Input tensor of shape [batch_size, in_channels, in_height, in_width].\n",
        "    weight,                     # Weight tensor of shape [out_channels, in_channels, kernel_height, kernel_width].\n",
        "    styles,                     # Modulation coefficients of shape [batch_size, in_channels].\n",
        "    noise           = None,     # Optional noise tensor to add to the output activations.\n",
        "    up              = 1,        # Integer upsampling factor.\n",
        "    down            = 1,        # Integer downsampling factor.\n",
        "    padding         = 0,        # Padding with respect to the upsampled image.\n",
        "    resample_filter = None,     # Low-pass filter to apply when resampling activations. Must be prepared beforehand by calling upfirdn2d.setup_filter().\n",
        "    demodulate      = True,     # Apply weight demodulation?\n",
        "    flip_weight     = True,     # False = convolution, True = correlation (matches torch.nn.functional.conv2d).\n",
        "    fused_modconv   = True,     # Perform modulation, convolution, and demodulation as a single fused operation?\n",
        "):\n",
        "    batch_size = x.shape[0]\n",
        "    out_channels, in_channels, kh, kw = weight.shape\n",
        "    misc.assert_shape(weight, [out_channels, in_channels, kh, kw]) # [OIkk]\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None]) # [NIHW]\n",
        "    misc.assert_shape(styles, [batch_size, in_channels]) # [NI]\n",
        "\n",
        "    # Pre-normalize inputs to avoid FP16 overflow.\n",
        "    if x.dtype == torch.float16 and demodulate:\n",
        "        weight = weight * (1 / np.sqrt(in_channels * kh * kw) / weight.norm(float('inf'), dim=[1,2,3], keepdim=True)) # max_Ikk\n",
        "        styles = styles / styles.norm(float('inf'), dim=1, keepdim=True) # max_I\n",
        "\n",
        "    # Calculate per-sample weights and demodulation coefficients.\n",
        "    w = None\n",
        "    dcoefs = None\n",
        "    if demodulate or fused_modconv:\n",
        "        w = weight.unsqueeze(0) # [NOIkk]\n",
        "        w = w * styles.reshape(batch_size, 1, -1, 1, 1) # [NOIkk]\n",
        "    if demodulate:\n",
        "        dcoefs = (w.square().sum(dim=[2,3,4]) + 1e-8).rsqrt() # [NO]\n",
        "    if demodulate and fused_modconv:\n",
        "        w = w * dcoefs.reshape(batch_size, -1, 1, 1, 1) # [NOIkk]\n",
        "\n",
        "    # Execute by scaling the activations before and after the convolution.\n",
        "    if not fused_modconv:\n",
        "        x = x * styles.to(x.dtype).reshape(batch_size, -1, 1, 1)\n",
        "        x = conv2d_resample.conv2d_resample(x=x, w=weight.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, flip_weight=flip_weight)\n",
        "        if demodulate and noise is not None:\n",
        "            x = fma.fma(x, dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1), noise.to(x.dtype))\n",
        "        elif demodulate:\n",
        "            x = x * dcoefs.to(x.dtype).reshape(batch_size, -1, 1, 1)\n",
        "        elif noise is not None:\n",
        "            x = x.add_(noise.to(x.dtype))\n",
        "        return x\n",
        "\n",
        "    # Execute as one fused op using grouped convolution.\n",
        "    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n",
        "        batch_size = int(batch_size)\n",
        "    misc.assert_shape(x, [batch_size, in_channels, None, None])\n",
        "    x = x.reshape(1, -1, *x.shape[2:])\n",
        "    w = w.reshape(-1, in_channels, kh, kw)\n",
        "    x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=resample_filter, up=up, down=down, padding=padding, groups=batch_size, flip_weight=flip_weight)\n",
        "    x = x.reshape(batch_size, -1, *x.shape[2:])\n",
        "    if noise is not None:\n",
        "        x = x.add_(noise)\n",
        "    return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class FullyConnectedLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_features,                # Number of input features.\n",
        "        out_features,               # Number of output features.\n",
        "        bias            = True,     # Apply additive bias before the activation function?\n",
        "        activation      = 'linear', # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 1,        # Learning rate multiplier.\n",
        "        bias_init       = 0,        # Initial value for the additive bias.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.activation = activation\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) / lr_multiplier)\n",
        "        self.bias = torch.nn.Parameter(torch.full([out_features], np.float32(bias_init))) if bias else None\n",
        "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
        "        self.bias_gain = lr_multiplier\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight.to(x.dtype) * self.weight_gain\n",
        "        b = self.bias\n",
        "        if b is not None:\n",
        "            b = b.to(x.dtype)\n",
        "            if self.bias_gain != 1:\n",
        "                b = b * self.bias_gain\n",
        "\n",
        "        if self.activation == 'linear' and b is not None:\n",
        "            x = torch.addmm(b.unsqueeze(0), x, w.t())\n",
        "        else:\n",
        "            x = x.matmul(w.t())\n",
        "            x = bias_act.bias_act(x, b, act=self.activation)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'in_features={self.in_features:d}, out_features={self.out_features:d}, activation={self.activation:s}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Conv2dLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        kernel_size,                    # Width and height of the convolution kernel.\n",
        "        bias            = True,         # Apply additive bias before the activation function?\n",
        "        activation      = 'linear',     # Activation function: 'relu', 'lrelu', etc.\n",
        "        up              = 1,            # Integer upsampling factor.\n",
        "        down            = 1,            # Integer downsampling factor.\n",
        "        resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp      = None,         # Clamp the output to +-X, None = disable clamping.\n",
        "        channels_last   = False,        # Expect the input to have memory_format=channels_last?\n",
        "        trainable       = True,         # Update the weights of this layer during training?\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.activation = activation\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.padding = kernel_size // 2\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "        self.act_gain = bias_act.activation_funcs[activation].def_gain\n",
        "\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        weight = torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format)\n",
        "        bias = torch.zeros([out_channels]) if bias else None\n",
        "        if trainable:\n",
        "            self.weight = torch.nn.Parameter(weight)\n",
        "            self.bias = torch.nn.Parameter(bias) if bias is not None else None\n",
        "        else:\n",
        "            self.register_buffer('weight', weight)\n",
        "            if bias is not None:\n",
        "                self.register_buffer('bias', bias)\n",
        "            else:\n",
        "                self.bias = None\n",
        "\n",
        "    def forward(self, x, gain=1):\n",
        "        w = self.weight * self.weight_gain\n",
        "        b = self.bias.to(x.dtype) if self.bias is not None else None\n",
        "        flip_weight = (self.up == 1) # slightly faster\n",
        "        x = conv2d_resample.conv2d_resample(x=x, w=w.to(x.dtype), f=self.resample_filter, up=self.up, down=self.down, padding=self.padding, flip_weight=flip_weight)\n",
        "\n",
        "        act_gain = self.act_gain * gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, b, act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return ' '.join([\n",
        "            f'in_channels={self.in_channels:d}, out_channels={self.out_channels:d}, activation={self.activation:s},',\n",
        "            f'up={self.up}, down={self.down}'])\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MappingNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        num_ws,                     # Number of intermediate latents to output, None = do not broadcast.\n",
        "        num_layers      = 8,        # Number of mapping layers.\n",
        "        embed_features  = None,     # Label embedding dimensionality, None = same as w_dim.\n",
        "        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n",
        "        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n",
        "        w_avg_beta      = 0.998,    # Decay for tracking the moving average of W during training, None = do not track.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_ws\n",
        "        self.num_layers = num_layers\n",
        "        self.w_avg_beta = w_avg_beta\n",
        "\n",
        "        if embed_features is None:\n",
        "            embed_features = w_dim\n",
        "        if c_dim == 0:\n",
        "            embed_features = 0\n",
        "        if layer_features is None:\n",
        "            layer_features = w_dim\n",
        "        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.embed = FullyConnectedLayer(c_dim, embed_features)\n",
        "        for idx in range(num_layers):\n",
        "            in_features = features_list[idx]\n",
        "            out_features = features_list[idx + 1]\n",
        "            layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier=lr_multiplier)\n",
        "            setattr(self, f'fc{idx}', layer)\n",
        "\n",
        "        if num_ws is not None and w_avg_beta is not None:\n",
        "            self.register_buffer('w_avg', torch.zeros([w_dim]))\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, update_emas=False):\n",
        "        # Embed, normalize, and concat inputs.\n",
        "        x = None\n",
        "        with torch.autograd.profiler.record_function('input'):\n",
        "            if self.z_dim > 0:\n",
        "                misc.assert_shape(z, [None, self.z_dim])\n",
        "                x = normalize_2nd_moment(z.to(torch.float32))\n",
        "            if self.c_dim > 0:\n",
        "                misc.assert_shape(c, [None, self.c_dim])\n",
        "                y = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n",
        "                x = torch.cat([x, y], dim=1) if x is not None else y\n",
        "\n",
        "        # Main layers.\n",
        "        for idx in range(self.num_layers):\n",
        "            layer = getattr(self, f'fc{idx}')\n",
        "            x = layer(x)\n",
        "\n",
        "        # Update moving average of W.\n",
        "        if update_emas and self.w_avg_beta is not None:\n",
        "            with torch.autograd.profiler.record_function('update_w_avg'):\n",
        "                self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n",
        "\n",
        "        # Broadcast.\n",
        "        if self.num_ws is not None:\n",
        "            with torch.autograd.profiler.record_function('broadcast'):\n",
        "                x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n",
        "\n",
        "        # Apply truncation.\n",
        "        if truncation_psi != 1:\n",
        "            with torch.autograd.profiler.record_function('truncate'):\n",
        "                assert self.w_avg_beta is not None\n",
        "                if self.num_ws is None or truncation_cutoff is None:\n",
        "                    x = self.w_avg.lerp(x, truncation_psi)\n",
        "                else:\n",
        "                    x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'z_dim={self.z_dim:d}, c_dim={self.c_dim:d}, w_dim={self.w_dim:d}, num_ws={self.num_ws:d}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisLayer(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        out_channels,                   # Number of output channels.\n",
        "        w_dim,                          # Intermediate latent (W) dimensionality.\n",
        "        resolution,                     # Resolution of this layer.\n",
        "        kernel_size     = 3,            # Convolution kernel size.\n",
        "        up              = 1,            # Integer upsampling factor.\n",
        "        use_noise       = True,         # Enable noise input?\n",
        "        activation      = 'lrelu',      # Activation function: 'relu', 'lrelu', etc.\n",
        "        resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp      = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        channels_last   = False,        # Use channels_last format for the weights?\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.w_dim = w_dim\n",
        "        self.resolution = resolution\n",
        "        self.up = up\n",
        "        self.use_noise = use_noise\n",
        "        self.activation = activation\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.padding = kernel_size // 2\n",
        "        self.act_gain = bias_act.activation_funcs[activation].def_gain\n",
        "\n",
        "        self.affine = FullyConnectedLayer(w_dim, in_channels, bias_init=1)\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n",
        "        if use_noise:\n",
        "            self.register_buffer('noise_const', torch.randn([resolution, resolution]))\n",
        "            self.noise_strength = torch.nn.Parameter(torch.zeros([]))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([out_channels]))\n",
        "\n",
        "    def forward(self, x, w, noise_mode='random', fused_modconv=True, gain=1):\n",
        "        assert noise_mode in ['random', 'const', 'none']\n",
        "        in_resolution = self.resolution // self.up\n",
        "        misc.assert_shape(x, [None, self.in_channels, in_resolution, in_resolution])\n",
        "        styles = self.affine(w)\n",
        "\n",
        "        noise = None\n",
        "        if self.use_noise and noise_mode == 'random':\n",
        "            noise = torch.randn([x.shape[0], 1, self.resolution, self.resolution], device=x.device) * self.noise_strength\n",
        "        if self.use_noise and noise_mode == 'const':\n",
        "            noise = self.noise_const * self.noise_strength\n",
        "\n",
        "        flip_weight = (self.up == 1) # slightly faster\n",
        "        x = modulated_conv2d(x=x, weight=self.weight, styles=styles, noise=noise, up=self.up,\n",
        "            padding=self.padding, resample_filter=self.resample_filter, flip_weight=flip_weight, fused_modconv=fused_modconv)\n",
        "\n",
        "        act_gain = self.act_gain * gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, self.bias.to(x.dtype), act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return ' '.join([\n",
        "            f'in_channels={self.in_channels:d}, out_channels={self.out_channels:d}, w_dim={self.w_dim:d},',\n",
        "            f'resolution={self.resolution:d}, up={self.up}, activation={self.activation:s}'])\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class ToRGBLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, w_dim, kernel_size=1, conv_clamp=None, channels_last=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.w_dim = w_dim\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.affine = FullyConnectedLayer(w_dim, in_channels, bias_init=1)\n",
        "        memory_format = torch.channels_last if channels_last else torch.contiguous_format\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_channels, in_channels, kernel_size, kernel_size]).to(memory_format=memory_format))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros([out_channels]))\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "\n",
        "    def forward(self, x, w, fused_modconv=True):\n",
        "        styles = self.affine(w) * self.weight_gain\n",
        "        x = modulated_conv2d(x=x, weight=self.weight, styles=styles, demodulate=False, fused_modconv=fused_modconv)\n",
        "        x = bias_act.bias_act(x, self.bias.to(x.dtype), clamp=self.conv_clamp)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'in_channels={self.in_channels:d}, out_channels={self.out_channels:d}, w_dim={self.w_dim:d}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                            # Number of input channels, 0 = first block.\n",
        "        out_channels,                           # Number of output channels.\n",
        "        w_dim,                                  # Intermediate latent (W) dimensionality.\n",
        "        resolution,                             # Resolution of this block.\n",
        "        img_channels,                           # Number of output color channels.\n",
        "        is_last,                                # Is this the last block?\n",
        "        architecture            = 'skip',       # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        resample_filter         = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp              = 256,          # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        use_fp16                = False,        # Use FP16 for this block?\n",
        "        fp16_channels_last      = False,        # Use channels-last memory format with FP16?\n",
        "        fused_modconv_default   = True,         # Default value of fused_modconv. 'inference_only' = True for inference, False for training.\n",
        "        **layer_kwargs,                         # Arguments for SynthesisLayer.\n",
        "    ):\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.w_dim = w_dim\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.is_last = is_last\n",
        "        self.architecture = architecture\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.channels_last = (use_fp16 and fp16_channels_last)\n",
        "        self.fused_modconv_default = fused_modconv_default\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.num_conv = 0\n",
        "        self.num_torgb = 0\n",
        "\n",
        "        if in_channels == 0:\n",
        "            self.const = torch.nn.Parameter(torch.randn([out_channels, resolution, resolution]))\n",
        "\n",
        "        if in_channels != 0:\n",
        "            self.conv0 = SynthesisLayer(in_channels, out_channels, w_dim=w_dim, resolution=resolution, up=2,\n",
        "                resample_filter=resample_filter, conv_clamp=conv_clamp, channels_last=self.channels_last, **layer_kwargs)\n",
        "            self.num_conv += 1\n",
        "\n",
        "        self.conv1 = SynthesisLayer(out_channels, out_channels, w_dim=w_dim, resolution=resolution,\n",
        "            conv_clamp=conv_clamp, channels_last=self.channels_last, **layer_kwargs)\n",
        "        self.num_conv += 1\n",
        "\n",
        "        if is_last or architecture == 'skip':\n",
        "            self.torgb = ToRGBLayer(out_channels, img_channels, w_dim=w_dim,\n",
        "                conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "            self.num_torgb += 1\n",
        "\n",
        "        if in_channels != 0 and architecture == 'resnet':\n",
        "            self.skip = Conv2dLayer(in_channels, out_channels, kernel_size=1, bias=False, up=2,\n",
        "                resample_filter=resample_filter, channels_last=self.channels_last)\n",
        "\n",
        "    def forward(self, x, img, ws, force_fp32=False, fused_modconv=None, update_emas=False, **layer_kwargs):\n",
        "        _ = update_emas # unused\n",
        "        misc.assert_shape(ws, [None, self.num_conv + self.num_torgb, self.w_dim])\n",
        "        w_iter = iter(ws.unbind(dim=1))\n",
        "        if ws.device.type != 'cuda':\n",
        "            force_fp32 = True\n",
        "        dtype = torch.float16 if self.use_fp16 and not force_fp32 else torch.float32\n",
        "        memory_format = torch.channels_last if self.channels_last and not force_fp32 else torch.contiguous_format\n",
        "        if fused_modconv is None:\n",
        "            fused_modconv = self.fused_modconv_default\n",
        "        if fused_modconv == 'inference_only':\n",
        "            fused_modconv = (not self.training)\n",
        "\n",
        "        # Input.\n",
        "        if self.in_channels == 0:\n",
        "            x = self.const.to(dtype=dtype, memory_format=memory_format)\n",
        "            x = x.unsqueeze(0).repeat([ws.shape[0], 1, 1, 1])\n",
        "        else:\n",
        "            misc.assert_shape(x, [None, self.in_channels, self.resolution // 2, self.resolution // 2])\n",
        "            x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "\n",
        "        # Main layers.\n",
        "        if self.in_channels == 0:\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "        elif self.architecture == 'resnet':\n",
        "            y = self.skip(x, gain=np.sqrt(0.5))\n",
        "            x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, gain=np.sqrt(0.5), **layer_kwargs)\n",
        "            x = y.add_(x)\n",
        "        else:\n",
        "            x = self.conv0(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "            x = self.conv1(x, next(w_iter), fused_modconv=fused_modconv, **layer_kwargs)\n",
        "\n",
        "        # ToRGB.\n",
        "        if img is not None:\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution // 2, self.resolution // 2])\n",
        "            img = upfirdn2d.upsample2d(img, self.resample_filter)\n",
        "        if self.is_last or self.architecture == 'skip':\n",
        "            y = self.torgb(x, next(w_iter), fused_modconv=fused_modconv)\n",
        "            y = y.to(dtype=torch.float32, memory_format=torch.contiguous_format)\n",
        "            img = img.add_(y) if img is not None else y\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        assert img is None or img.dtype == torch.float32\n",
        "        return x, img\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'resolution={self.resolution:d}, architecture={self.architecture:s}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class SynthesisNetwork(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output image resolution.\n",
        "        img_channels,               # Number of color channels.\n",
        "        channel_base    = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max     = 512,      # Maximum number of channels in any layer.\n",
        "        num_fp16_res    = 4,        # Use FP16 for the N highest resolutions.\n",
        "        **block_kwargs,             # Arguments for SynthesisBlock.\n",
        "    ):\n",
        "        assert img_resolution >= 4 and img_resolution & (img_resolution - 1) == 0\n",
        "        super().__init__()\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_resolution_log2 = int(np.log2(img_resolution))\n",
        "        self.img_channels = img_channels\n",
        "        self.num_fp16_res = num_fp16_res\n",
        "        self.block_resolutions = [2 ** i for i in range(2, self.img_resolution_log2 + 1)]\n",
        "        channels_dict = {res: min(channel_base // res, channel_max) for res in self.block_resolutions}\n",
        "        fp16_resolution = max(2 ** (self.img_resolution_log2 + 1 - num_fp16_res), 8)\n",
        "\n",
        "        self.num_ws = 0\n",
        "        for res in self.block_resolutions:\n",
        "            in_channels = channels_dict[res // 2] if res > 4 else 0\n",
        "            out_channels = channels_dict[res]\n",
        "            use_fp16 = (res >= fp16_resolution)\n",
        "            is_last = (res == self.img_resolution)\n",
        "            block = SynthesisBlock(in_channels, out_channels, w_dim=w_dim, resolution=res,\n",
        "                img_channels=img_channels, is_last=is_last, use_fp16=use_fp16, **block_kwargs)\n",
        "            self.num_ws += block.num_conv\n",
        "            if is_last:\n",
        "                self.num_ws += block.num_torgb\n",
        "            setattr(self, f'b{res}', block)\n",
        "\n",
        "    def forward(self, ws, **block_kwargs):\n",
        "        block_ws = []\n",
        "        with torch.autograd.profiler.record_function('split_ws'):\n",
        "            misc.assert_shape(ws, [None, self.num_ws, self.w_dim])\n",
        "            ws = ws.to(torch.float32)\n",
        "            w_idx = 0\n",
        "            for res in self.block_resolutions:\n",
        "                block = getattr(self, f'b{res}')\n",
        "                block_ws.append(ws.narrow(1, w_idx, block.num_conv + block.num_torgb))\n",
        "                w_idx += block.num_conv\n",
        "\n",
        "        x = img = None\n",
        "        for res, cur_ws in zip(self.block_resolutions, block_ws):\n",
        "            block = getattr(self, f'b{res}')\n",
        "            x, img = block(x, img, cur_ws, **block_kwargs)\n",
        "        return img\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return ' '.join([\n",
        "            f'w_dim={self.w_dim:d}, num_ws={self.num_ws:d},',\n",
        "            f'img_resolution={self.img_resolution:d}, img_channels={self.img_channels:d},',\n",
        "            f'num_fp16_res={self.num_fp16_res:d}'])\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        img_resolution,             # Output resolution.\n",
        "        img_channels,               # Number of output color channels.\n",
        "        mapping_kwargs      = {},   # Arguments for MappingNetwork.\n",
        "        **synthesis_kwargs,         # Arguments for SynthesisNetwork.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.synthesis = SynthesisNetwork(w_dim=w_dim, img_resolution=img_resolution, img_channels=img_channels, **synthesis_kwargs)\n",
        "        self.num_ws = self.synthesis.num_ws\n",
        "        self.mapping = MappingNetwork(z_dim=z_dim, c_dim=c_dim, w_dim=w_dim, num_ws=self.num_ws, **mapping_kwargs)\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, update_emas=False, **synthesis_kwargs):\n",
        "        ws = self.mapping(z, c, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff, update_emas=update_emas)\n",
        "        img = self.synthesis(ws, update_emas=update_emas, **synthesis_kwargs)\n",
        "        return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class DiscriminatorBlock(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                        # Number of input channels, 0 = first block.\n",
        "        tmp_channels,                       # Number of intermediate channels.\n",
        "        out_channels,                       # Number of output channels.\n",
        "        resolution,                         # Resolution of this block.\n",
        "        img_channels,                       # Number of input color channels.\n",
        "        first_layer_idx,                    # Index of the first layer.\n",
        "        architecture        = 'resnet',     # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        activation          = 'lrelu',      # Activation function: 'relu', 'lrelu', etc.\n",
        "        resample_filter     = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "        conv_clamp          = None,         # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        use_fp16            = False,        # Use FP16 for this block?\n",
        "        fp16_channels_last  = False,        # Use channels-last memory format with FP16?\n",
        "        freeze_layers       = 0,            # Freeze-D: Number of layers to freeze.\n",
        "    ):\n",
        "        assert in_channels in [0, tmp_channels]\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.first_layer_idx = first_layer_idx\n",
        "        self.architecture = architecture\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.channels_last = (use_fp16 and fp16_channels_last)\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "\n",
        "        self.num_layers = 0\n",
        "        def trainable_gen():\n",
        "            while True:\n",
        "                layer_idx = self.first_layer_idx + self.num_layers\n",
        "                trainable = (layer_idx >= freeze_layers)\n",
        "                self.num_layers += 1\n",
        "                yield trainable\n",
        "        trainable_iter = trainable_gen()\n",
        "\n",
        "        if in_channels == 0 or architecture == 'skip':\n",
        "            self.fromrgb = Conv2dLayer(img_channels, tmp_channels, kernel_size=1, activation=activation,\n",
        "                trainable=next(trainable_iter), conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        self.conv0 = Conv2dLayer(tmp_channels, tmp_channels, kernel_size=3, activation=activation,\n",
        "            trainable=next(trainable_iter), conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        self.conv1 = Conv2dLayer(tmp_channels, out_channels, kernel_size=3, activation=activation, down=2,\n",
        "            trainable=next(trainable_iter), resample_filter=resample_filter, conv_clamp=conv_clamp, channels_last=self.channels_last)\n",
        "\n",
        "        if architecture == 'resnet':\n",
        "            self.skip = Conv2dLayer(tmp_channels, out_channels, kernel_size=1, bias=False, down=2,\n",
        "                trainable=next(trainable_iter), resample_filter=resample_filter, channels_last=self.channels_last)\n",
        "\n",
        "    def forward(self, x, img, force_fp32=False):\n",
        "        if (x if x is not None else img).device.type != 'cuda':\n",
        "            force_fp32 = True\n",
        "        dtype = torch.float16 if self.use_fp16 and not force_fp32 else torch.float32\n",
        "        memory_format = torch.channels_last if self.channels_last and not force_fp32 else torch.contiguous_format\n",
        "\n",
        "        # Input.\n",
        "        if x is not None:\n",
        "            misc.assert_shape(x, [None, self.in_channels, self.resolution, self.resolution])\n",
        "            x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "\n",
        "        # FromRGB.\n",
        "        if self.in_channels == 0 or self.architecture == 'skip':\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution, self.resolution])\n",
        "            img = img.to(dtype=dtype, memory_format=memory_format)\n",
        "            y = self.fromrgb(img)\n",
        "            x = x + y if x is not None else y\n",
        "            img = upfirdn2d.downsample2d(img, self.resample_filter) if self.architecture == 'skip' else None\n",
        "\n",
        "        # Main layers.\n",
        "        if self.architecture == 'resnet':\n",
        "            y = self.skip(x, gain=np.sqrt(0.5))\n",
        "            x = self.conv0(x)\n",
        "            x = self.conv1(x, gain=np.sqrt(0.5))\n",
        "            x = y.add_(x)\n",
        "        else:\n",
        "            x = self.conv0(x)\n",
        "            x = self.conv1(x)\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        return x, img\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'resolution={self.resolution:d}, architecture={self.architecture:s}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class MinibatchStdLayer(torch.nn.Module):\n",
        "    def __init__(self, group_size, num_channels=1):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        with misc.suppress_tracer_warnings(): # as_tensor results are registered as constants\n",
        "            G = torch.min(torch.as_tensor(self.group_size), torch.as_tensor(N)) if self.group_size is not None else N\n",
        "        F = self.num_channels\n",
        "        c = C // F\n",
        "\n",
        "        y = x.reshape(G, -1, F, c, H, W)    # [GnFcHW] Split minibatch N into n groups of size G, and channels C into F groups of size c.\n",
        "        y = y - y.mean(dim=0)               # [GnFcHW] Subtract mean over group.\n",
        "        y = y.square().mean(dim=0)          # [nFcHW]  Calc variance over group.\n",
        "        y = (y + 1e-8).sqrt()               # [nFcHW]  Calc stddev over group.\n",
        "        y = y.mean(dim=[2,3,4])             # [nF]     Take average over channels and pixels.\n",
        "        y = y.reshape(-1, F, 1, 1)          # [nF11]   Add missing dimensions.\n",
        "        y = y.repeat(G, 1, H, W)            # [NFHW]   Replicate over group and pixels.\n",
        "        x = torch.cat([x, y], dim=1)        # [NCHW]   Append to input as new channels.\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'group_size={self.group_size}, num_channels={self.num_channels:d}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class DiscriminatorEpilogue(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        in_channels,                    # Number of input channels.\n",
        "        cmap_dim,                       # Dimensionality of mapped conditioning label, 0 = no label.\n",
        "        resolution,                     # Resolution of this block.\n",
        "        img_channels,                   # Number of input color channels.\n",
        "        architecture        = 'resnet', # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        mbstd_group_size    = 4,        # Group size for the minibatch standard deviation layer, None = entire minibatch.\n",
        "        mbstd_num_channels  = 1,        # Number of features for the minibatch standard deviation layer, 0 = disable.\n",
        "        activation          = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        conv_clamp          = None,     # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "    ):\n",
        "        assert architecture in ['orig', 'skip', 'resnet']\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.cmap_dim = cmap_dim\n",
        "        self.resolution = resolution\n",
        "        self.img_channels = img_channels\n",
        "        self.architecture = architecture\n",
        "\n",
        "        if architecture == 'skip':\n",
        "            self.fromrgb = Conv2dLayer(img_channels, in_channels, kernel_size=1, activation=activation)\n",
        "        self.mbstd = MinibatchStdLayer(group_size=mbstd_group_size, num_channels=mbstd_num_channels) if mbstd_num_channels > 0 else None\n",
        "        self.conv = Conv2dLayer(in_channels + mbstd_num_channels, in_channels, kernel_size=3, activation=activation, conv_clamp=conv_clamp)\n",
        "        self.fc = FullyConnectedLayer(in_channels * (resolution ** 2), in_channels, activation=activation)\n",
        "        self.out = FullyConnectedLayer(in_channels, 1 if cmap_dim == 0 else cmap_dim)\n",
        "\n",
        "    def forward(self, x, img, cmap, force_fp32=False):\n",
        "        misc.assert_shape(x, [None, self.in_channels, self.resolution, self.resolution]) # [NCHW]\n",
        "        _ = force_fp32 # unused\n",
        "        dtype = torch.float32\n",
        "        memory_format = torch.contiguous_format\n",
        "\n",
        "        # FromRGB.\n",
        "        x = x.to(dtype=dtype, memory_format=memory_format)\n",
        "        if self.architecture == 'skip':\n",
        "            misc.assert_shape(img, [None, self.img_channels, self.resolution, self.resolution])\n",
        "            img = img.to(dtype=dtype, memory_format=memory_format)\n",
        "            x = x + self.fromrgb(img)\n",
        "\n",
        "        # Main layers.\n",
        "        if self.mbstd is not None:\n",
        "            x = self.mbstd(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x.flatten(1))\n",
        "        x = self.out(x)\n",
        "\n",
        "        # Conditioning.\n",
        "        if self.cmap_dim > 0:\n",
        "            misc.assert_shape(cmap, [None, self.cmap_dim])\n",
        "            x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n",
        "\n",
        "        assert x.dtype == dtype\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'resolution={self.resolution:d}, architecture={self.architecture:s}'\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@persistence.persistent_class\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        c_dim,                          # Conditioning label (C) dimensionality.\n",
        "        img_resolution,                 # Input resolution.\n",
        "        img_channels,                   # Number of input color channels.\n",
        "        architecture        = 'resnet', # Architecture: 'orig', 'skip', 'resnet'.\n",
        "        channel_base        = 32768,    # Overall multiplier for the number of channels.\n",
        "        channel_max         = 512,      # Maximum number of channels in any layer.\n",
        "        num_fp16_res        = 4,        # Use FP16 for the N highest resolutions.\n",
        "        conv_clamp          = 256,      # Clamp the output of convolution layers to +-X, None = disable clamping.\n",
        "        cmap_dim            = None,     # Dimensionality of mapped conditioning label, None = default.\n",
        "        block_kwargs        = {},       # Arguments for DiscriminatorBlock.\n",
        "        mapping_kwargs      = {},       # Arguments for MappingNetwork.\n",
        "        epilogue_kwargs     = {},       # Arguments for DiscriminatorEpilogue.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.c_dim = c_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_resolution_log2 = int(np.log2(img_resolution))\n",
        "        self.img_channels = img_channels\n",
        "        self.block_resolutions = [2 ** i for i in range(self.img_resolution_log2, 2, -1)]\n",
        "        channels_dict = {res: min(channel_base // res, channel_max) for res in self.block_resolutions + [4]}\n",
        "        fp16_resolution = max(2 ** (self.img_resolution_log2 + 1 - num_fp16_res), 8)\n",
        "\n",
        "        if cmap_dim is None:\n",
        "            cmap_dim = channels_dict[4]\n",
        "        if c_dim == 0:\n",
        "            cmap_dim = 0\n",
        "\n",
        "        common_kwargs = dict(img_channels=img_channels, architecture=architecture, conv_clamp=conv_clamp)\n",
        "        cur_layer_idx = 0\n",
        "        for res in self.block_resolutions:\n",
        "            in_channels = channels_dict[res] if res < img_resolution else 0\n",
        "            tmp_channels = channels_dict[res]\n",
        "            out_channels = channels_dict[res // 2]\n",
        "            use_fp16 = (res >= fp16_resolution)\n",
        "            block = DiscriminatorBlock(in_channels, tmp_channels, out_channels, resolution=res,\n",
        "                first_layer_idx=cur_layer_idx, use_fp16=use_fp16, **block_kwargs, **common_kwargs)\n",
        "            setattr(self, f'b{res}', block)\n",
        "            cur_layer_idx += block.num_layers\n",
        "        if c_dim > 0:\n",
        "            self.mapping = MappingNetwork(z_dim=0, c_dim=c_dim, w_dim=cmap_dim, num_ws=None, w_avg_beta=None, **mapping_kwargs)\n",
        "        self.b4 = DiscriminatorEpilogue(channels_dict[4], cmap_dim=cmap_dim, resolution=4, **epilogue_kwargs, **common_kwargs)\n",
        "\n",
        "    def forward(self, img, c, update_emas=False, **block_kwargs):\n",
        "        _ = update_emas # unused\n",
        "        x = None\n",
        "        for res in self.block_resolutions:\n",
        "            block = getattr(self, f'b{res}')\n",
        "            x, img = block(x, img, **block_kwargs)\n",
        "\n",
        "        cmap = None\n",
        "        if self.c_dim > 0:\n",
        "            cmap = self.mapping(None, c)\n",
        "        x = self.b4(x, img, cmap)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return f'c_dim={self.c_dim:d}, img_resolution={self.img_resolution:d}, img_channels={self.img_channels:d}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Vq5bkxAcyQ"
      },
      "outputs": [],
      "source": [
        "#this is from: https://github.com/NVlabs/stylegan3/blob/main/training/loss.py\n",
        "#loss\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import training_stats\n",
        "from torch_utils.ops import conv2d_gradfix\n",
        "from torch_utils.ops import upfirdn2d\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class Loss:\n",
        "    def accumulate_gradients(self, phase, real_img, real_c, gen_z, gen_c, gain, cur_nimg): # to be overridden by subclass\n",
        "        raise NotImplementedError()\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class StyleGAN2Loss(Loss):\n",
        "    def __init__(self, device, G, D, augment_pipe=None, r1_gamma=10, style_mixing_prob=0, pl_weight=0, pl_batch_shrink=2, pl_decay=0.01, pl_no_weight_grad=False, blur_init_sigma=0, blur_fade_kimg=0):\n",
        "        super().__init__()\n",
        "        self.device             = device\n",
        "        self.G                  = G\n",
        "        self.D                  = D\n",
        "        self.augment_pipe       = augment_pipe\n",
        "        self.r1_gamma           = r1_gamma\n",
        "        self.style_mixing_prob  = style_mixing_prob\n",
        "        self.pl_weight          = pl_weight\n",
        "        self.pl_batch_shrink    = pl_batch_shrink\n",
        "        self.pl_decay           = pl_decay\n",
        "        self.pl_no_weight_grad  = pl_no_weight_grad\n",
        "        self.pl_mean            = torch.zeros([], device=device)\n",
        "        self.blur_init_sigma    = blur_init_sigma\n",
        "        self.blur_fade_kimg     = blur_fade_kimg\n",
        "\n",
        "    def run_G(self, z, c, update_emas=False):\n",
        "        ws = self.G.mapping(z, c, update_emas=update_emas)\n",
        "        if self.style_mixing_prob > 0:\n",
        "            with torch.autograd.profiler.record_function('style_mixing'):\n",
        "                cutoff = torch.empty([], dtype=torch.int64, device=ws.device).random_(1, ws.shape[1])\n",
        "                cutoff = torch.where(torch.rand([], device=ws.device) < self.style_mixing_prob, cutoff, torch.full_like(cutoff, ws.shape[1]))\n",
        "                ws[:, cutoff:] = self.G.mapping(torch.randn_like(z), c, update_emas=False)[:, cutoff:]\n",
        "        img = self.G.synthesis(ws, update_emas=update_emas)\n",
        "        return img, ws\n",
        "\n",
        "    def run_D(self, img, c, blur_sigma=0, update_emas=False):\n",
        "        blur_size = np.floor(blur_sigma * 3)\n",
        "        if blur_size > 0:\n",
        "            with torch.autograd.profiler.record_function('blur'):\n",
        "                f = torch.arange(-blur_size, blur_size + 1, device=img.device).div(blur_sigma).square().neg().exp2()\n",
        "                img = upfirdn2d.filter2d(img, f / f.sum())\n",
        "        if self.augment_pipe is not None:\n",
        "            img = self.augment_pipe(img)\n",
        "        logits = self.D(img, c, update_emas=update_emas)\n",
        "        return logits\n",
        "\n",
        "    def accumulate_gradients(self, phase, real_img, real_c, gen_z, gen_c, gain, cur_nimg):\n",
        "        assert phase in ['Gmain', 'Greg', 'Gboth', 'Dmain', 'Dreg', 'Dboth']\n",
        "        if self.pl_weight == 0:\n",
        "            phase = {'Greg': 'none', 'Gboth': 'Gmain'}.get(phase, phase)\n",
        "        if self.r1_gamma == 0:\n",
        "            phase = {'Dreg': 'none', 'Dboth': 'Dmain'}.get(phase, phase)\n",
        "        blur_sigma = max(1 - cur_nimg / (self.blur_fade_kimg * 1e3), 0) * self.blur_init_sigma if self.blur_fade_kimg > 0 else 0\n",
        "\n",
        "        # Gmain: Maximize logits for generated images.\n",
        "        if phase in ['Gmain', 'Gboth']:\n",
        "            with torch.autograd.profiler.record_function('Gmain_forward'):\n",
        "                gen_img, _gen_ws = self.run_G(gen_z, gen_c)\n",
        "                gen_logits = self.run_D(gen_img, gen_c, blur_sigma=blur_sigma)\n",
        "                training_stats.report('Loss/scores/fake', gen_logits)\n",
        "                training_stats.report('Loss/signs/fake', gen_logits.sign())\n",
        "                loss_Gmain = torch.nn.functional.softplus(-gen_logits) # -log(sigmoid(gen_logits))\n",
        "                training_stats.report('Loss/G/loss', loss_Gmain)\n",
        "            with torch.autograd.profiler.record_function('Gmain_backward'):\n",
        "                loss_Gmain.mean().mul(gain).backward()\n",
        "\n",
        "        # Gpl: Apply path length regularization.\n",
        "        if phase in ['Greg', 'Gboth']:\n",
        "            with torch.autograd.profiler.record_function('Gpl_forward'):\n",
        "                batch_size = gen_z.shape[0] // self.pl_batch_shrink\n",
        "                gen_img, gen_ws = self.run_G(gen_z[:batch_size], gen_c[:batch_size])\n",
        "                pl_noise = torch.randn_like(gen_img) / np.sqrt(gen_img.shape[2] * gen_img.shape[3])\n",
        "                with torch.autograd.profiler.record_function('pl_grads'), conv2d_gradfix.no_weight_gradients(self.pl_no_weight_grad):\n",
        "                    pl_grads = torch.autograd.grad(outputs=[(gen_img * pl_noise).sum()], inputs=[gen_ws], create_graph=True, only_inputs=True)[0]\n",
        "                pl_lengths = pl_grads.square().sum(2).mean(1).sqrt()\n",
        "                pl_mean = self.pl_mean.lerp(pl_lengths.mean(), self.pl_decay)\n",
        "                self.pl_mean.copy_(pl_mean.detach())\n",
        "                pl_penalty = (pl_lengths - pl_mean).square()\n",
        "                training_stats.report('Loss/pl_penalty', pl_penalty)\n",
        "                loss_Gpl = pl_penalty * self.pl_weight\n",
        "                training_stats.report('Loss/G/reg', loss_Gpl)\n",
        "            with torch.autograd.profiler.record_function('Gpl_backward'):\n",
        "                loss_Gpl.mean().mul(gain).backward()\n",
        "\n",
        "        # Dmain: Minimize logits for generated images.\n",
        "        loss_Dgen = 0\n",
        "        if phase in ['Dmain', 'Dboth']:\n",
        "            with torch.autograd.profiler.record_function('Dgen_forward'):\n",
        "                gen_img, _gen_ws = self.run_G(gen_z, gen_c, update_emas=True)\n",
        "                gen_logits = self.run_D(gen_img, gen_c, blur_sigma=blur_sigma, update_emas=True)\n",
        "                training_stats.report('Loss/scores/fake', gen_logits)\n",
        "                training_stats.report('Loss/signs/fake', gen_logits.sign())\n",
        "                loss_Dgen = torch.nn.functional.softplus(gen_logits) # -log(1 - sigmoid(gen_logits))\n",
        "            with torch.autograd.profiler.record_function('Dgen_backward'):\n",
        "                loss_Dgen.mean().mul(gain).backward()\n",
        "\n",
        "        # Dmain: Maximize logits for real images.\n",
        "        # Dr1: Apply R1 regularization.\n",
        "        if phase in ['Dmain', 'Dreg', 'Dboth']:\n",
        "            name = 'Dreal' if phase == 'Dmain' else 'Dr1' if phase == 'Dreg' else 'Dreal_Dr1'\n",
        "            with torch.autograd.profiler.record_function(name + '_forward'):\n",
        "                real_img_tmp = real_img.detach().requires_grad_(phase in ['Dreg', 'Dboth'])\n",
        "                real_logits = self.run_D(real_img_tmp, real_c, blur_sigma=blur_sigma)\n",
        "                training_stats.report('Loss/scores/real', real_logits)\n",
        "                training_stats.report('Loss/signs/real', real_logits.sign())\n",
        "\n",
        "                loss_Dreal = 0\n",
        "                if phase in ['Dmain', 'Dboth']:\n",
        "                    loss_Dreal = torch.nn.functional.softplus(-real_logits) # -log(sigmoid(real_logits))\n",
        "                    training_stats.report('Loss/D/loss', loss_Dgen + loss_Dreal)\n",
        "\n",
        "                loss_Dr1 = 0\n",
        "                if phase in ['Dreg', 'Dboth']:\n",
        "                    with torch.autograd.profiler.record_function('r1_grads'), conv2d_gradfix.no_weight_gradients():\n",
        "                        r1_grads = torch.autograd.grad(outputs=[real_logits.sum()], inputs=[real_img_tmp], create_graph=True, only_inputs=True)[0]\n",
        "                    r1_penalty = r1_grads.square().sum([1,2,3])\n",
        "                    loss_Dr1 = r1_penalty * (self.r1_gamma / 2)\n",
        "                    training_stats.report('Loss/r1_penalty', r1_penalty)\n",
        "                    training_stats.report('Loss/D/reg', loss_Dr1)\n",
        "\n",
        "            with torch.autograd.profiler.record_function(name + '_backward'):\n",
        "                (loss_Dreal + loss_Dr1).mean().mul(gain).backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUhmilKAALJi"
      },
      "outputs": [],
      "source": [
        "#this is from: https://github.com/NVlabs/stylegan3/blob/main/training/dataset.py\n",
        "#dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import PIL.Image\n",
        "import json\n",
        "import torch\n",
        "import dnnlib\n",
        "\n",
        "try:\n",
        "    import pyspng\n",
        "except ImportError:\n",
        "    pyspng = None\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "        name,                   # Name of the dataset.\n",
        "        raw_shape,              # Shape of the raw image data (NCHW).\n",
        "        max_size    = None,     # Artificially limit the size of the dataset. None = no limit. Applied before xflip.\n",
        "        use_labels  = False,    # Enable conditioning labels? False = label dimension is zero.\n",
        "        xflip       = False,    # Artificially double the size of the dataset via x-flips. Applied after max_size.\n",
        "        random_seed = 0,        # Random seed to use when applying max_size.\n",
        "    ):\n",
        "        self._name = name\n",
        "        self._raw_shape = list(raw_shape)\n",
        "        self._use_labels = use_labels\n",
        "        self._raw_labels = None\n",
        "        self._label_shape = None\n",
        "\n",
        "        # Apply max_size.\n",
        "        self._raw_idx = np.arange(self._raw_shape[0], dtype=np.int64)\n",
        "        if (max_size is not None) and (self._raw_idx.size > max_size):\n",
        "            np.random.RandomState(random_seed).shuffle(self._raw_idx)\n",
        "            self._raw_idx = np.sort(self._raw_idx[:max_size])\n",
        "\n",
        "        # Apply xflip.\n",
        "        self._xflip = np.zeros(self._raw_idx.size, dtype=np.uint8)\n",
        "        if xflip:\n",
        "            self._raw_idx = np.tile(self._raw_idx, 2)\n",
        "            self._xflip = np.concatenate([self._xflip, np.ones_like(self._xflip)])\n",
        "\n",
        "    def _get_raw_labels(self):\n",
        "        if self._raw_labels is None:\n",
        "            self._raw_labels = self._load_raw_labels() if self._use_labels else None\n",
        "            if self._raw_labels is None:\n",
        "                self._raw_labels = np.zeros([self._raw_shape[0], 0], dtype=np.float32)\n",
        "            assert isinstance(self._raw_labels, np.ndarray)\n",
        "            assert self._raw_labels.shape[0] == self._raw_shape[0]\n",
        "            assert self._raw_labels.dtype in [np.float32, np.int64]\n",
        "            if self._raw_labels.dtype == np.int64:\n",
        "                assert self._raw_labels.ndim == 1\n",
        "                assert np.all(self._raw_labels >= 0)\n",
        "        return self._raw_labels\n",
        "\n",
        "    def close(self): # to be overridden by subclass\n",
        "        pass\n",
        "\n",
        "    def _load_raw_image(self, raw_idx): # to be overridden by subclass\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _load_raw_labels(self): # to be overridden by subclass\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return dict(self.__dict__, _raw_labels=None)\n",
        "\n",
        "    def __del__(self):\n",
        "        try:\n",
        "            self.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._raw_idx.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self._load_raw_image(self._raw_idx[idx])\n",
        "        assert isinstance(image, np.ndarray)\n",
        "        assert list(image.shape) == self.image_shape\n",
        "        assert image.dtype == np.uint8\n",
        "        if self._xflip[idx]:\n",
        "            assert image.ndim == 3 # CHW\n",
        "            image = image[:, :, ::-1]\n",
        "        return image.copy(), self.get_label(idx)\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        label = self._get_raw_labels()[self._raw_idx[idx]]\n",
        "        if label.dtype == np.int64:\n",
        "            onehot = np.zeros(self.label_shape, dtype=np.float32)\n",
        "            onehot[label] = 1\n",
        "            label = onehot\n",
        "        return label.copy()\n",
        "\n",
        "    def get_details(self, idx):\n",
        "        d = dnnlib.EasyDict()\n",
        "        d.raw_idx = int(self._raw_idx[idx])\n",
        "        d.xflip = (int(self._xflip[idx]) != 0)\n",
        "        d.raw_label = self._get_raw_labels()[d.raw_idx].copy()\n",
        "        return d\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return self._name\n",
        "\n",
        "    @property\n",
        "    def image_shape(self):\n",
        "        return list(self._raw_shape[1:])\n",
        "\n",
        "    @property\n",
        "    def num_channels(self):\n",
        "        assert len(self.image_shape) == 3 # CHW\n",
        "        return self.image_shape[0]\n",
        "\n",
        "    @property\n",
        "    def resolution(self):\n",
        "        assert len(self.image_shape) == 3 # CHW\n",
        "        assert self.image_shape[1] == self.image_shape[2]\n",
        "        return self.image_shape[1]\n",
        "\n",
        "    @property\n",
        "    def label_shape(self):\n",
        "        if self._label_shape is None:\n",
        "            raw_labels = self._get_raw_labels()\n",
        "            if raw_labels.dtype == np.int64:\n",
        "                self._label_shape = [int(np.max(raw_labels)) + 1]\n",
        "            else:\n",
        "                self._label_shape = raw_labels.shape[1:]\n",
        "        return list(self._label_shape)\n",
        "\n",
        "    @property\n",
        "    def label_dim(self):\n",
        "        assert len(self.label_shape) == 1\n",
        "        return self.label_shape[0]\n",
        "\n",
        "    @property\n",
        "    def has_labels(self):\n",
        "        return any(x != 0 for x in self.label_shape)\n",
        "\n",
        "    @property\n",
        "    def has_onehot_labels(self):\n",
        "        return self._get_raw_labels().dtype == np.int64\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(self,\n",
        "        path,                   # Path to directory or zip.\n",
        "        resolution      = None, # Ensure specific resolution, None = highest available.\n",
        "        **super_kwargs,         # Additional arguments for the Dataset base class.\n",
        "    ):\n",
        "        self._path = path\n",
        "        self._zipfile = None\n",
        "\n",
        "        if os.path.isdir(self._path):\n",
        "            self._type = 'dir'\n",
        "            self._all_fnames = {os.path.relpath(os.path.join(root, fname), start=self._path) for root, _dirs, files in os.walk(self._path) for fname in files}\n",
        "        elif self._file_ext(self._path) == '.zip':\n",
        "            self._type = 'zip'\n",
        "            self._all_fnames = set(self._get_zipfile().namelist())\n",
        "        else:\n",
        "            raise IOError('Path must point to a directory or zip')\n",
        "\n",
        "        PIL.Image.init()\n",
        "        self._image_fnames = sorted(fname for fname in self._all_fnames if self._file_ext(fname) in PIL.Image.EXTENSION)\n",
        "        if len(self._image_fnames) == 0:\n",
        "            raise IOError('No image files found in the specified path')\n",
        "\n",
        "        name = os.path.splitext(os.path.basename(self._path))[0]\n",
        "        raw_shape = [len(self._image_fnames)] + list(self._load_raw_image(0).shape)\n",
        "        if resolution is not None and (raw_shape[2] != resolution or raw_shape[3] != resolution):\n",
        "            raise IOError('Image files do not match the specified resolution')\n",
        "        super().__init__(name=name, raw_shape=raw_shape, **super_kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def _file_ext(fname):\n",
        "        return os.path.splitext(fname)[1].lower()\n",
        "\n",
        "    def _get_zipfile(self):\n",
        "        assert self._type == 'zip'\n",
        "        if self._zipfile is None:\n",
        "            self._zipfile = zipfile.ZipFile(self._path)\n",
        "        return self._zipfile\n",
        "\n",
        "    def _open_file(self, fname):\n",
        "        if self._type == 'dir':\n",
        "            return open(os.path.join(self._path, fname), 'rb')\n",
        "        if self._type == 'zip':\n",
        "            return self._get_zipfile().open(fname, 'r')\n",
        "        return None\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            if self._zipfile is not None:\n",
        "                self._zipfile.close()\n",
        "        finally:\n",
        "            self._zipfile = None\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return dict(super().__getstate__(), _zipfile=None)\n",
        "\n",
        "    def _load_raw_image(self, raw_idx):\n",
        "        fname = self._image_fnames[raw_idx]\n",
        "        with self._open_file(fname) as f:\n",
        "            if pyspng is not None and self._file_ext(fname) == '.png':\n",
        "                image = pyspng.load(f.read())\n",
        "            else:\n",
        "                image = np.array(PIL.Image.open(f))\n",
        "        if image.ndim == 2:\n",
        "            image = image[:, :, np.newaxis] # HW => HWC\n",
        "        image = image.transpose(2, 0, 1) # HWC => CHW\n",
        "        return image\n",
        "\n",
        "    def _load_raw_labels(self):\n",
        "        fname = 'dataset.json'\n",
        "        if fname not in self._all_fnames:\n",
        "            return None\n",
        "        with self._open_file(fname) as f:\n",
        "            labels = json.load(f)['labels']\n",
        "        if labels is None:\n",
        "            return None\n",
        "        labels = dict(labels)\n",
        "        labels = [labels[fname.replace('\\\\', '/')] for fname in self._image_fnames]\n",
        "        labels = np.array(labels)\n",
        "        labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n",
        "        return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg3Xh-QeAE6S"
      },
      "outputs": [],
      "source": [
        "#this is from: https://github.com/NVlabs/stylegan3/blob/main/training/augment.py\n",
        "#augment\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import torch\n",
        "from torch_utils import persistence\n",
        "from torch_utils import misc\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from torch_utils.ops import grid_sample_gradfix\n",
        "from torch_utils.ops import conv2d_gradfix\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Coefficients of various wavelet decomposition low-pass filters.\n",
        "\n",
        "wavelets = {\n",
        "    'haar': [0.7071067811865476, 0.7071067811865476],\n",
        "    'db1':  [0.7071067811865476, 0.7071067811865476],\n",
        "    'db2':  [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'db3':  [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'db4':  [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523],\n",
        "    'db5':  [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125],\n",
        "    'db6':  [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017],\n",
        "    'db7':  [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236],\n",
        "    'db8':  [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161],\n",
        "    'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025],\n",
        "    'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569],\n",
        "    'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427],\n",
        "    'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728],\n",
        "    'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148],\n",
        "    'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255],\n",
        "    'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609],\n",
        "}\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Helpers for constructing transformation matrices.\n",
        "\n",
        "def matrix(*rows, device=None):\n",
        "    assert all(len(row) == len(rows[0]) for row in rows)\n",
        "    elems = [x for row in rows for x in row]\n",
        "    ref = [x for x in elems if isinstance(x, torch.Tensor)]\n",
        "    if len(ref) == 0:\n",
        "        return misc.constant(np.asarray(rows), device=device)\n",
        "    assert device is None or device == ref[0].device\n",
        "    elems = [x if isinstance(x, torch.Tensor) else misc.constant(x, shape=ref[0].shape, device=ref[0].device) for x in elems]\n",
        "    return torch.stack(elems, dim=-1).reshape(ref[0].shape + (len(rows), -1))\n",
        "\n",
        "def translate2d(tx, ty, **kwargs):\n",
        "    return matrix(\n",
        "        [1, 0, tx],\n",
        "        [0, 1, ty],\n",
        "        [0, 0, 1],\n",
        "        **kwargs)\n",
        "\n",
        "def translate3d(tx, ty, tz, **kwargs):\n",
        "    return matrix(\n",
        "        [1, 0, 0, tx],\n",
        "        [0, 1, 0, ty],\n",
        "        [0, 0, 1, tz],\n",
        "        [0, 0, 0, 1],\n",
        "        **kwargs)\n",
        "\n",
        "def scale2d(sx, sy, **kwargs):\n",
        "    return matrix(\n",
        "        [sx, 0,  0],\n",
        "        [0,  sy, 0],\n",
        "        [0,  0,  1],\n",
        "        **kwargs)\n",
        "\n",
        "def scale3d(sx, sy, sz, **kwargs):\n",
        "    return matrix(\n",
        "        [sx, 0,  0,  0],\n",
        "        [0,  sy, 0,  0],\n",
        "        [0,  0,  sz, 0],\n",
        "        [0,  0,  0,  1],\n",
        "        **kwargs)\n",
        "\n",
        "def rotate2d(theta, **kwargs):\n",
        "    return matrix(\n",
        "        [torch.cos(theta), torch.sin(-theta), 0],\n",
        "        [torch.sin(theta), torch.cos(theta),  0],\n",
        "        [0,                0,                 1],\n",
        "        **kwargs)\n",
        "\n",
        "def rotate3d(v, theta, **kwargs):\n",
        "    vx = v[..., 0]; vy = v[..., 1]; vz = v[..., 2]\n",
        "    s = torch.sin(theta); c = torch.cos(theta); cc = 1 - c\n",
        "    return matrix(\n",
        "        [vx*vx*cc+c,    vx*vy*cc-vz*s, vx*vz*cc+vy*s, 0],\n",
        "        [vy*vx*cc+vz*s, vy*vy*cc+c,    vy*vz*cc-vx*s, 0],\n",
        "        [vz*vx*cc-vy*s, vz*vy*cc+vx*s, vz*vz*cc+c,    0],\n",
        "        [0,             0,             0,             1],\n",
        "        **kwargs)\n",
        "\n",
        "def translate2d_inv(tx, ty, **kwargs):\n",
        "    return translate2d(-tx, -ty, **kwargs)\n",
        "\n",
        "def scale2d_inv(sx, sy, **kwargs):\n",
        "    return scale2d(1 / sx, 1 / sy, **kwargs)\n",
        "\n",
        "def rotate2d_inv(theta, **kwargs):\n",
        "    return rotate2d(-theta, **kwargs)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Versatile image augmentation pipeline from the paper\n",
        "# \"Training Generative Adversarial Networks with Limited Data\".\n",
        "#\n",
        "# All augmentations are disabled by default; individual augmentations can\n",
        "# be enabled by setting their probability multipliers to 1.\n",
        "\n",
        "@persistence.persistent_class\n",
        "class AugmentPipe(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        xflip=0, rotate90=0, xint=0, xint_max=0.125,\n",
        "        scale=0, rotate=0, aniso=0, xfrac=0, scale_std=0.2, rotate_max=1, aniso_std=0.2, xfrac_std=0.125,\n",
        "        brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0, brightness_std=0.2, contrast_std=0.5, hue_max=1, saturation_std=1,\n",
        "        imgfilter=0, imgfilter_bands=[1,1,1,1], imgfilter_std=1,\n",
        "        noise=0, cutout=0, noise_std=0.1, cutout_size=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.register_buffer('p', torch.ones([]))       # Overall multiplier for augmentation probability.\n",
        "\n",
        "        # Pixel blitting.\n",
        "        self.xflip            = float(xflip)            # Probability multiplier for x-flip.\n",
        "        self.rotate90         = float(rotate90)         # Probability multiplier for 90 degree rotations.\n",
        "        self.xint             = float(xint)             # Probability multiplier for integer translation.\n",
        "        self.xint_max         = float(xint_max)         # Range of integer translation, relative to image dimensions.\n",
        "\n",
        "        # General geometric transformations.\n",
        "        self.scale            = float(scale)            # Probability multiplier for isotropic scaling.\n",
        "        self.rotate           = float(rotate)           # Probability multiplier for arbitrary rotation.\n",
        "        self.aniso            = float(aniso)            # Probability multiplier for anisotropic scaling.\n",
        "        self.xfrac            = float(xfrac)            # Probability multiplier for fractional translation.\n",
        "        self.scale_std        = float(scale_std)        # Log2 standard deviation of isotropic scaling.\n",
        "        self.rotate_max       = float(rotate_max)       # Range of arbitrary rotation, 1 = full circle.\n",
        "        self.aniso_std        = float(aniso_std)        # Log2 standard deviation of anisotropic scaling.\n",
        "        self.xfrac_std        = float(xfrac_std)        # Standard deviation of frational translation, relative to image dimensions.\n",
        "\n",
        "        # Color transformations.\n",
        "        self.brightness       = float(brightness)       # Probability multiplier for brightness.\n",
        "        self.contrast         = float(contrast)         # Probability multiplier for contrast.\n",
        "        self.lumaflip         = float(lumaflip)         # Probability multiplier for luma flip.\n",
        "        self.hue              = float(hue)              # Probability multiplier for hue rotation.\n",
        "        self.saturation       = float(saturation)       # Probability multiplier for saturation.\n",
        "        self.brightness_std   = float(brightness_std)   # Standard deviation of brightness.\n",
        "        self.contrast_std     = float(contrast_std)     # Log2 standard deviation of contrast.\n",
        "        self.hue_max          = float(hue_max)          # Range of hue rotation, 1 = full circle.\n",
        "        self.saturation_std   = float(saturation_std)   # Log2 standard deviation of saturation.\n",
        "\n",
        "        # Image-space filtering.\n",
        "        self.imgfilter        = float(imgfilter)        # Probability multiplier for image-space filtering.\n",
        "        self.imgfilter_bands  = list(imgfilter_bands)   # Probability multipliers for individual frequency bands.\n",
        "        self.imgfilter_std    = float(imgfilter_std)    # Log2 standard deviation of image-space filter amplification.\n",
        "\n",
        "        # Image-space corruptions.\n",
        "        self.noise            = float(noise)            # Probability multiplier for additive RGB noise.\n",
        "        self.cutout           = float(cutout)           # Probability multiplier for cutout.\n",
        "        self.noise_std        = float(noise_std)        # Standard deviation of additive RGB noise.\n",
        "        self.cutout_size      = float(cutout_size)      # Size of the cutout rectangle, relative to image dimensions.\n",
        "\n",
        "        # Setup orthogonal lowpass filter for geometric augmentations.\n",
        "        self.register_buffer('Hz_geom', upfirdn2d.setup_filter(wavelets['sym6']))\n",
        "\n",
        "        # Construct filter bank for image-space filtering.\n",
        "        Hz_lo = np.asarray(wavelets['sym2'])            # H(z)\n",
        "        Hz_hi = Hz_lo * ((-1) ** np.arange(Hz_lo.size)) # H(-z)\n",
        "        Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2    # H(z) * H(z^-1) / 2\n",
        "        Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2    # H(-z) * H(-z^-1) / 2\n",
        "        Hz_fbank = np.eye(4, 1)                         # Bandpass(H(z), b_i)\n",
        "        for i in range(1, Hz_fbank.shape[0]):\n",
        "            Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]\n",
        "            Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])\n",
        "            Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2 : (Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2\n",
        "        self.register_buffer('Hz_fbank', torch.as_tensor(Hz_fbank, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, images, debug_percentile=None):\n",
        "        assert isinstance(images, torch.Tensor) and images.ndim == 4\n",
        "        batch_size, num_channels, height, width = images.shape\n",
        "        device = images.device\n",
        "        if debug_percentile is not None:\n",
        "            debug_percentile = torch.as_tensor(debug_percentile, dtype=torch.float32, device=device)\n",
        "\n",
        "        # -------------------------------------\n",
        "        # Select parameters for pixel blitting.\n",
        "        # -------------------------------------\n",
        "\n",
        "        # Initialize inverse homogeneous 2D transform: G_inv @ pixel_out ==> pixel_in\n",
        "        I_3 = torch.eye(3, device=device)\n",
        "        G_inv = I_3\n",
        "\n",
        "        # Apply x-flip with probability (xflip * strength).\n",
        "        if self.xflip > 0:\n",
        "            i = torch.floor(torch.rand([batch_size], device=device) * 2)\n",
        "            i = torch.where(torch.rand([batch_size], device=device) < self.xflip * self.p, i, torch.zeros_like(i))\n",
        "            if debug_percentile is not None:\n",
        "                i = torch.full_like(i, torch.floor(debug_percentile * 2))\n",
        "            G_inv = G_inv @ scale2d_inv(1 - 2 * i, 1)\n",
        "\n",
        "        # Apply 90 degree rotations with probability (rotate90 * strength).\n",
        "        if self.rotate90 > 0:\n",
        "            i = torch.floor(torch.rand([batch_size], device=device) * 4)\n",
        "            i = torch.where(torch.rand([batch_size], device=device) < self.rotate90 * self.p, i, torch.zeros_like(i))\n",
        "            if debug_percentile is not None:\n",
        "                i = torch.full_like(i, torch.floor(debug_percentile * 4))\n",
        "            G_inv = G_inv @ rotate2d_inv(-np.pi / 2 * i)\n",
        "\n",
        "        # Apply integer translation with probability (xint * strength).\n",
        "        if self.xint > 0:\n",
        "            t = (torch.rand([batch_size, 2], device=device) * 2 - 1) * self.xint_max\n",
        "            t = torch.where(torch.rand([batch_size, 1], device=device) < self.xint * self.p, t, torch.zeros_like(t))\n",
        "            if debug_percentile is not None:\n",
        "                t = torch.full_like(t, (debug_percentile * 2 - 1) * self.xint_max)\n",
        "            G_inv = G_inv @ translate2d_inv(torch.round(t[:,0] * width), torch.round(t[:,1] * height))\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # Select parameters for general geometric transformations.\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "        # Apply isotropic scaling with probability (scale * strength).\n",
        "        if self.scale > 0:\n",
        "            s = torch.exp2(torch.randn([batch_size], device=device) * self.scale_std)\n",
        "            s = torch.where(torch.rand([batch_size], device=device) < self.scale * self.p, s, torch.ones_like(s))\n",
        "            if debug_percentile is not None:\n",
        "                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.scale_std))\n",
        "            G_inv = G_inv @ scale2d_inv(s, s)\n",
        "\n",
        "        # Apply pre-rotation with probability p_rot.\n",
        "        p_rot = 1 - torch.sqrt((1 - self.rotate * self.p).clamp(0, 1)) # P(pre OR post) = p\n",
        "        if self.rotate > 0:\n",
        "            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max\n",
        "            theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))\n",
        "            if debug_percentile is not None:\n",
        "                theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.rotate_max)\n",
        "            G_inv = G_inv @ rotate2d_inv(-theta) # Before anisotropic scaling.\n",
        "\n",
        "        # Apply anisotropic scaling with probability (aniso * strength).\n",
        "        if self.aniso > 0:\n",
        "            s = torch.exp2(torch.randn([batch_size], device=device) * self.aniso_std)\n",
        "            s = torch.where(torch.rand([batch_size], device=device) < self.aniso * self.p, s, torch.ones_like(s))\n",
        "            if debug_percentile is not None:\n",
        "                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.aniso_std))\n",
        "            G_inv = G_inv @ scale2d_inv(s, 1 / s)\n",
        "\n",
        "        # Apply post-rotation with probability p_rot.\n",
        "        if self.rotate > 0:\n",
        "            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max\n",
        "            theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))\n",
        "            if debug_percentile is not None:\n",
        "                theta = torch.zeros_like(theta)\n",
        "            G_inv = G_inv @ rotate2d_inv(-theta) # After anisotropic scaling.\n",
        "\n",
        "        # Apply fractional translation with probability (xfrac * strength).\n",
        "        if self.xfrac > 0:\n",
        "            t = torch.randn([batch_size, 2], device=device) * self.xfrac_std\n",
        "            t = torch.where(torch.rand([batch_size, 1], device=device) < self.xfrac * self.p, t, torch.zeros_like(t))\n",
        "            if debug_percentile is not None:\n",
        "                t = torch.full_like(t, torch.erfinv(debug_percentile * 2 - 1) * self.xfrac_std)\n",
        "            G_inv = G_inv @ translate2d_inv(t[:,0] * width, t[:,1] * height)\n",
        "\n",
        "        # ----------------------------------\n",
        "        # Execute geometric transformations.\n",
        "        # ----------------------------------\n",
        "\n",
        "        # Execute if the transform is not identity.\n",
        "        if G_inv is not I_3:\n",
        "\n",
        "            # Calculate padding.\n",
        "            cx = (width - 1) / 2\n",
        "            cy = (height - 1) / 2\n",
        "            cp = matrix([-cx, -cy, 1], [cx, -cy, 1], [cx, cy, 1], [-cx, cy, 1], device=device) # [idx, xyz]\n",
        "            cp = G_inv @ cp.t() # [batch, xyz, idx]\n",
        "            Hz_pad = self.Hz_geom.shape[0] // 4\n",
        "            margin = cp[:, :2, :].permute(1, 0, 2).flatten(1) # [xy, batch * idx]\n",
        "            margin = torch.cat([-margin, margin]).max(dim=1).values # [x0, y0, x1, y1]\n",
        "            margin = margin + misc.constant([Hz_pad * 2 - cx, Hz_pad * 2 - cy] * 2, device=device)\n",
        "            margin = margin.max(misc.constant([0, 0] * 2, device=device))\n",
        "            margin = margin.min(misc.constant([width-1, height-1] * 2, device=device))\n",
        "            mx0, my0, mx1, my1 = margin.ceil().to(torch.int32)\n",
        "\n",
        "            # Pad image and adjust origin.\n",
        "            images = torch.nn.functional.pad(input=images, pad=[mx0,mx1,my0,my1], mode='reflect')\n",
        "            G_inv = translate2d((mx0 - mx1) / 2, (my0 - my1) / 2) @ G_inv\n",
        "\n",
        "            # Upsample.\n",
        "            images = upfirdn2d.upsample2d(x=images, f=self.Hz_geom, up=2)\n",
        "            G_inv = scale2d(2, 2, device=device) @ G_inv @ scale2d_inv(2, 2, device=device)\n",
        "            G_inv = translate2d(-0.5, -0.5, device=device) @ G_inv @ translate2d_inv(-0.5, -0.5, device=device)\n",
        "\n",
        "            # Execute transformation.\n",
        "            shape = [batch_size, num_channels, (height + Hz_pad * 2) * 2, (width + Hz_pad * 2) * 2]\n",
        "            G_inv = scale2d(2 / images.shape[3], 2 / images.shape[2], device=device) @ G_inv @ scale2d_inv(2 / shape[3], 2 / shape[2], device=device)\n",
        "            grid = torch.nn.functional.affine_grid(theta=G_inv[:,:2,:], size=shape, align_corners=False)\n",
        "            images = grid_sample_gradfix.grid_sample(images, grid)\n",
        "\n",
        "            # Downsample and crop.\n",
        "            images = upfirdn2d.downsample2d(x=images, f=self.Hz_geom, down=2, padding=-Hz_pad*2, flip_filter=True)\n",
        "\n",
        "        # --------------------------------------------\n",
        "        # Select parameters for color transformations.\n",
        "        # --------------------------------------------\n",
        "\n",
        "        # Initialize homogeneous 3D transformation matrix: C @ color_in ==> color_out\n",
        "        I_4 = torch.eye(4, device=device)\n",
        "        C = I_4\n",
        "\n",
        "        # Apply brightness with probability (brightness * strength).\n",
        "        if self.brightness > 0:\n",
        "            b = torch.randn([batch_size], device=device) * self.brightness_std\n",
        "            b = torch.where(torch.rand([batch_size], device=device) < self.brightness * self.p, b, torch.zeros_like(b))\n",
        "            if debug_percentile is not None:\n",
        "                b = torch.full_like(b, torch.erfinv(debug_percentile * 2 - 1) * self.brightness_std)\n",
        "            C = translate3d(b, b, b) @ C\n",
        "\n",
        "        # Apply contrast with probability (contrast * strength).\n",
        "        if self.contrast > 0:\n",
        "            c = torch.exp2(torch.randn([batch_size], device=device) * self.contrast_std)\n",
        "            c = torch.where(torch.rand([batch_size], device=device) < self.contrast * self.p, c, torch.ones_like(c))\n",
        "            if debug_percentile is not None:\n",
        "                c = torch.full_like(c, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.contrast_std))\n",
        "            C = scale3d(c, c, c) @ C\n",
        "\n",
        "        # Apply luma flip with probability (lumaflip * strength).\n",
        "        v = misc.constant(np.asarray([1, 1, 1, 0]) / np.sqrt(3), device=device) # Luma axis.\n",
        "        if self.lumaflip > 0:\n",
        "            i = torch.floor(torch.rand([batch_size, 1, 1], device=device) * 2)\n",
        "            i = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.lumaflip * self.p, i, torch.zeros_like(i))\n",
        "            if debug_percentile is not None:\n",
        "                i = torch.full_like(i, torch.floor(debug_percentile * 2))\n",
        "            C = (I_4 - 2 * v.ger(v) * i) @ C # Householder reflection.\n",
        "\n",
        "        # Apply hue rotation with probability (hue * strength).\n",
        "        if self.hue > 0 and num_channels > 1:\n",
        "            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.hue_max\n",
        "            theta = torch.where(torch.rand([batch_size], device=device) < self.hue * self.p, theta, torch.zeros_like(theta))\n",
        "            if debug_percentile is not None:\n",
        "                theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.hue_max)\n",
        "            C = rotate3d(v, theta) @ C # Rotate around v.\n",
        "\n",
        "        # Apply saturation with probability (saturation * strength).\n",
        "        if self.saturation > 0 and num_channels > 1:\n",
        "            s = torch.exp2(torch.randn([batch_size, 1, 1], device=device) * self.saturation_std)\n",
        "            s = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.saturation * self.p, s, torch.ones_like(s))\n",
        "            if debug_percentile is not None:\n",
        "                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.saturation_std))\n",
        "            C = (v.ger(v) + (I_4 - v.ger(v)) * s) @ C\n",
        "\n",
        "        # ------------------------------\n",
        "        # Execute color transformations.\n",
        "        # ------------------------------\n",
        "\n",
        "        # Execute if the transform is not identity.\n",
        "        if C is not I_4:\n",
        "            images = images.reshape([batch_size, num_channels, height * width])\n",
        "            if num_channels == 3:\n",
        "                images = C[:, :3, :3] @ images + C[:, :3, 3:]\n",
        "            elif num_channels == 1:\n",
        "                C = C[:, :3, :].mean(dim=1, keepdims=True)\n",
        "                images = images * C[:, :, :3].sum(dim=2, keepdims=True) + C[:, :, 3:]\n",
        "            else:\n",
        "                raise ValueError('Image must be RGB (3 channels) or L (1 channel)')\n",
        "            images = images.reshape([batch_size, num_channels, height, width])\n",
        "\n",
        "        # ----------------------\n",
        "        # Image-space filtering.\n",
        "        # ----------------------\n",
        "\n",
        "        if self.imgfilter > 0:\n",
        "            num_bands = self.Hz_fbank.shape[0]\n",
        "            assert len(self.imgfilter_bands) == num_bands\n",
        "            expected_power = misc.constant(np.array([10, 1, 1, 1]) / 13, device=device) # Expected power spectrum (1/f).\n",
        "\n",
        "            # Apply amplification for each band with probability (imgfilter * strength * band_strength).\n",
        "            g = torch.ones([batch_size, num_bands], device=device) # Global gain vector (identity).\n",
        "            for i, band_strength in enumerate(self.imgfilter_bands):\n",
        "                t_i = torch.exp2(torch.randn([batch_size], device=device) * self.imgfilter_std)\n",
        "                t_i = torch.where(torch.rand([batch_size], device=device) < self.imgfilter * self.p * band_strength, t_i, torch.ones_like(t_i))\n",
        "                if debug_percentile is not None:\n",
        "                    t_i = torch.full_like(t_i, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.imgfilter_std)) if band_strength > 0 else torch.ones_like(t_i)\n",
        "                t = torch.ones([batch_size, num_bands], device=device)                  # Temporary gain vector.\n",
        "                t[:, i] = t_i                                                           # Replace i'th element.\n",
        "                t = t / (expected_power * t.square()).sum(dim=-1, keepdims=True).sqrt() # Normalize power.\n",
        "                g = g * t                                                               # Accumulate into global gain.\n",
        "\n",
        "            # Construct combined amplification filter.\n",
        "            Hz_prime = g @ self.Hz_fbank                                    # [batch, tap]\n",
        "            Hz_prime = Hz_prime.unsqueeze(1).repeat([1, num_channels, 1])   # [batch, channels, tap]\n",
        "            Hz_prime = Hz_prime.reshape([batch_size * num_channels, 1, -1]) # [batch * channels, 1, tap]\n",
        "\n",
        "            # Apply filter.\n",
        "            p = self.Hz_fbank.shape[1] // 2\n",
        "            images = images.reshape([1, batch_size * num_channels, height, width])\n",
        "            images = torch.nn.functional.pad(input=images, pad=[p,p,p,p], mode='reflect')\n",
        "            images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(2), groups=batch_size*num_channels)\n",
        "            images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(3), groups=batch_size*num_channels)\n",
        "            images = images.reshape([batch_size, num_channels, height, width])\n",
        "\n",
        "        # ------------------------\n",
        "        # Image-space corruptions.\n",
        "        # ------------------------\n",
        "\n",
        "        # Apply additive RGB noise with probability (noise * strength).\n",
        "        if self.noise > 0:\n",
        "            sigma = torch.randn([batch_size, 1, 1, 1], device=device).abs() * self.noise_std\n",
        "            sigma = torch.where(torch.rand([batch_size, 1, 1, 1], device=device) < self.noise * self.p, sigma, torch.zeros_like(sigma))\n",
        "            if debug_percentile is not None:\n",
        "                sigma = torch.full_like(sigma, torch.erfinv(debug_percentile) * self.noise_std)\n",
        "            images = images + torch.randn([batch_size, num_channels, height, width], device=device) * sigma\n",
        "\n",
        "        # Apply cutout with probability (cutout * strength).\n",
        "        if self.cutout > 0:\n",
        "            size = torch.full([batch_size, 2, 1, 1, 1], self.cutout_size, device=device)\n",
        "            size = torch.where(torch.rand([batch_size, 1, 1, 1, 1], device=device) < self.cutout * self.p, size, torch.zeros_like(size))\n",
        "            center = torch.rand([batch_size, 2, 1, 1, 1], device=device)\n",
        "            if debug_percentile is not None:\n",
        "                size = torch.full_like(size, self.cutout_size)\n",
        "                center = torch.full_like(center, debug_percentile)\n",
        "            coord_x = torch.arange(width, device=device).reshape([1, 1, 1, -1])\n",
        "            coord_y = torch.arange(height, device=device).reshape([1, 1, -1, 1])\n",
        "            mask_x = (((coord_x + 0.5) / width - center[:, 0]).abs() >= size[:, 0] / 2)\n",
        "            mask_y = (((coord_y + 0.5) / height - center[:, 1]).abs() >= size[:, 1] / 2)\n",
        "            mask = torch.logical_or(mask_x, mask_y).to(torch.float32)\n",
        "            images = images * mask\n",
        "\n",
        "        return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7SJqRCD_3p-"
      },
      "outputs": [],
      "source": [
        "#this is taken from: https://github.com/NVlabs/stylegan3/blob/main/training/training_loop.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGQgraFUzKX5"
      },
      "outputs": [],
      "source": [
        "\"\"\"Main training loop.\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import json\n",
        "import pickle\n",
        "import psutil\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "from torch_utils import training_stats\n",
        "from torch_utils.ops import conv2d_gradfix\n",
        "from torch_utils.ops import grid_sample_gradfix\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def setup_snapshot_image_grid(training_set, random_seed=0):\n",
        "    rnd = np.random.RandomState(random_seed)\n",
        "    gw = np.clip(7680 // training_set.image_shape[2], 7, 32)\n",
        "    gh = np.clip(4320 // training_set.image_shape[1], 4, 32)\n",
        "\n",
        "    # No labels => show random subset of training samples.\n",
        "    if not training_set.has_labels:\n",
        "        all_indices = list(range(len(training_set)))\n",
        "        rnd.shuffle(all_indices)\n",
        "        grid_indices = [all_indices[i % len(all_indices)] for i in range(gw * gh)]\n",
        "\n",
        "    else:\n",
        "        # Group training samples by label.\n",
        "        label_groups = dict() # label => [idx, ...]\n",
        "        for idx in range(len(training_set)):\n",
        "            label = tuple(training_set.get_details(idx).raw_label.flat[::-1])\n",
        "            if label not in label_groups:\n",
        "                label_groups[label] = []\n",
        "            label_groups[label].append(idx)\n",
        "\n",
        "        # Reorder.\n",
        "        label_order = sorted(label_groups.keys())\n",
        "        for label in label_order:\n",
        "            rnd.shuffle(label_groups[label])\n",
        "\n",
        "        # Organize into grid.\n",
        "        grid_indices = []\n",
        "        for y in range(gh):\n",
        "            label = label_order[y % len(label_order)]\n",
        "            indices = label_groups[label]\n",
        "            grid_indices += [indices[x % len(indices)] for x in range(gw)]\n",
        "            label_groups[label] = [indices[(i + gw) % len(indices)] for i in range(len(indices))]\n",
        "\n",
        "    # Load data.\n",
        "    images, labels = zip(*[training_set[i] for i in grid_indices])\n",
        "    return (gw, gh), np.stack(images), np.stack(labels)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def save_image_grid(img, fname, drange, grid_size):\n",
        "    lo, hi = drange\n",
        "    img = np.asarray(img, dtype=np.float32)\n",
        "    img = (img - lo) * (255 / (hi - lo))\n",
        "    img = np.rint(img).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    gw, gh = grid_size\n",
        "    _N, C, H, W = img.shape\n",
        "    img = img.reshape([gh, gw, C, H, W])\n",
        "    img = img.transpose(0, 3, 1, 4, 2)\n",
        "    img = img.reshape([gh * H, gw * W, C])\n",
        "\n",
        "    assert C in [1, 3]\n",
        "    if C == 1:\n",
        "        PIL.Image.fromarray(img[:, :, 0], 'L').save(fname)\n",
        "    if C == 3:\n",
        "        PIL.Image.fromarray(img, 'RGB').save(fname)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def training_loop(\n",
        "    run_dir                 = '.',      # Output directory.\n",
        "    training_set_kwargs     = {},       # Options for training set.\n",
        "    data_loader_kwargs      = {},       # Options for torch.utils.data.DataLoader.\n",
        "    G_kwargs                = {},       # Options for generator network.\n",
        "    D_kwargs                = {},       # Options for discriminator network.\n",
        "    G_opt_kwargs            = {},       # Options for generator optimizer.\n",
        "    D_opt_kwargs            = {},       # Options for discriminator optimizer.\n",
        "    augment_kwargs          = None,     # Options for augmentation pipeline. None = disable.\n",
        "    loss_kwargs             = {},       # Options for loss function.\n",
        "    metrics                 = [],       # Metrics to evaluate during training.\n",
        "    random_seed             = 0,        # Global random seed.\n",
        "    num_gpus                = 1,        # Number of GPUs participating in the training.\n",
        "    rank                    = 0,        # Rank of the current process in [0, num_gpus[.\n",
        "    batch_size              = 4,        # Total batch size for one training iteration. Can be larger than batch_gpu * num_gpus.\n",
        "    batch_gpu               = 4,        # Number of samples processed at a time by one GPU.\n",
        "    ema_kimg                = 10,       # Half-life of the exponential moving average (EMA) of generator weights.\n",
        "    ema_rampup              = 0.05,     # EMA ramp-up coefficient. None = no rampup.\n",
        "    G_reg_interval          = None,     # How often to perform regularization for G? None = disable lazy regularization.\n",
        "    D_reg_interval          = 16,       # How often to perform regularization for D? None = disable lazy regularization.\n",
        "    augment_p               = 0,        # Initial value of augmentation probability.\n",
        "    ada_target              = None,     # ADA target value. None = fixed p.\n",
        "    ada_interval            = 4,        # How often to perform ADA adjustment?\n",
        "    ada_kimg                = 500,      # ADA adjustment speed, measured in how many kimg it takes for p to increase/decrease by one unit.\n",
        "    total_kimg              = 25000,    # Total length of the training, measured in thousands of real images.\n",
        "    kimg_per_tick           = 4,        # Progress snapshot interval.\n",
        "    image_snapshot_ticks    = 50,       # How often to save image snapshots? None = disable.\n",
        "    network_snapshot_ticks  = 50,       # How often to save network snapshots? None = disable.\n",
        "    resume_pkl              = None,     # Network pickle to resume training from.\n",
        "    resume_kimg             = 0,        # First kimg to report when resuming training.\n",
        "    cudnn_benchmark         = True,     # Enable torch.backends.cudnn.benchmark?\n",
        "    abort_fn                = None,     # Callback function for determining whether to abort training. Must return consistent results across ranks.\n",
        "    progress_fn             = None,     # Callback function for updating training progress. Called for all ranks.\n",
        "):\n",
        "    # Initialize.\n",
        "    start_time = time.time()\n",
        "    device = torch.device('cuda', rank)\n",
        "    np.random.seed(random_seed * num_gpus + rank)\n",
        "    torch.manual_seed(random_seed * num_gpus + rank)\n",
        "    torch.backends.cudnn.benchmark = cudnn_benchmark    # Improves training speed.\n",
        "    torch.backends.cuda.matmul.allow_tf32 = False       # Improves numerical accuracy.\n",
        "    torch.backends.cudnn.allow_tf32 = False             # Improves numerical accuracy.\n",
        "    conv2d_gradfix.enabled = True                       # Improves training speed.\n",
        "    grid_sample_gradfix.enabled = True                  # Avoids errors with the augmentation pipe.\n",
        "\n",
        "    # Load training set.\n",
        "    training_set = construct_class_by_name(**training_set_kwargs) # subclass of training.dataset.Dataset\n",
        "    training_set_sampler = misc.InfiniteSampler(dataset=training_set, rank=rank, num_replicas=num_gpus, seed=random_seed)\n",
        "    training_set_iterator = iter(torch.utils.data.DataLoader(dataset=training_set, sampler=training_set_sampler, batch_size=batch_size//num_gpus, **data_loader_kwargs))\n",
        "\n",
        "    # Construct networks.\n",
        "    common_kwargs = dict(c_dim=training_set.label_dim, img_resolution=training_set.resolution, img_channels=training_set.num_channels)\n",
        "    G = construct_class_by_name(**G_kwargs, **common_kwargs).train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
        "    D = construct_class_by_name(**D_kwargs, **common_kwargs).train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
        "    G_ema = copy.deepcopy(G).eval()\n",
        "\n",
        "    # Resume from existing pickle.\n",
        "    if (resume_pkl is not None) and (rank == 0):\n",
        "        with open_url(resume_pkl) as f:\n",
        "            resume_data = load_network_pkl(f)\n",
        "        for name, module in [('G', G), ('D', D), ('G_ema', G_ema)]:\n",
        "            misc.copy_params_and_buffers(resume_data[name], module, require_all=False)\n",
        "\n",
        "    # Print network summary tables.\n",
        "    if rank == 0:\n",
        "        z = torch.empty([batch_gpu, G.z_dim], device=device)\n",
        "        c = torch.empty([batch_gpu, G.c_dim], device=device)\n",
        "        img = misc.print_module_summary(G, [z, c])\n",
        "        misc.print_module_summary(D, [img, c])\n",
        "\n",
        "    # Setup augmentation.\n",
        "    augment_pipe = None\n",
        "    ada_stats = None\n",
        "    if (augment_kwargs is not None) and (augment_p > 0 or ada_target is not None):\n",
        "        augment_pipe = construct_class_by_name(**augment_kwargs).train().requires_grad_(False).to(device) # subclass of torch.nn.Module\n",
        "        augment_pipe.p.copy_(torch.as_tensor(augment_p))\n",
        "        if ada_target is not None:\n",
        "            ada_stats = training_stats.Collector(regex='Loss/signs/real')\n",
        "\n",
        "    # Distribute across GPUs.\n",
        "    for module in [G, D, G_ema, augment_pipe]:\n",
        "        if module is not None:\n",
        "            for param in misc.params_and_buffers(module):\n",
        "                if param.numel() > 0 and num_gpus > 1:\n",
        "                    torch.distributed.broadcast(param, src=0)\n",
        "\n",
        "    # Setup training phases.\n",
        "    loss = construct_class_by_name(device=device, G=G, D=D, augment_pipe=augment_pipe, **loss_kwargs) # subclass of training.loss.Loss\n",
        "    phases = []\n",
        "    for name, module, opt_kwargs, reg_interval in [('G', G, G_opt_kwargs, G_reg_interval), ('D', D, D_opt_kwargs, D_reg_interval)]:\n",
        "        if reg_interval is None:\n",
        "            opt = construct_class_by_name(params=module.parameters(), **opt_kwargs) # subclass of torch.optim.Optimizer\n",
        "            phases += [EasyDict(name=name+'both', module=module, opt=opt, interval=1)]\n",
        "        else: # Lazy regularization.\n",
        "            mb_ratio = reg_interval / (reg_interval + 1)\n",
        "            opt_kwargs = EasyDict(opt_kwargs)\n",
        "            opt_kwargs.lr = opt_kwargs.lr * mb_ratio\n",
        "            opt_kwargs.betas = [beta ** mb_ratio for beta in opt_kwargs.betas]\n",
        "            opt = construct_class_by_name(module.parameters(), **opt_kwargs) # subclass of torch.optim.Optimizer\n",
        "            phases += [EasyDict(name=name+'main', module=module, opt=opt, interval=1)]\n",
        "            phases += [EasyDict(name=name+'reg', module=module, opt=opt, interval=reg_interval)]\n",
        "    for phase in phases:\n",
        "        phase.start_event = None\n",
        "        phase.end_event = None\n",
        "        if rank == 0:\n",
        "            phase.start_event = torch.cuda.Event(enable_timing=True)\n",
        "            phase.end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # Export sample images.\n",
        "    grid_size = None\n",
        "    grid_z = None\n",
        "    grid_c = None\n",
        "    if rank == 0:\n",
        "        grid_size, images, labels = setup_snapshot_image_grid(training_set=training_set)\n",
        "        save_image_grid(images, os.path.join(run_dir, 'reals.jpg'), drange=[0,255], grid_size=grid_size)\n",
        "        grid_z = torch.randn([labels.shape[0], G.z_dim], device=device).split(batch_gpu)\n",
        "        grid_c = torch.from_numpy(labels).to(device).split(batch_gpu)\n",
        "        images = torch.cat([G_ema(z=z, c=c, noise_mode='const').cpu() for z, c in zip(grid_z, grid_c)]).numpy()\n",
        "        save_image_grid(images, os.path.join(run_dir, 'fakes_init.jpg'), drange=[-1,1], grid_size=grid_size)\n",
        "\n",
        "    # Initialize logs.\n",
        "    if rank == 0:\n",
        "        print('Initializing logs...')\n",
        "    stats_collector = training_stats.Collector(regex='.*')\n",
        "    stats_metrics = dict()\n",
        "    stats_jsonl = None\n",
        "    stats_tfevents = None\n",
        "    if rank == 0:\n",
        "        stats_jsonl = open(os.path.join(run_dir, 'stats.jsonl'), 'wt')\n",
        "        try:\n",
        "            import torch.utils.tensorboard as tensorboard\n",
        "            stats_tfevents = tensorboard.SummaryWriter(run_dir)\n",
        "        except ImportError as err:\n",
        "            print('Skipping tfevents export:', err)\n",
        "\n",
        "    # Train.\n",
        "    cur_nimg = resume_kimg * 1000\n",
        "    cur_tick = 0\n",
        "    tick_start_nimg = cur_nimg\n",
        "    tick_start_time = time.time()\n",
        "    maintenance_time = tick_start_time - start_time\n",
        "    batch_idx = 0\n",
        "    if progress_fn is not None:\n",
        "        progress_fn(0, total_kimg)\n",
        "    while True:\n",
        "\n",
        "        # Fetch training data.\n",
        "        with torch.autograd.profiler.record_function('data_fetch'):\n",
        "            phase_real_img, phase_real_c = next(training_set_iterator)\n",
        "            phase_real_img = (phase_real_img.to(device).to(torch.float32) / 127.5 - 1).split(batch_gpu)\n",
        "            phase_real_c = phase_real_c.to(device).split(batch_gpu)\n",
        "            all_gen_z = torch.randn([len(phases) * batch_size, G.z_dim], device=device)\n",
        "            all_gen_z = [phase_gen_z.split(batch_gpu) for phase_gen_z in all_gen_z.split(batch_size)]\n",
        "            all_gen_c = [training_set.get_label(np.random.randint(len(training_set))) for _ in range(len(phases) * batch_size)]\n",
        "            all_gen_c = torch.from_numpy(np.stack(all_gen_c)).pin_memory().to(device)\n",
        "            all_gen_c = [phase_gen_c.split(batch_gpu) for phase_gen_c in all_gen_c.split(batch_size)]\n",
        "\n",
        "        # Execute training phases.\n",
        "        for phase, phase_gen_z, phase_gen_c in zip(phases, all_gen_z, all_gen_c):\n",
        "            if batch_idx % phase.interval != 0:\n",
        "                continue\n",
        "            if phase.start_event is not None:\n",
        "                phase.start_event.record(torch.cuda.current_stream(device))\n",
        "\n",
        "            # Accumulate gradients.\n",
        "            phase.opt.zero_grad(set_to_none=True)\n",
        "            phase.module.requires_grad_(True)\n",
        "            for real_img, real_c, gen_z, gen_c in zip(phase_real_img, phase_real_c, phase_gen_z, phase_gen_c):\n",
        "                loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)\n",
        "            phase.module.requires_grad_(False)\n",
        "\n",
        "            # Update weights.\n",
        "            with torch.autograd.profiler.record_function(phase.name + '_opt'):\n",
        "                params = [param for param in phase.module.parameters() if param.numel() > 0 and param.grad is not None]\n",
        "                if len(params) > 0:\n",
        "                    flat = torch.cat([param.grad.flatten() for param in params])\n",
        "                    if num_gpus > 1:\n",
        "                        torch.distributed.all_reduce(flat)\n",
        "                        flat /= num_gpus\n",
        "                    misc.nan_to_num(flat, nan=0, posinf=1e5, neginf=-1e5, out=flat)\n",
        "                    grads = flat.split([param.numel() for param in params])\n",
        "                    for param, grad in zip(params, grads):\n",
        "                        param.grad = grad.reshape(param.shape)\n",
        "                phase.opt.step()\n",
        "\n",
        "            # Phase done.\n",
        "            if phase.end_event is not None:\n",
        "                phase.end_event.record(torch.cuda.current_stream(device))\n",
        "\n",
        "        # Update G_ema.\n",
        "        with torch.autograd.profiler.record_function('Gema'):\n",
        "            ema_nimg = ema_kimg * 1000\n",
        "            if ema_rampup is not None:\n",
        "                ema_nimg = min(ema_nimg, cur_nimg * ema_rampup)\n",
        "            ema_beta = 0.5 ** (batch_size / max(ema_nimg, 1e-8))\n",
        "            for p_ema, p in zip(G_ema.parameters(), G.parameters()):\n",
        "                p_ema.copy_(p.lerp(p_ema, ema_beta))\n",
        "            for b_ema, b in zip(G_ema.buffers(), G.buffers()):\n",
        "                b_ema.copy_(b)\n",
        "\n",
        "        # Update state.\n",
        "        cur_nimg += batch_size\n",
        "        batch_idx += 1\n",
        "\n",
        "        # Execute ADA heuristic.\n",
        "        if (ada_stats is not None) and (batch_idx % ada_interval == 0):\n",
        "            ada_stats.update()\n",
        "            adjust = np.sign(ada_stats['Loss/signs/real'] - ada_target) * (batch_size * ada_interval) / (ada_kimg * 1000)\n",
        "            augment_pipe.p.copy_((augment_pipe.p + adjust).max(misc.constant(0, device=device)))\n",
        "\n",
        "        # Perform maintenance tasks once per tick.\n",
        "        done = (cur_nimg >= total_kimg * 1000)\n",
        "        if (not done) and (cur_tick != 0) and (cur_nimg < tick_start_nimg + kimg_per_tick * 1000):\n",
        "            continue\n",
        "\n",
        "        # Print status line, accumulating the same information in training_stats.\n",
        "        tick_end_time = time.time()\n",
        "        fields = []\n",
        "        fields += [f\"tick {training_stats.report0('Progress/tick', cur_tick):<5d}\"]\n",
        "        fields += [f\"kimg {training_stats.report0('Progress/kimg', cur_nimg / 1e3):<8.1f}\"]\n",
        "        fields += [f\"time {format_time(training_stats.report0('Timing/total_sec', tick_end_time - start_time)):<12s}\"]\n",
        "        fields += [f\"sec/tick {training_stats.report0('Timing/sec_per_tick', tick_end_time - tick_start_time):<7.1f}\"]\n",
        "        fields += [f\"sec/kimg {training_stats.report0('Timing/sec_per_kimg', (tick_end_time - tick_start_time) / (cur_nimg - tick_start_nimg) * 1e3):<7.2f}\"]\n",
        "        fields += [f\"maintenance {training_stats.report0('Timing/maintenance_sec', maintenance_time):<6.1f}\"]\n",
        "        fields += [f\"cpumem {training_stats.report0('Resources/cpu_mem_gb', psutil.Process(os.getpid()).memory_info().rss / 2**30):<6.2f}\"]\n",
        "        fields += [f\"gpumem {training_stats.report0('Resources/peak_gpu_mem_gb', torch.cuda.max_memory_allocated(device) / 2**30):<6.2f}\"]\n",
        "        fields += [f\"reserved {training_stats.report0('Resources/peak_gpu_mem_reserved_gb', torch.cuda.max_memory_reserved(device) / 2**30):<6.2f}\"]\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        fields += [f\"augment {training_stats.report0('Progress/augment', float(augment_pipe.p.cpu()) if augment_pipe is not None else 0):.3f}\"]\n",
        "        training_stats.report0('Timing/total_hours', (tick_end_time - start_time) / (60 * 60))\n",
        "        training_stats.report0('Timing/total_days', (tick_end_time - start_time) / (24 * 60 * 60))\n",
        "        if rank == 0:\n",
        "            print(' '.join(fields))\n",
        "\n",
        "        # Check for abort.\n",
        "        if (not done) and (abort_fn is not None) and abort_fn():\n",
        "            done = True\n",
        "\n",
        "        # Save image snapshot.\n",
        "        if (rank == 0) and (image_snapshot_ticks is not None) and (done or cur_tick % image_snapshot_ticks == 0):\n",
        "            images = torch.cat([G_ema(z=z, c=c, noise_mode='const').cpu() for z, c in zip(grid_z, grid_c)]).numpy()\n",
        "            save_image_grid(images, os.path.join(run_dir, f'fakes{cur_nimg//1000:06d}.jpg'), drange=[-1,1], grid_size=grid_size)\n",
        "\n",
        "        # Save network snapshot.\n",
        "        snapshot_pkl = None\n",
        "        snapshot_data = None\n",
        "        if (network_snapshot_ticks is not None) and (done or cur_tick % network_snapshot_ticks == 0):\n",
        "            snapshot_data = dict(training_set_kwargs=dict(training_set_kwargs))\n",
        "            for name, module in [('G', G), ('D', D), ('G_ema', G_ema), ('augment_pipe', augment_pipe)]:\n",
        "                if module is not None:\n",
        "                    if num_gpus > 1:\n",
        "                        misc.check_ddp_consistency(module, ignore_regex=r'.*\\.[^.]+_(avg|ema)')\n",
        "                    module = copy.deepcopy(module).eval().requires_grad_(False).cpu()\n",
        "                snapshot_data[name] = module\n",
        "                del module # conserve memory\n",
        "            snapshot_pkl = os.path.join(run_dir, f'network-snapshot-{cur_nimg//1000:06d}.pkl')\n",
        "            if rank == 0:\n",
        "                with open(snapshot_pkl, 'wb') as f:\n",
        "                    pickle.dump(snapshot_data, f)\n",
        "\n",
        "        # Evaluate metrics.\n",
        "        if (snapshot_data is not None) and (len(metrics) > 0):\n",
        "            if rank == 0:\n",
        "                print('Evaluating metrics...')\n",
        "            for metric in metrics:\n",
        "                result_dict = calc_metric(metric=metric, G=snapshot_data['G_ema'],\n",
        "                    dataset_kwargs=training_set_kwargs, num_gpus=num_gpus, rank=rank, device=device)\n",
        "                if rank == 0:\n",
        "                    report_metric(result_dict, run_dir=run_dir, snapshot_pkl=snapshot_pkl)\n",
        "                stats_metrics.update(result_dict.results)\n",
        "        del snapshot_data # conserve memory\n",
        "\n",
        "        # Collect statistics.\n",
        "        for phase in phases:\n",
        "            value = []\n",
        "            if (phase.start_event is not None) and (phase.end_event is not None):\n",
        "                phase.end_event.synchronize()\n",
        "                value = phase.start_event.elapsed_time(phase.end_event)\n",
        "            training_stats.report0('Timing/' + phase.name, value)\n",
        "        stats_collector.update()\n",
        "        stats_dict = stats_collector.as_dict()\n",
        "\n",
        "        # Update logs.\n",
        "        timestamp = time.time()\n",
        "        if stats_jsonl is not None:\n",
        "            fields = dict(stats_dict, timestamp=timestamp)\n",
        "            stats_jsonl.write(json.dumps(fields) + '\\n')\n",
        "            stats_jsonl.flush()\n",
        "        if stats_tfevents is not None:\n",
        "            global_step = int(cur_nimg / 1e3)\n",
        "            walltime = timestamp - start_time\n",
        "            for name, value in stats_dict.items():\n",
        "                stats_tfevents.add_scalar(name, value.mean, global_step=global_step, walltime=walltime)\n",
        "            for name, value in stats_metrics.items():\n",
        "                stats_tfevents.add_scalar(f'Metrics/{name}', value, global_step=global_step, walltime=walltime)\n",
        "            stats_tfevents.flush()\n",
        "        if progress_fn is not None:\n",
        "            progress_fn(cur_nimg // 1000, total_kimg)\n",
        "\n",
        "        # Update state.\n",
        "        cur_tick += 1\n",
        "        tick_start_nimg = cur_nimg\n",
        "        tick_start_time = time.time()\n",
        "        maintenance_time = tick_start_time - tick_end_time\n",
        "        if done:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E6pvD0v_vgK"
      },
      "outputs": [],
      "source": [
        "#this is taken from: https://github.com/NVlabs/stylegan3/blob/main/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "edEDjKb9q5P9",
        "outputId": "6acf2b84-4320-4573-a4ec-743d95b461be"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-49229730d8a6>\"\u001b[0;36m, line \u001b[0;32m224\u001b[0m\n\u001b[0;31m    main() --outdir=\"/content/drive/MyDrive/results\" --cfg=stylegan3-r --data='/content/drive/MyDrive/mydataset.zip'\\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to operator\n"
          ]
        }
      ],
      "source": [
        "#train.py\n",
        "\"\"\"Train a GAN using the techniques described in the paper\n",
        "\"Alias-Free Generative Adversarial Networks\".\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import click\n",
        "import re\n",
        "import json\n",
        "import tempfile\n",
        "import torch\n",
        "\n",
        "from torch_utils import training_stats\n",
        "from torch_utils import custom_ops\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def subprocess_fn(rank, c, temp_dir):\n",
        "    Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n",
        "\n",
        "    # Init torch.distributed.\n",
        "    if c.num_gpus > 1:\n",
        "        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n",
        "        if os.name == 'nt':\n",
        "            init_method = 'file:///' + init_file.replace('\\\\', '/')\n",
        "            torch.distributed.init_process_group(backend='gloo', init_method=init_method, rank=rank, world_size=c.num_gpus)\n",
        "        else:\n",
        "            init_method = f'file://{init_file}'\n",
        "            torch.distributed.init_process_group(backend='nccl', init_method=init_method, rank=rank, world_size=c.num_gpus)\n",
        "\n",
        "    # Init torch_utils.\n",
        "    sync_device = torch.device('cuda', rank) if c.num_gpus > 1 else None\n",
        "    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n",
        "    if rank != 0:\n",
        "        custom_ops.verbosity = 'none'\n",
        "\n",
        "    # Execute training loop.\n",
        "    training_loop(rank=rank, **c)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def launch_training(c, desc, outdir, dry_run):\n",
        "    Logger(should_flush=True)\n",
        "\n",
        "    # Pick output directory.\n",
        "    prev_run_dirs = []\n",
        "    if os.path.isdir(outdir):\n",
        "        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, x))]\n",
        "    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n",
        "    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n",
        "    cur_run_id = max(prev_run_ids, default=-1) + 1\n",
        "    c.run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{desc}')\n",
        "    assert not os.path.exists(c.run_dir)\n",
        "\n",
        "    os.makedirs(c.run_dir)\n",
        "    with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n",
        "        json.dump(c, f, indent=2)\n",
        "\n",
        "    torch.multiprocessing.set_start_method('spawn')\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        if c.num_gpus == 1:\n",
        "            subprocess_fn(rank=0, c=c, temp_dir=temp_dir)\n",
        "        else:\n",
        "            torch.multiprocessing.spawn(fn=subprocess_fn, args=(c, temp_dir), nprocs=c.num_gpus)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def init_dataset_kwargs(data):\n",
        "    try:\n",
        "        dataset_kwargs = EasyDict(class_name='training.dataset.ImageFolderDataset', path=data, use_labels=True, max_size=None, xflip=False)\n",
        "        dataset_obj =construct_class_by_name(**dataset_kwargs) # Subclass of training.dataset.Dataset.\n",
        "        dataset_kwargs.resolution = dataset_obj.resolution # Be explicit about resolution.\n",
        "        dataset_kwargs.use_labels = dataset_obj.has_labels # Be explicit about labels.\n",
        "        dataset_kwargs.max_size = len(dataset_obj) # Be explicit about dataset size.\n",
        "        return dataset_kwargs, dataset_obj.name\n",
        "    except IOError as err:\n",
        "        pass\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def parse_comma_separated_list(s):\n",
        "    if isinstance(s, list):\n",
        "        return s\n",
        "    if s is None or s.lower() == 'none' or s == '':\n",
        "        return []\n",
        "    return s.split(',')\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "# Finds the latest pkl file in the `outdir`, including its kimg number.\n",
        "# Reimplementation of https://github.com/skyflynil/stylegan2/commit/8c57ee4633d334e480a23d7f82433c7649d50866\n",
        "def locate_latest_pkl(outdir: str):\n",
        "    allpickles = sorted(glob.glob(os.path.join(outdir, '0*', 'network-*.pkl')))\n",
        "    latest_pkl = allpickles[-1]\n",
        "    RE_KIMG = re.compile('network-snapshot-(\\d+).pkl')\n",
        "    latest_kimg = int(RE_KIMG.match(os.path.basename(latest_pkl)).group(1))\n",
        "    return latest_pkl, latest_kimg\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@click.command()\n",
        "\n",
        "\n",
        "# Optional features.\n",
        "@click.option('--cond',         help='Train conditional model', metavar='BOOL',                 type=bool, default=False, show_default=True)\n",
        "@click.option('--aug',          help='Augmentation mode',                                       type=click.Choice(['noaug', 'ada', 'fixed']), default='ada', show_default=True)\n",
        "@click.option('--augpipe',      help='Augmentation pipeline',                                   type=click.Choice(['b','bg', 'bgc']), default='bgc', show_default=True)\n",
        "@click.option('--resume',       help='Resume from given network pickle (PATH, URL or \"latest\")', metavar='[PATH|URL|\"latest\"]',  type=str)\n",
        "@click.option('--initstrength', help='Override ADA strength at start',                          type=click.FloatRange(min=0))\n",
        "\n",
        "# Misc hyperparameters.\n",
        "@click.option('--p',            help='Probability for --aug=fixed', metavar='FLOAT',            type=click.FloatRange(min=0, max=1), default=0.2, show_default=True)\n",
        "@click.option('--target',       help='Target value for --aug=ada', metavar='FLOAT',             type=click.FloatRange(min=0, max=1), default=0.6, show_default=True)\n",
        "@click.option('--glr',          help='G learning rate  [default: varies]', metavar='FLOAT',     type=click.FloatRange(min=0))\n",
        "@click.option('--map-depth',    help='Mapping network depth  [default: varies]', metavar='INT', type=click.IntRange(min=1))\n",
        "@click.option('--mbstd-group',  help='Minibatch std group size', metavar='INT',                 type=click.IntRange(min=1), default=4, show_default=True)\n",
        "\n",
        "# Misc settings.\n",
        "@click.option('--desc',         help='String to include in result dir name', metavar='STR',     type=str)\n",
        "@click.option('--kimg',         help='Total training duration', metavar='KIMG',                 type=click.IntRange(min=1), default=25000, show_default=True)\n",
        "@click.option('--fp32',         help='Disable mixed-precision', metavar='BOOL',                 type=bool, default=False, show_default=True)\n",
        "@click.option('--nobench',      help='Disable cuDNN benchmarking', metavar='BOOL',              type=bool, default=False, show_default=True)\n",
        "@click.option('-n','--dry-run', help='Print training options and exit',                         is_flag=True)\n",
        "\n",
        "def main(**kwargs):\n",
        "\n",
        "\n",
        "    # Initialize config.\n",
        "    opts = EasyDict(kwargs) # Command line arguments.\n",
        "    c = EasyDict() # Main config dict.\n",
        "    c.G_kwargs = EasyDict(class_name=None, z_dim=512, w_dim=512, mapping_kwargs=EasyDict())\n",
        "    c.D_kwargs = EasyDict(class_name='Discriminator', block_kwargs=EasyDict(), mapping_kwargs=EasyDict(), epilogue_kwargs=EasyDict())\n",
        "    c.G_opt_kwargs = EasyDict(class_name='torch.optim.Adam', betas=[0,0.99], eps=1e-8)\n",
        "    c.D_opt_kwargs = EasyDict(class_name='torch.optim.Adam', betas=[0,0.99], eps=1e-8)\n",
        "    c.loss_kwargs = EasyDict(class_name='training.loss.StyleGAN2Loss')\n",
        "    c.data_loader_kwargs = EasyDict(pin_memory=True, prefetch_factor=2)\n",
        "\n",
        "    # Training set.\n",
        "    c.training_set_kwargs, dataset_name = init_dataset_kwargs(data='/content/drive/MyDrive/mydataset.zip')\n",
        "    if opts.cond and not c.training_set_kwargs.use_labels:\n",
        "        raise click.ClickException('--cond=True requires labels specified in dataset.json')\n",
        "    c.training_set_kwargs.use_labels = opts.cond\n",
        "    c.training_set_kwargs.xflip = 1\n",
        "    c.training_set_kwargs.yflip = False\n",
        "\n",
        "    # Hyperparameters & settings.\n",
        "    c.num_gpus = 1\n",
        "    c.batch_size = 32\n",
        "    c.batch_gpu = 4\n",
        "    c.G_kwargs.channel_base = c.D_kwargs.channel_base = 32768\n",
        "    c.G_kwargs.channel_max = c.D_kwargs.channel_max = 512\n",
        "    c.G_kwargs.mapping_kwargs.num_layers = 2\n",
        "    c.D_kwargs.block_kwargs.freeze_layers = 0\n",
        "    c.D_kwargs.epilogue_kwargs.mbstd_group_size = 4\n",
        "    c.loss_kwargs.r1_gamma = 8\n",
        "    c.G_opt_kwargs.lr = 0.0025\n",
        "    c.D_opt_kwargs.lr = 0.002\n",
        "    c.metrics = 'fid50k_full'\n",
        "    c.total_kimg = 100\n",
        "    c.kimg_per_tick = 4\n",
        "    c.image_snapshot_ticks = c.network_snapshot_ticks = 1\n",
        "    c.random_seed = c.training_set_kwargs.random_seed = 0\n",
        "    c.data_loader_kwargs.num_workers = 3\n",
        "\n",
        "    # Base configuration.\n",
        "    c.ema_kimg = c.batch_size * 10 / 32\n",
        "    c.G_kwargs.class_name = 'Generator3'\n",
        "    c.G_kwargs.magnitude_ema_beta = 0.5 ** (c.batch_size / (20 * 1e3))\n",
        "    if opts.cfg == 'stylegan3-r':\n",
        "        c.G_kwargs.conv_kernel = 1 # Use 1x1 convolutions.\n",
        "        c.G_kwargs.channel_base *= 2 # Double the number of feature maps.\n",
        "        c.G_kwargs.channel_max *= 2\n",
        "        c.G_kwargs.use_radial_filters = True # Use radially symmetric downsampling filters.\n",
        "        c.loss_kwargs.blur_init_sigma = 10 # Blur the images seen by the discriminator.\n",
        "        c.loss_kwargs.blur_fade_kimg = c.batch_size * 200 / 32 # Fade out the blur during the first N kimg.\n",
        "\n",
        "    # Augmentation.\n",
        "    if opts.aug != 'noaug':\n",
        "        if opts.augpipe == 'bg':\n",
        "            # xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1\n",
        "            c.augment_kwargs = EasyDict(class_name='training.augment.AugmentPipe', xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0)\n",
        "        elif opts.augpipe == 'b':\n",
        "            # xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1\n",
        "            c.augment_kwargs = EasyDict(class_name='training.augment.AugmentPipe', xflip=0, rotate90=0, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0)\n",
        "        else:\n",
        "            c.augment_kwargs = EasyDict(class_name='training.augment.AugmentPipe', xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1)\n",
        "        if opts.aug == 'ada':\n",
        "            c.ada_target = opts.target\n",
        "        if opts.aug == 'fixed':\n",
        "            c.augment_p = opts.p\n",
        "\n",
        "    # Initial Augmentation Strength.\n",
        "    if opts.initstrength is not None:\n",
        "        assert isinstance(opts.initstrength, float)\n",
        "        c.augment_p = opts.initstrength\n",
        "\n",
        "    # Resume.\n",
        "    if opts.resume is not None:\n",
        "        if opts.resume == \"latest\":\n",
        "            c.resume_pkl, c.resume_kimg = locate_latest_pkl(\"/content/drive/MyDrive/results\")\n",
        "        else:\n",
        "            c.resume_pkl = opts.resume\n",
        "        c.ada_kimg = 100 # Make ADA react faster at the beginning.\n",
        "        c.ema_rampup = None # Disable EMA rampup.\n",
        "        c.loss_kwargs.blur_init_sigma = 0 # Disable blur rampup.\n",
        "\n",
        "    # Performance-related toggles.\n",
        "    if opts.fp32:\n",
        "        c.G_kwargs.num_fp16_res = c.D_kwargs.num_fp16_res = 0\n",
        "        c.G_kwargs.conv_clamp = c.D_kwargs.conv_clamp = None\n",
        "    if opts.nobench:\n",
        "        c.cudnn_benchmark = False\n",
        "\n",
        "    # Description string.\n",
        "    desc = f'{opts.cfg:s}-{dataset_name:s}-gpus{c.num_gpus:d}-batch{c.batch_size:d}-gamma{c.loss_kwargs.r1_gamma:g}'\n",
        "    if opts.desc is not None:\n",
        "        desc += f'-{opts.desc}'\n",
        "\n",
        "    # Launch.\n",
        "    launch_training(c=c, desc=desc, outdir=\"/content/drive/MyDrive/results\", dry_run=opts.dry_run)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "main() # pylint: disable=no-value-for-parameter\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YMK6ADrN1_O"
      },
      "source": [
        "## Image Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPfIbo_uNt6R"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p3pLcrt_lFw"
      },
      "outputs": [],
      "source": [
        "#this is taken from: https://github.com/NVlabs/stylegan3/blob/main/legacy.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptsjtN78AKP"
      },
      "outputs": [],
      "source": [
        "import click\n",
        "import pickle\n",
        "import re\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def load_network_pkl(f, force_fp16=False):\n",
        "    data = _LegacyUnpickler(f).load()\n",
        "\n",
        "    # Legacy TensorFlow pickle => convert.\n",
        "    if isinstance(data, tuple) and len(data) == 3 and all(isinstance(net, _TFNetworkStub) for net in data):\n",
        "        tf_G, tf_D, tf_Gs = data\n",
        "        G = convert_tf_generator(tf_G)\n",
        "        D = convert_tf_discriminator(tf_D)\n",
        "        G_ema = convert_tf_generator(tf_Gs)\n",
        "        data = dict(G=G, D=D, G_ema=G_ema)\n",
        "\n",
        "    # Add missing fields.\n",
        "    if 'training_set_kwargs' not in data:\n",
        "        data['training_set_kwargs'] = None\n",
        "    if 'augment_pipe' not in data:\n",
        "        data['augment_pipe'] = None\n",
        "\n",
        "    # Validate contents.\n",
        "    assert isinstance(data['G'], torch.nn.Module)\n",
        "    assert isinstance(data['D'], torch.nn.Module)\n",
        "    assert isinstance(data['G_ema'], torch.nn.Module)\n",
        "    assert isinstance(data['training_set_kwargs'], (dict, type(None)))\n",
        "    assert isinstance(data['augment_pipe'], (torch.nn.Module, type(None)))\n",
        "\n",
        "    # Force FP16.\n",
        "    if force_fp16:\n",
        "        for key in ['G', 'D', 'G_ema']:\n",
        "            old = data[key]\n",
        "            kwargs = copy.deepcopy(old.init_kwargs)\n",
        "            fp16_kwargs = kwargs.get('synthesis_kwargs', kwargs)\n",
        "            fp16_kwargs.num_fp16_res = 4\n",
        "            fp16_kwargs.conv_clamp = 256\n",
        "            if kwargs != old.init_kwargs:\n",
        "                new = type(old)(**kwargs).eval().requires_grad_(False)\n",
        "                misc.copy_params_and_buffers(old, new, require_all=True)\n",
        "                data[key] = new\n",
        "    return data\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "class _TFNetworkStub(EasyDict):\n",
        "    pass\n",
        "\n",
        "class _LegacyUnpickler(pickle.Unpickler):\n",
        "    def find_class(self, module, name):\n",
        "        if module == 'tflib.network' and name == 'Network':\n",
        "            return _TFNetworkStub\n",
        "        return super().find_class(module, name)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _collect_tf_params(tf_net):\n",
        "    # pylint: disable=protected-access\n",
        "    tf_params = dict()\n",
        "    def recurse(prefix, tf_net):\n",
        "        for name, value in tf_net.variables:\n",
        "            tf_params[prefix + name] = value\n",
        "        for name, comp in tf_net.components.items():\n",
        "            recurse(prefix + name + '/', comp)\n",
        "    recurse('', tf_net)\n",
        "    return tf_params\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def _populate_module_params(module, *patterns):\n",
        "    for name, tensor in misc.named_params_and_buffers(module):\n",
        "        found = False\n",
        "        value = None\n",
        "        for pattern, value_fn in zip(patterns[0::2], patterns[1::2]):\n",
        "            match = re.fullmatch(pattern, name)\n",
        "            if match:\n",
        "                found = True\n",
        "                if value_fn is not None:\n",
        "                    value = value_fn(*match.groups())\n",
        "                break\n",
        "        try:\n",
        "            assert found\n",
        "            if value is not None:\n",
        "                tensor.copy_(torch.from_numpy(np.array(value)))\n",
        "        except:\n",
        "            print(name, list(tensor.shape))\n",
        "            raise\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def convert_tf_generator(tf_G):\n",
        "    if tf_G.version < 4:\n",
        "        raise ValueError('TensorFlow pickle version too low')\n",
        "\n",
        "    # Collect kwargs.\n",
        "    tf_kwargs = tf_G.static_kwargs\n",
        "    known_kwargs = set()\n",
        "    def kwarg(tf_name, default=None, none=None):\n",
        "        known_kwargs.add(tf_name)\n",
        "        val = tf_kwargs.get(tf_name, default)\n",
        "        return val if val is not None else none\n",
        "\n",
        "    # Convert kwargs.\n",
        "    network_class = Generator\n",
        "    kwargs = EasyDict(\n",
        "        z_dim               = kwarg('latent_size',          512),\n",
        "        c_dim               = kwarg('label_size',           0),\n",
        "        w_dim               = kwarg('dlatent_size',         512),\n",
        "        img_resolution      = kwarg('resolution',           1024),\n",
        "        img_channels        = kwarg('num_channels',         3),\n",
        "        channel_base        = kwarg('fmap_base',            16384) * 2,\n",
        "        channel_max         = kwarg('fmap_max',             512),\n",
        "        num_fp16_res        = kwarg('num_fp16_res',         0),\n",
        "        conv_clamp          = kwarg('conv_clamp',           None),\n",
        "        architecture        = kwarg('architecture',         'skip'),\n",
        "        resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n",
        "        use_noise           = kwarg('use_noise',            True),\n",
        "        activation          = kwarg('nonlinearity',         'lrelu'),\n",
        "        mapping_kwargs      = EasyDict(\n",
        "            num_layers      = kwarg('mapping_layers',       8),\n",
        "            embed_features  = kwarg('label_fmaps',          None),\n",
        "            layer_features  = kwarg('mapping_fmaps',        None),\n",
        "            activation      = kwarg('mapping_nonlinearity', 'lrelu'),\n",
        "            lr_multiplier   = kwarg('mapping_lrmul',        0.01),\n",
        "            w_avg_beta      = kwarg('w_avg_beta',           0.995,  none=1),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Check for unknown kwargs.\n",
        "    kwarg('truncation_psi')\n",
        "    kwarg('truncation_cutoff')\n",
        "    kwarg('style_mixing_prob')\n",
        "    kwarg('structure')\n",
        "    kwarg('conditioning')\n",
        "    kwarg('fused_modconv')\n",
        "    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n",
        "    if len(unknown_kwargs) > 0:\n",
        "        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n",
        "\n",
        "    # Collect params.\n",
        "    tf_params = _collect_tf_params(tf_G)\n",
        "    for name, value in list(tf_params.items()):\n",
        "        match = re.fullmatch(r'ToRGB_lod(\\d+)/(.*)', name)\n",
        "        if match:\n",
        "            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n",
        "            tf_params[f'{r}x{r}/ToRGB/{match.group(2)}'] = value\n",
        "            kwargs.synthesis.kwargs.architecture = 'orig'\n",
        "    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n",
        "\n",
        "    # Convert params.\n",
        "    G = network_class(**kwargs).eval().requires_grad_(False)\n",
        "    # pylint: disable=unnecessary-lambda\n",
        "    # pylint: disable=f-string-without-interpolation\n",
        "    _populate_module_params(G,\n",
        "        r'mapping\\.w_avg',                                  lambda:     tf_params[f'dlatent_avg'],\n",
        "        r'mapping\\.embed\\.weight',                          lambda:     tf_params[f'mapping/LabelEmbed/weight'].transpose(),\n",
        "        r'mapping\\.embed\\.bias',                            lambda:     tf_params[f'mapping/LabelEmbed/bias'],\n",
        "        r'mapping\\.fc(\\d+)\\.weight',                        lambda i:   tf_params[f'mapping/Dense{i}/weight'].transpose(),\n",
        "        r'mapping\\.fc(\\d+)\\.bias',                          lambda i:   tf_params[f'mapping/Dense{i}/bias'],\n",
        "        r'synthesis\\.b4\\.const',                            lambda:     tf_params[f'synthesis/4x4/Const/const'][0],\n",
        "        r'synthesis\\.b4\\.conv1\\.weight',                    lambda:     tf_params[f'synthesis/4x4/Conv/weight'].transpose(3, 2, 0, 1),\n",
        "        r'synthesis\\.b4\\.conv1\\.bias',                      lambda:     tf_params[f'synthesis/4x4/Conv/bias'],\n",
        "        r'synthesis\\.b4\\.conv1\\.noise_const',               lambda:     tf_params[f'synthesis/noise0'][0, 0],\n",
        "        r'synthesis\\.b4\\.conv1\\.noise_strength',            lambda:     tf_params[f'synthesis/4x4/Conv/noise_strength'],\n",
        "        r'synthesis\\.b4\\.conv1\\.affine\\.weight',            lambda:     tf_params[f'synthesis/4x4/Conv/mod_weight'].transpose(),\n",
        "        r'synthesis\\.b4\\.conv1\\.affine\\.bias',              lambda:     tf_params[f'synthesis/4x4/Conv/mod_bias'] + 1,\n",
        "        r'synthesis\\.b(\\d+)\\.conv0\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n",
        "        r'synthesis\\.b(\\d+)\\.conv0\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/bias'],\n",
        "        r'synthesis\\.b(\\d+)\\.conv0\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-5}'][0, 0],\n",
        "        r'synthesis\\.b(\\d+)\\.conv0\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/noise_strength'],\n",
        "        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_weight'].transpose(),\n",
        "        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_bias'] + 1,\n",
        "        r'synthesis\\.b(\\d+)\\.conv1\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/weight'].transpose(3, 2, 0, 1),\n",
        "        r'synthesis\\.b(\\d+)\\.conv1\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/bias'],\n",
        "        r'synthesis\\.b(\\d+)\\.conv1\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-4}'][0, 0],\n",
        "        r'synthesis\\.b(\\d+)\\.conv1\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/noise_strength'],\n",
        "        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_weight'].transpose(),\n",
        "        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_bias'] + 1,\n",
        "        r'synthesis\\.b(\\d+)\\.torgb\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/weight'].transpose(3, 2, 0, 1),\n",
        "        r'synthesis\\.b(\\d+)\\.torgb\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/bias'],\n",
        "        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_weight'].transpose(),\n",
        "        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_bias'] + 1,\n",
        "        r'synthesis\\.b(\\d+)\\.skip\\.weight',                 lambda r:   tf_params[f'synthesis/{r}x{r}/Skip/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n",
        "        r'.*\\.resample_filter',                             None,\n",
        "        r'.*\\.act_filter',                                  None,\n",
        "    )\n",
        "    return G\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def convert_tf_discriminator(tf_D):\n",
        "    if tf_D.version < 4:\n",
        "        raise ValueError('TensorFlow pickle version too low')\n",
        "\n",
        "    # Collect kwargs.\n",
        "    tf_kwargs = tf_D.static_kwargs\n",
        "    known_kwargs = set()\n",
        "    def kwarg(tf_name, default=None):\n",
        "        known_kwargs.add(tf_name)\n",
        "        return tf_kwargs.get(tf_name, default)\n",
        "\n",
        "    # Convert kwargs.\n",
        "    kwargs = EasyDict(\n",
        "        c_dim                   = kwarg('label_size',           0),\n",
        "        img_resolution          = kwarg('resolution',           1024),\n",
        "        img_channels            = kwarg('num_channels',         3),\n",
        "        architecture            = kwarg('architecture',         'resnet'),\n",
        "        channel_base            = kwarg('fmap_base',            16384) * 2,\n",
        "        channel_max             = kwarg('fmap_max',             512),\n",
        "        num_fp16_res            = kwarg('num_fp16_res',         0),\n",
        "        conv_clamp              = kwarg('conv_clamp',           None),\n",
        "        cmap_dim                = kwarg('mapping_fmaps',        None),\n",
        "        block_kwargs = EasyDict(\n",
        "            activation          = kwarg('nonlinearity',         'lrelu'),\n",
        "            resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n",
        "            freeze_layers       = kwarg('freeze_layers',        0),\n",
        "        ),\n",
        "        mapping_kwargs = EasyDict(\n",
        "            num_layers          = kwarg('mapping_layers',       0),\n",
        "            embed_features      = kwarg('mapping_fmaps',        None),\n",
        "            layer_features      = kwarg('mapping_fmaps',        None),\n",
        "            activation          = kwarg('nonlinearity',         'lrelu'),\n",
        "            lr_multiplier       = kwarg('mapping_lrmul',        0.1),\n",
        "        ),\n",
        "        epilogue_kwargs = EasyDict(\n",
        "            mbstd_group_size    = kwarg('mbstd_group_size',     None),\n",
        "            mbstd_num_channels  = kwarg('mbstd_num_features',   1),\n",
        "            activation          = kwarg('nonlinearity',         'lrelu'),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Check for unknown kwargs.\n",
        "    kwarg('structure')\n",
        "    kwarg('conditioning')\n",
        "    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n",
        "    if len(unknown_kwargs) > 0:\n",
        "        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n",
        "\n",
        "    # Collect params.\n",
        "    tf_params = _collect_tf_params(tf_D)\n",
        "    for name, value in list(tf_params.items()):\n",
        "        match = re.fullmatch(r'FromRGB_lod(\\d+)/(.*)', name)\n",
        "        if match:\n",
        "            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n",
        "            tf_params[f'{r}x{r}/FromRGB/{match.group(2)}'] = value\n",
        "            kwargs.architecture = 'orig'\n",
        "    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n",
        "\n",
        "    # Convert params\n",
        "    D = Discriminator(**kwargs).eval().requires_grad_(False)\n",
        "    # pylint: disable=unnecessary-lambda\n",
        "    # pylint: disable=f-string-without-interpolation\n",
        "    _populate_module_params(D,\n",
        "        r'b(\\d+)\\.fromrgb\\.weight',     lambda r:       tf_params[f'{r}x{r}/FromRGB/weight'].transpose(3, 2, 0, 1),\n",
        "        r'b(\\d+)\\.fromrgb\\.bias',       lambda r:       tf_params[f'{r}x{r}/FromRGB/bias'],\n",
        "        r'b(\\d+)\\.conv(\\d+)\\.weight',   lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/weight'].transpose(3, 2, 0, 1),\n",
        "        r'b(\\d+)\\.conv(\\d+)\\.bias',     lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/bias'],\n",
        "        r'b(\\d+)\\.skip\\.weight',        lambda r:       tf_params[f'{r}x{r}/Skip/weight'].transpose(3, 2, 0, 1),\n",
        "        r'mapping\\.embed\\.weight',      lambda:         tf_params[f'LabelEmbed/weight'].transpose(),\n",
        "        r'mapping\\.embed\\.bias',        lambda:         tf_params[f'LabelEmbed/bias'],\n",
        "        r'mapping\\.fc(\\d+)\\.weight',    lambda i:       tf_params[f'Mapping{i}/weight'].transpose(),\n",
        "        r'mapping\\.fc(\\d+)\\.bias',      lambda i:       tf_params[f'Mapping{i}/bias'],\n",
        "        r'b4\\.conv\\.weight',            lambda:         tf_params[f'4x4/Conv/weight'].transpose(3, 2, 0, 1),\n",
        "        r'b4\\.conv\\.bias',              lambda:         tf_params[f'4x4/Conv/bias'],\n",
        "        r'b4\\.fc\\.weight',              lambda:         tf_params[f'4x4/Dense0/weight'].transpose(),\n",
        "        r'b4\\.fc\\.bias',                lambda:         tf_params[f'4x4/Dense0/bias'],\n",
        "        r'b4\\.out\\.weight',             lambda:         tf_params[f'Output/weight'].transpose(),\n",
        "        r'b4\\.out\\.bias',               lambda:         tf_params[f'Output/bias'],\n",
        "        r'.*\\.resample_filter',         None,\n",
        "    )\n",
        "    return D\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@click.command()\n",
        "@click.option('--source', help='Input pickle', required=True, metavar='PATH')\n",
        "@click.option('--dest', help='Output pickle', required=True, metavar='PATH')\n",
        "@click.option('--force-fp16', help='Force the networks to use FP16', type=bool, default=False, metavar='BOOL', show_default=True)\n",
        "def convert_network_pickle(source, dest, force_fp16):\n",
        "\n",
        "    with open_url(source) as f:\n",
        "        data = load_network_pkl(f, force_fp16=force_fp16)\n",
        "    with open(dest, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_network_pickle() # pylint: disable=no-value-for-parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9InRMiYx_BCf"
      },
      "outputs": [],
      "source": [
        "#this is taken from: https://github.com/NVlabs/stylegan2-ada/blob/main/generate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1YstCYp-bFk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "from typing import List, Optional\n",
        "\n",
        "import click\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "import PIL.Image\n",
        "import torch\n",
        "\n",
        "\n",
        "from opensimplex import OpenSimplex\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class OSN():\n",
        "    min = -1\n",
        "    max = 1\n",
        "\n",
        "    def __init__(self, seed, diameter):\n",
        "        self.tmp = OpenSimplex(seed)\n",
        "        self.d = diameter\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "\n",
        "    def get_val(self, angle):\n",
        "        self.xoff = valmap(np.cos(angle), -1, 1, self.x, self.x + self.d)\n",
        "        self.yoff = valmap(np.sin(angle), -1, 1, self.y, self.y + self.d)\n",
        "        return self.tmp.noise2(self.xoff,self.yoff)\n",
        "\n",
        "def circularloop(nf, d, seed, seeds):\n",
        "    r = d/2\n",
        "\n",
        "    zs = []\n",
        "    # hardcoding in 512, prob TODO fix needed\n",
        "    # latents_c = rnd.randn(1, G.input_shape[1])\n",
        "\n",
        "    if(seeds is None):\n",
        "        if seed:\n",
        "            rnd = np.random.RandomState(seed)\n",
        "        else:\n",
        "            rnd = np.random\n",
        "        latents_a = rnd.randn(1, 512)\n",
        "        latents_b = rnd.randn(1, 512)\n",
        "        latents_c = rnd.randn(1, 512)\n",
        "    elif(len(seeds) is not 3):\n",
        "        assert('Must choose exactly 3 seeds!')\n",
        "    else:\n",
        "        latents_a = np.random.RandomState(int(seeds[0])).randn(1, 512)\n",
        "        latents_b = np.random.RandomState(int(seeds[1])).randn(1, 512)\n",
        "        latents_c = np.random.RandomState(int(seeds[2])).randn(1, 512)\n",
        "\n",
        "    latents = (latents_a, latents_b, latents_c)\n",
        "\n",
        "    current_pos = 0.0\n",
        "    step = 1./nf\n",
        "\n",
        "    while(current_pos < 1.0):\n",
        "        zs.append(circular_interpolation(r, latents, current_pos))\n",
        "        current_pos += step\n",
        "    return zs\n",
        "\n",
        "def circular_interpolation(radius, latents_persistent, latents_interpolate):\n",
        "    latents_a, latents_b, latents_c = latents_persistent\n",
        "\n",
        "    latents_axis_x = (latents_a - latents_b).flatten() / linalg.norm(latents_a - latents_b)\n",
        "    latents_axis_y = (latents_a - latents_c).flatten() / linalg.norm(latents_a - latents_c)\n",
        "\n",
        "    latents_x = np.sin(np.pi * 2.0 * latents_interpolate) * radius\n",
        "    latents_y = np.cos(np.pi * 2.0 * latents_interpolate) * radius\n",
        "\n",
        "    latents = latents_a + latents_x * latents_axis_x + latents_y * latents_axis_y\n",
        "    return latents\n",
        "\n",
        "def num_range(s: str) -> List[int]:\n",
        "    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n",
        "    m = range_re.match(s)\n",
        "    if m:\n",
        "        return list(range(int(m.group(1)), int(m.group(2))+1))\n",
        "    vals = s.split(',')\n",
        "    return [int(x) for x in vals]\n",
        "\n",
        "def size_range(s: str) -> List[int]:\n",
        "    return [int(v) for v in s.split('-')][::-1]\n",
        "\n",
        "def line_interpolate(zs, steps, easing):\n",
        "    out = []\n",
        "    for i in range(len(zs)-1):\n",
        "        for index in range(steps):\n",
        "            t = index/float(steps)\n",
        "\n",
        "            if(easing == 'linear'):\n",
        "                out.append(zs[i+1]*t + zs[i]*(1-t))\n",
        "            elif (easing == 'easeInOutQuad'):\n",
        "                if(t < 0.5):\n",
        "                    fr = 2 * t * t\n",
        "                else:\n",
        "                    fr = (-2 * t * t) + (4 * t) - 1\n",
        "                out.append(zs[i+1]*fr + zs[i]*(1-fr))\n",
        "            elif (easing == 'bounceEaseOut'):\n",
        "                if (t < 4/11):\n",
        "                    fr = 121 * t * t / 16\n",
        "                elif (t < 8/11):\n",
        "                    fr = (363 / 40.0 * t * t) - (99 / 10.0 * t) + 17 / 5.0\n",
        "                elif t < 9/10:\n",
        "                    fr = (4356 / 361.0 * t * t) - (35442 / 1805.0 * t) + 16061 / 1805.0\n",
        "                else:\n",
        "                    fr = (54 / 5.0 * t * t) - (513 / 25.0 * t) + 268 / 25.0\n",
        "                out.append(zs[i+1]*fr + zs[i]*(1-fr))\n",
        "            elif (easing == 'circularEaseOut'):\n",
        "                fr = np.sqrt((2 - t) * t)\n",
        "                out.append(zs[i+1]*fr + zs[i]*(1-fr))\n",
        "            elif (easing == 'circularEaseOut2'):\n",
        "                fr = np.sqrt(np.sqrt((2 - t) * t))\n",
        "                out.append(zs[i+1]*fr + zs[i]*(1-fr))\n",
        "            elif(easing == 'backEaseOut'):\n",
        "                p = 1 - t\n",
        "                fr = 1 - (p * p * p - p * math.sin(p * math.pi))\n",
        "                out.append(zs[i+1]*fr + zs[i]*(1-fr))\n",
        "    return out\n",
        "\n",
        "def noiseloop(nf, d, seed):\n",
        "    if seed:\n",
        "        np.random.RandomState(seed)\n",
        "\n",
        "    features = []\n",
        "    zs = []\n",
        "    for i in range(512):\n",
        "      features.append(OSN(i+seed,d))\n",
        "\n",
        "    inc = (np.pi*2)/nf\n",
        "    for f in range(nf):\n",
        "      z = np.random.randn(1, 512)\n",
        "      for i in range(512):\n",
        "        z[0,i] = features[i].get_val(inc*f)\n",
        "      zs.append(z)\n",
        "\n",
        "    return zs\n",
        "\n",
        "def images(G,device,inputs,space,truncation_psi,label,noise_mode,outdir,start=None,stop=None):\n",
        "    if(start is not None and stop is not None):\n",
        "        tp = start\n",
        "        tp_i = (stop-start)/len(inputs)\n",
        "\n",
        "    for idx, i in enumerate(inputs):\n",
        "        print('Generating image for frame %d/%d ...' % (idx, len(inputs)))\n",
        "        \n",
        "        if (space=='z'):\n",
        "            z = torch.from_numpy(i).to(device)\n",
        "            if(start is not None and stop is not None):\n",
        "                img = G(z, label, truncation_psi=tp, noise_mode=noise_mode)\n",
        "                tp = tp+tp_i\n",
        "            else:\n",
        "                img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "        else:\n",
        "            if len(i.shape) == 2: \n",
        "              i = torch.from_numpy(i).unsqueeze(0).to(device)\n",
        "            img = G.synthesis(i, noise_mode=noise_mode, force_fp32=True)\n",
        "        img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "        PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/frame{idx:04d}.png')\n",
        "\n",
        "def interpolate(G,device,projected_w,seeds,random_seed,space,truncation_psi,label,frames,noise_mode,outdir,interpolation,easing,diameter,start=None,stop=None):\n",
        "    if(interpolation=='noiseloop' or interpolation=='circularloop'):\n",
        "        if seeds is not None:\n",
        "            print(f'Warning: interpolation type: \"{interpolation}\" doesnt support set seeds.')\n",
        "\n",
        "        if(interpolation=='noiseloop'):\n",
        "            points = noiseloop(frames, diameter, random_seed)\n",
        "        elif(interpolation=='circularloop'):\n",
        "            points = circularloop(frames, diameter, random_seed, seeds)\n",
        "\n",
        "    else:\n",
        "        if projected_w is not None:\n",
        "            points = np.load(projected_w)['w']\n",
        "        else:\n",
        "            # get zs from seeds\n",
        "            points = seeds_to_zs(G,seeds)  \n",
        "            # convert to ws\n",
        "            if(space=='w'):\n",
        "                points = zs_to_ws(G,device,label,truncation_psi,points)\n",
        "\n",
        "        # get interpolation points\n",
        "        if(interpolation=='linear'):\n",
        "            points = line_interpolate(points,frames,easing)\n",
        "        elif(interpolation=='slerp'):\n",
        "            points = slerp_interpolate(points,frames)\n",
        "            \n",
        "    # generate frames\n",
        "    images(G,device,points,space,truncation_psi,label,noise_mode,outdir,start,stop)\n",
        "\n",
        "def seeds_to_zs(G,seeds):\n",
        "    zs = []\n",
        "    for seed_idx, seed in enumerate(seeds):\n",
        "        z = np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "        zs.append(z)\n",
        "    return zs\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "@click.command()\n",
        "@click.pass_context\n",
        "@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n",
        "@click.option('--seeds', type=num_range, help='List of random seeds')\n",
        "@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n",
        "@click.option('--class', 'class_idx', type=int, help='Class label (unconditional if not specified)')\n",
        "@click.option('--diameter', type=float, help='diameter of loops', default=100.0, show_default=True)\n",
        "@click.option('--frames', type=int, help='how many frames to produce (with seeds this is frames between each step, with loops this is total length)', default=240, show_default=True)\n",
        "@click.option('--fps', type=int, help='framerate for video', default=24, show_default=True)\n",
        "@click.option('--increment', type=float, help='truncation increment value', default=0.01, show_default=True)\n",
        "@click.option('--interpolation', type=click.Choice(['linear', 'slerp', 'noiseloop', 'circularloop']), default='linear', help='interpolation type', required=True)\n",
        "@click.option('--easing',\n",
        "              type=click.Choice(['linear', 'easeInOutQuad', 'bounceEaseOut','circularEaseOut','circularEaseOut2']),\n",
        "              default='linear', help='easing method', required=True)\n",
        "@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n",
        "@click.option('--noise-mode', help='Noise mode', type=click.Choice(['const', 'random', 'none']), default='const', show_default=True)\n",
        "@click.option('--outdir', help='Where to save the output images', type=str, required=True, metavar='DIR')\n",
        "@click.option('--process', type=click.Choice(['image', 'interpolation','truncation','interpolation-truncation']), default='image', help='generation method', required=True)\n",
        "@click.option('--projected-w', help='Projection result file', type=str, metavar='FILE')\n",
        "@click.option('--random_seed', type=int, help='random seed value (used in noise and circular loop)', default=0, show_default=True)\n",
        "@click.option('--scale-type',\n",
        "                type=click.Choice(['pad', 'padside', 'symm','symmside']),\n",
        "                default='pad', help='scaling method for --size', required=False)\n",
        "@click.option('--size', type=size_range, help='size of output (in format x-y)')\n",
        "@click.option('--seeds', type=num_range, help='List of random seeds')\n",
        "@click.option('--space', type=click.Choice(['z', 'w']), default='z', help='latent space', required=True)\n",
        "@click.option('--start', type=float, help='starting truncation value', default=0.0, show_default=True)\n",
        "@click.option('--stop', type=float, help='stopping truncation value', default=1.0, show_default=True)\n",
        "@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n",
        "\n",
        "def generate_images(\n",
        "    ctx: click.Context,\n",
        "    easing: str,\n",
        "    interpolation: str,\n",
        "    increment: Optional[float],\n",
        "    network_pkl: str,\n",
        "    process: str,\n",
        "    random_seed: Optional[int],\n",
        "    diameter: Optional[float],\n",
        "    scale_type: Optional[str],\n",
        "    size: Optional[List[int]],\n",
        "    seeds: Optional[List[int]],\n",
        "    space: str,\n",
        "    fps: Optional[int],\n",
        "    frames: Optional[int],\n",
        "    truncation_psi: float,\n",
        "    noise_mode: str,\n",
        "    outdir: str,\n",
        "    class_idx: Optional[int],\n",
        "    projected_w: Optional[str],\n",
        "    start: Optional[float],\n",
        "    stop: Optional[float],\n",
        "):\n",
        "\n",
        "    # custom size code from https://github.com/eps696/stylegan2ada/blob/master/src/_genSGAN2.py\n",
        "    if(size): \n",
        "        print('render custom size: ',size)\n",
        "        print('padding method:', scale_type )\n",
        "        custom = True\n",
        "    else:\n",
        "        custom = False\n",
        "\n",
        "    G_kwargs = EasyDict()\n",
        "    G_kwargs.size = size \n",
        "    G_kwargs.scale_type = scale_type\n",
        "\n",
        "    # mask/blend latents with external latmask or by splitting the frame\n",
        "    latmask = False #temp\n",
        "    if latmask is None:\n",
        "        nHW = [int(s) for s in a.nXY.split('-')][::-1]\n",
        "        assert len(nHW)==2, ' Wrong count nXY: %d (must be 2)' % len(nHW)\n",
        "        n_mult = nHW[0] * nHW[1]\n",
        "        # if a.verbose is True and n_mult > 1: print(' Latent blending w/split frame %d x %d' % (nHW[1], nHW[0]))\n",
        "        lmask = np.tile(np.asarray([[[[1]]]]), (1,n_mult,1,1))\n",
        "        Gs_kwargs.countHW = nHW\n",
        "        Gs_kwargs.splitfine = a.splitfine\n",
        "        lmask = torch.from_numpy(lmask).to(device)\n",
        "    # else:\n",
        "        # if a.verbose is True: print(' Latent blending with mask', a.latmask)\n",
        "        # n_mult = 2\n",
        "        # if os.path.isfile(a.latmask): # single file\n",
        "        #     lmask = np.asarray([[img_read(a.latmask)[:,:,0] / 255.]]) # [h,w]\n",
        "        # elif os.path.isdir(a.latmask): # directory with frame sequence\n",
        "        #     lmask = np.asarray([[img_read(f)[:,:,0] / 255. for f in img_list(a.latmask)]]) # [h,w]\n",
        "        # else:\n",
        "        #     print(' !! Blending mask not found:', a.latmask); exit(1)\n",
        "        # lmask = np.concatenate((lmask, 1 - lmask), 1) # [frm,2,h,w]\n",
        "    # lmask = torch.from_numpy(lmask).to(device)\n",
        "\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "    device = torch.device('cuda')\n",
        "    with open_url(network_pkl) as f:\n",
        "        G = load_network_pkl(f, custom=custom, **G_kwargs)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # Synthesize the result of a W projection.\n",
        "    if (process=='image') and projected_w is not None:\n",
        "        if seeds is not None:\n",
        "            print ('Warning: --seeds is ignored when using --projected-w')\n",
        "        print(f'Generating images from projected W \"{projected_w}\"')\n",
        "        ws = np.load(projected_w)['w']\n",
        "        ws = torch.tensor(ws, device=device) # pylint: disable=not-callable\n",
        "        assert ws.shape[1:] == (G.num_ws, G.w_dim)\n",
        "        for idx, w in enumerate(ws):\n",
        "            img = G.synthesis(w.unsqueeze(0), noise_mode=noise_mode)\n",
        "            img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "            img = PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/proj{idx:02d}.png')\n",
        "        return\n",
        "\n",
        "    # Labels.\n",
        "    label = torch.zeros([1, G.c_dim], device=device)\n",
        "    if G.c_dim != 0:\n",
        "        if class_idx is None:\n",
        "            ctx.fail('Must specify class label with --class when using a conditional network')\n",
        "        label[:, class_idx] = 1\n",
        "    else:\n",
        "        if class_idx is not None:\n",
        "            print ('warn: --class=lbl ignored when running on an unconditional network')\n",
        "\n",
        "\n",
        "    if(process=='image'):\n",
        "        if seeds is None:\n",
        "            ctx.fail('--seeds option is required when not using --projected-w')\n",
        "\n",
        "        # Generate images.\n",
        "        for seed_idx, seed in enumerate(seeds):\n",
        "            print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n",
        "            z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
        "            img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "            img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "            PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n",
        "\n",
        "    elif(process=='interpolation' or process=='interpolation-truncation'):\n",
        "        # create path for frames\n",
        "        dirpath = os.path.join(outdir,'frames')\n",
        "        os.makedirs(dirpath, exist_ok=True)\n",
        "\n",
        "        # autogenerate video name: not great!\n",
        "        if seeds is not None:\n",
        "            seedstr = '_'.join([str(seed) for seed in seeds])\n",
        "            vidname = f'{process}-{interpolation}-seeds_{seedstr}-{fps}fps'\n",
        "        elif(interpolation=='noiseloop' or 'circularloop'):\n",
        "            vidname = f'{process}-{interpolation}-{diameter}dia-seed_{random_seed}-{fps}fps'\n",
        "\n",
        "        if process=='interpolation-truncation':\n",
        "            interpolate(G,device,projected_w,seeds,random_seed,space,truncation_psi,label,frames,noise_mode,dirpath,interpolation,easing,diameter,start,stop)\n",
        "        else:\n",
        "            interpolate(G,device,projected_w,seeds,random_seed,space,truncation_psi,label,frames,noise_mode,dirpath,interpolation,easing,diameter)\n",
        "\n",
        "        # convert to video\n",
        "        cmd=f'ffmpeg -y -r {fps} -i {dirpath}/frame%04d.png -vcodec libx264 -pix_fmt yuv420p {outdir}/{vidname}.mp4'\n",
        "        subprocess.call(cmd, shell=True)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_images() # pylint: disable=no-value-for-parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHVJkWOwfMhE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
